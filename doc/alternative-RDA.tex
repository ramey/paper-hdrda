\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{An Alternative Parameterization of Regularized Discriminant Analysis}

\author{John A. Ramey and Dean M. Young}

\begin{document}

\newtheorem*{thm}{Theorem}
\newtheorem*{cor}{Corollary}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle



\begin{abstract}
We introduce an alternative parameterization of regularized discriminant analysis (\emph{RDA}) from \cite{Friedman:1989tm} that is simpler and has a more intuitive interpretation. Additionally, by exploiting properties of the singular value decomposition (SVD) as well as the so-called Fast SVD algorithm, we propose a classifier that is faster to compute for small-sample, high-dimensional data sets.
\end{abstract}

\section{Introduction}

We wish to correctly assign an unlabeled $p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes (or populations) by constructing a classifier from $n$ training observations that can accurately predict the class membership of $\bm x$. Let $\bm x_i = (x_{i1}, x_{i2}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots, n)$ with true, unique membership $y_i \in \mathcal{K} = \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{m \times n}$ denotes the matrix space of all $m \times n$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of class membership of the $k$th class.

Quadratic discriminant analysis (\emph{QDA}) is the optimal Bayesian decision rule with respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate normal distribution with known mean vector  $\bm\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite matrices. Because the parameters are typically unknown, we estimate the unknown parameters $\bm \mu_k$ and $\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the MLEs into the \emph{QDA} classifier. Assuming, for simplicity, that the prior probabilities of class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an unlabeled observation $\bm x$ to class $\omega_k$ using the sample \emph{QDA} classifier:
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further that the covariance matrix parameters are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$ for all $k$), then \eqref{eq:qda} reduces to Fisher's linear discriminant analysis (LDA) classifier,
\begin{align}
	D_{LDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}^{-1}(\bm x - \xbar_k), \label{eq:lda}
\end{align}
where $\widehat{\bm\Sigma}$ is the pooled sample covariance matrix
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{n} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}

\section{Regularized Discriminant Analysis}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$ to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda n$, $\bm S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret \eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in \eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}$ used in the \emph{LDA} classifier in \eqref{eq:lda}, in which case we assume implicitly that $\bm\Sigma_k = \bm \Sigma$ for $k = 1, \ldots, K$.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased \emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter that controls the shrinkage of \eqref{eq:sig-rda}  towards the $p$-dimensional identity matrix $\bm I_p$, which is weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from $\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the amount of shrinkage applied.

Substituting \eqref{eq:sig-rda} into \eqref{eq:qda}, we have the \emph{RDA} classifier, where we assign an unlabeled observation $\bm x$ to class $\omega_k$:
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}

\section{An Alternative Approach to Regularized Discriminant Analysis}

Here, we present a more straightforward, alternative parameterization of the \emph{RDA} classifier. We first define explicitly
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we no longer have need for the convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. Before examining the shrinkage applied to \eqref{eq:sig-lambda-alternative}, we examine some of its properties and write \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'}.\label{eq:sig-lambda-alternative2}
\end{align}
From \eqref{eq:sig-lambda-alternative2}, we see the weight $\lambda$ places on the covariance matrices from the remaining $K - 1$ classes. The interesting part here is that $\lambda$ controls the contribution of each of the $N$ observations to estimating $\Sigma_k$. Effectively, this states that for $0 < \lambda \le 1$, we use $N$ observations to estimate $\bm \Sigma_k$ rather than $n_k$ as done in the \emph{QDA} classifier. To see this further, suppose that we have centered each observation $\bm x_i$, $i = 1, \ldots, N$, by its class sample mean. Then, we write \eqref{eq:sig-lambda-alternative2} as
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative3}
\intertext{where}
	c_{ik}(\lambda) &= \begin{cases}
		(1 - \lambda)n_k^{-1} + \lambda N^{-1}, & y_i = k,\\
		\lambda N^{-1}, & y_i \ne k.\\
	\end{cases}\nonumber
\end{align}
Therefore, after centering each observation by its corresponding class sample mean, we have that $\widehat{\bm\Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda-alternative3} incorporates each centered observation into the estimation of $\bm \Sigma_k$. Notice that if $\lambda  = 0$, then the observations from the remaining $K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in \eqref{eq:sig-lambda-alternative3} reduce to $1/N$, corresponding to $\widehat{\bm\Sigma}$.

\subsection{Computational Advantages}

Thus far, our alternative parameterization simplifies the definition of the \emph{RDA} classifier and has an improved interpretation in terms of the contribution of each observation. Here, we demonstrate that our parameterization yields significant computational advantages over the originally proposed \emph{RDA} classifier from \cite{Friedman:1989tm} by writing \eqref{eq:sig-lambda-alternative3} as
\begin{align*}
	\widehat{\bm\Sigma}_k(\lambda) &= \bm X_k(\lambda)' \bm X_k(\lambda),
\end{align*}
where $\bm X_k(\lambda) = [\sqrt{c_{1k}(\lambda)} \bm x_1', \ldots,  \sqrt{c_{Nk}(\lambda)} \bm x_N']'$. Now, we can apply the Fast SVD to compute the eigenvalue decomposition of $\widehat{\bm\Sigma}_k(\lambda)$ efficiently. By writing the SVD of $\bm X_k(\lambda) = \bm U_k \bm D_k \bm V_k'$, we have the spectral decomposition $\widehat{\bm\Sigma}_k(\lambda) = \bm X_k(\lambda)' \bm X_k(\lambda) = \bm V_k \bm D_k^2 \bm V_k'$, where $\bm U_k \in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D_k \in \mathbb{R}_{N \times N}$ is a diagonal matrix consisting of the singular values of $\bm X_k(\lambda)$, and $\bm V_k \in \mathbb{R}_{p \times N}$ is orthogonal. Notice that $\bm X_k(\lambda)\bm X_k(\lambda)' = \bm U_k \bm D_k^2 \bm U_k'$, so that the eigenvalue decomposition of $\bm X_k(\lambda)\bm X_k(\lambda)'$ yields $\bm U$ and $\bm D$. This is much faster when $p \gg N$ because $\bm X_k(\lambda)\bm X_k(\lambda)' \in \mathbb{R}_{N \times N}$. Also, notice that the singular values of $\bm D$ are nonzero, so that $\bm D^{-1}$ exists. After obtaining $\bm U_k$ and $\bm D_k$, we compute the matrix consisting of the first $N$ eigenvectors of $\widehat{\bm\Sigma}_k(\lambda)$ by simply computing\footnote{TODO: Update this paragraph to theorem/proof.}
\begin{align*}
	\bm V_k = \bm X_k(\lambda)' \bm U_k \bm D_k^{-1}.
\end{align*}

\subsection{Covariance-Matrix Regularization}

We are now ready to discuss the shrinkage employed in the \emph{RDA} covariance-matrix estimator. As we are interested primarily in the case that $p \gg N$, rank$\{\widehat{\bm\Sigma}_k(\lambda)\} = q_k < p$, which implies that $p - q_k$ of the eigenvalues of $\widehat{\bm\Sigma}_k(\lambda)$ are equal to zero. Thus, for $\gamma > 0$, shrinking $\widehat{\bm\Sigma}_k(\lambda)$ towards $\bm I_p$ as in \eqref{eq:sig-rda} increases the rank of $\widehat{\bm\Sigma}_k(\lambda)$ from $q_k$ to $p$. However, \cite{Ye:2009gd} and \cite{YeOtherPaperCitedInTheFirst} have shown that shrinking the zero eigenvalues does not improve the classification performance of the \emph{RDA} classifier. Consequently, we instead shrink only the nonzero eigenvalues of $\widehat{\bm\Sigma}_k(\lambda)$, which has two additional benefits. First, by considering only the nonzero eigenvalues of $\widehat{\bm\Sigma}_k(\lambda)$, we effectively ignore the eigenvectors corresponding to the zero eigenvalues, which reduces significantly the number of parameters that we incorporate into our \emph{RDA} classifier because $p \gg q_k$ if $p \gg N$. Furthermore, the size of the matrix $\bm V_k$ is $p \times N$ as opposed to a $p \times p$ matrix in the original definition of the \emph{RDA} classifier. For significantly large $p$, the reduced size of $\bm V_k$ expedites calculation of the \emph{RDA} classifier and reduces its memory usage.\footnote{TODO: Provide an example for a realistic $p$ and $N$.}

Specifically, we shrink only the nonzero eigenvalues of $\widehat{\bm \Sigma}_k(\lambda)$ by utilizing the estimator

\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} (\bm I_{q_k} \oplus \bm 0_{p - q_k}),\label{eq:sig-rda-alternative}
\end{align}
where $\bm A \oplus \bm B$ denote the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Therefore, substituting \eqref{eq:sig-rda-alternative} into \eqref{eq:rda}, we have our alternative parameterization of the \emph{RDA} classifier.

\bibliographystyle{plainnat}
\bibliography{../reports/rda}


\end{document} 