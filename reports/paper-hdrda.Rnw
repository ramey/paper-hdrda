\documentclass{article}

\usepackage{graphicx, amsmath, amssymb, bm, mathtools, amsthm}
\usepackage{natbib, setspace}
\usepackage{algorithm2e}

\bibpunct{(}{)}{;}{a}{,}{,}

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.5in
\textheight 9.5in

\begin{document}

\title{High-Dimensional Regularized Discriminant Analysis}

\author{John A. Ramey, Caleb K. Stein, Phil D. Young, and Dean M. Young}

\maketitle

\doublespacing

\begin{abstract}

\citeauthor{Friedman:1989tm} proposed the popular regularized discriminant
analysis (RDA) classifier that utilizes a biased covariance-matrix estimator to
partially pool the sample covariance matrices from linear and quadratic
disriminant analysis before shrinking the resulting estimator towards a scaled
identity matrix. The RDA classifier's two tuning parameters are typically
estimated via a computationally burdensome cross-validation procedure from a
grid of candidate values. We reformulate the RDA classifier in the small-sample,
high-dimensional setting and then show that the classification decision rule is
equivalent to one in a subspace having a much lower dimension. As a result, the
dimension reduction involved yields a substantial reduction in computation
during model selection. Our parameterization also offers interpretability
previously lacking with the RDA classifier. We demonstrate with three artificial
and four real high-dimensional data sets that our proposed classifier is often
superior to several recently proposed sparse and regularized classifiers in
terms of classification accuracy. Finally, we provide an implementation of our
proposed classifier in the {\tt sparsediscrim} R package, available on CRAN.

\end{abstract}

<<knitr_setup, include=FALSE, echo=FALSE, cache=FALSE>>=
# Round digits throughout document including \Sexpr statements.
# See Yihui's response here:
# http://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr
options(digits = 3)

opts_chunk$set(fig.align = 'default', dev = 'png', message = FALSE,
               warning = FALSE, cache = TRUE, echo = FALSE, autodep = TRUE,
               fig.width = 12, fig.height = 12, out.width = "\\linewidth")
@

<<setup>>=
setwd("..")
library(ProjectTemplate)
load.project()

max_se <- 1234567890
@


\section{Introduction}

As machine-learning algorithms are increasingly utilized to automate data-driven
decisions, researchers continue to develop a wealth of \emph{classifiers}
employed in a variety of disciplines, such as medical research, facial
recognition, remote sensing, and chemical analysis
\citep{Ching:2012fu,Li:2012ev,Zhang:2010va}. Despite the advances in
classification to facilitate technological advancements, well-established
classifiers often degrade in accuracy with modern, high-dimensional data sets
when the number of features $p$ exceeds the total sample size $N$, due to the
\emph{curse of dimensionality} \citep{Bellman:1961tn}. For instance, linear
discriminant analysis (LDA) and quadratic discriminant analysis (QDA) are
popular because of their simplicity, speed of computation, and
interpretability. However, both methods are incalculable when $p > N$ because
the class sample covariance matrices are singular \citep*{Bouveyron:2007gx}.

As a result, researchers have emphasized three standard approaches to modifying
the \emph{LDA} and \emph{QDA} classifiers for application to high-dimensional
data: 1) covariance-matrix regularization, 2) dimension reduction, and 3)
parameter restrictions. Covariance-matrix regularization methods are often based
on the well-known ridge-regression approach of \cite{Hoerl:1970cd}.
\cite{Ching:2012fu}, \cite{Zhang:2010va}, \cite{Ye:2009gd}, \cite{Ji:2008wp},
\cite*{Guo:2007te}, \cite*{Srivastava:2007ud}, \cite{Ye:2006vx}, and
\cite{Ye:2005uu} have proposed regularization methods to improve the estimation
of the class covariance matrices by adjusting the eigenvalues of the sample
covariance matrices to ensure positive definiteness. Alternatively, dimension
reduction typically transforms the original data in the high-dimensional space
to a subspace having a much smaller dimension such that redundant and noisy
information is removed
\citep{Fan:2012iq,Ye:2009gd,Bouveyron:2007gx,Ye:2006vx,Ye:2005hd,Antoniadis:2003bk}. Finally,
\cite{Mai:2012bf}, \cite{Fan:2012iq}, \cite{Merchante:2012vk},
\cite{Clemmensen:2011kr}, \cite{Huang:2010ju}, and \cite{Dudoit:2002ev} have
proposed restrictions on the population covariance matrices to reduce the number
of parameters that must be estimated. For instance, \cite{Dudoit:2002ev} have
assumed the features within each class are independent so that the population
covariance matrices are diagonal, resulting in positive-definite sample
covariance matrices. For excellent overviews of the regularization problem in
discriminant analysis and other techniques, see \cite{Murphy:2012uq} and
\cite{Mkhadri:1997gy}.

Here, we present a new classification method intended for the $p > N$ case that
builds on the classic regularized discriminant analysis (\emph{RDA}) proposed by
\cite{Friedman:1989tm}. Our proposed high-dimensional \emph{RDA} (\emph{HDRDA})
classifier employs a biased covariance matrix estimator that partially pools the
individual sample covariance matrices from the \emph{QDA} classifier with the
pooled sample covariance matrix from the \emph{LDA} classifier. We then shrink
the resulting covariance-matrix estimator towards a scaled identity matrix to
ensure positive definiteness. By reparameterizing the \emph{RDA} classifier
similar to \cite{Hastie:2008dt} and \cite{Halbe:2007ea}, we first show the
pooling parameter in the \emph{HDRDA} classifier determines the contribution of
each training observation to the estimation of each class covariance matrix,
enabling interpretability that has been previously lacking with the \emph{RDA}
classifier \citep{Bensmail:1996fw}.

Next, we achieve a substantial reduction in dimension to reduce greatly the
computation during model selection by applying a reasoning similar to
\cite{Ye:2006tq}. We establish that the matrix operations corresponding to the
null space of the pooled sample covariance matrix are redundant and can be
discarded from the \emph{HDRDA} decision rule without loss of classificatory
information. Moreover, because the \emph{HDRDA} decision rule is invariant to
adjustments to the zero eigenvalues, the decision rule in the original feature
space is equivalent to a decision rule in a much lower dimension, such that the
matrix inverses and determinants of relatively small matrices can be rapidly
computed. Furthermore, we show that a large number of shrinkage methods that are
special cases of the \emph{HDRDA} classifier have no effect on the zero
eigenvalues of the covariance-matrix estimators. Such methods include work from
\cite{Srivastava:2007ww}, \cite{Rao:1971ul}, and other methods studied by
\cite{Ramey:2013ji} and \citet*{Xu:2009fl}.

We provide an efficient algorithm that expedites the model-selection process for
the \emph{HDRDA} classifier when $p \gg N$. We then study the classification
performance of the \emph{HDRDA} classifier on four real high-dimensional data
sets along with a simulation experiment that generalizes that initially
conducted by \cite{Guo:2007te}. We demonstrate that the \emph{HDRDA} classifier
often attains superior classification accuracy to several recent classifiers
designed for small-sample, high-dimensional data from \cite{Tong:2012hw},
\cite{Witten:2011kc}, \cite{Pang:2009ik}, and \cite{Guo:2007te}. We also include
as a benchmark the random forest from \cite{Breiman:2001fb} because
\cite{FernandezDelgado:2014ul} has concluded that the random forest is often
superior to other classifiers in benchmark studies.

The remainder of this document is organized as follows. In Section
\ref{sec:preliminaries} we present the discrimination problem and the necessary
notation to describe our contributions. In Section \ref{sec:rda} we present the
\emph{HDRDA} classifier along with its interpretation, properties, and
computationally efficient model-selection procedure. Section \ref{sec:sims}
describes our simulation studies of artificial and real data sets and also
describes the experimental results. We conclude with a brief discussion in
Section \ref{sec:discussion}.

\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Notation}

To facilitate our discussion of covariance-matrix regularization and regularized
discriminant analysis, we require the following notation. Let $\mathbb{R}_{a
  \times b}$ denote the matrix space of all $a \times b$ matrices over the real
field $\mathbb{R}$. Denote by $\bm I_m$ the $m \times m$ identity matrix, and
let $\bm 0_{m \times p}$ be the $m \times p$ matrix of zeros, such that $\bm
0_m$ is understood to denote $\bm 0_{m \times m}$. Define $\bm 1_m \in
\mathbb{R}_{m \times 1}$ as a vector of ones. Let $\bm A \oplus \bm B$ denote
the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in
\mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Let $\bm A^{T}$,
$\bm A^+$, and $\mathcal{N}(\bm A) = \{\bm z \in \mathbb{R}_{p \times 1} : \bm A
\bm z = \bm 0_{m \times 1}\}$ denote the transpose, the Moore-Penrose
pseudoinverse, and the null space of $\bm A \in \mathbb{R}_{m \times p}$,
respectively. Denote by $\mathbb{R}_{p \times p}^{>}$ the cone of real $p \times
p$ positive-definite matrices. Similarly, let $\mathbb{R}_{p \times p}^{\ge}$
denote the cone of real $p \times p$ positive-semidefinite matrices. Let
$V^{\perp} = \{\bm y \in \mathbb{R}_{p \times 1} : \bm y^{T} \bm v = 0$ for all
$\bm v \in V \}$ denote the orthogonal complement of a vector space $V \subset
\mathbb{R}_{p \times 1}$. For $c \in \mathbb{R}$, let $c^+ = 1/c$ if $c \ne 0$
and $0$ otherwise.

\subsection{Discriminant Analysis}

In discriminant analysis we wish to assign an unlabeled vector $\bm x \in
\mathbb{R}_{p \times 1}$ to one of $K$ unique, known classes by constructing a
classifier from $N$ training observations. Let $\bm x_i = (x_{i1}, \ldots,
x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots,
N)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$. We
assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution
$p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x |
\omega_k)$ is the probability density function (PDF) of the $k$th class and
$p(\omega_k)$ is the prior probability of class membership of the $k$th
class. We further assume that each of $p(\omega_k)$ is equal, $k = 1, \ldots,
K$.

Quadratic discriminant analysis is the optimal Bayesian decision rule with
respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the probability
density function of the multivariate normal distribution with known mean vectors
$\bm \mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrices $\bm
\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$. Because $\bm
\mu_k$ and $\bm \Sigma_k$ are typically unknown, we assign an unlabeled
observation $\bm x$ to class $\omega_k$ with the sample quadratic discriminant
analysis classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k} (\bm x - \xbar_k)^{T}\widehat{\bm
    \Sigma}_k^{-1}(\bm x - \xbar_k) + \log |\widehat{\bm \Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm \Sigma}_k$ are the maximum likelihood
estimators (MLEs) of $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume
further that $\bm \Sigma_k = \bm \Sigma$, $k = 1, \ldots, K$, then the pooled
sample covariance matrix $\widehat{\bm \Sigma}$ is substituted for $\widehat{
  \bm \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm \Sigma} = N^{-1} \sum_{k=1}^K n_k \widehat{\bm \Sigma}_k \label{eq:pooled-cov}
\end{align}
is the MLE for $\bm \Sigma$. In this case \eqref{eq:qda} reduces to the sample
linear discriminant analysis classifier, and the log-determinant is omitted
because it is constant across the $K$ classes.

The smallest eigenvalues of $\widehat{\bm \Sigma}_k$ and the directions
associated with their eigenvectors can highly influence \eqref{eq:qda}. In fact,
the eigenvalues of $\widehat{ \bm \Sigma}_k$ are well-known to be biased if $p
\ge n_k$ such that the smallest eigenvalues are underestimated
\citep{Seber:2004uh}. Moreover, if $p > n_k$, then rank$(\widehat{\bm \Sigma}_k)
\le n_k$, which implies that at least $p - n_k$ eigenvalues of $\widehat{\bm
  \Sigma}_k$ are zero. Furthermore, although more feature information is
available to discriminate among the $K$ classes, if $p > n_k$, \eqref{eq:qda} is
incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does not exist.

Several regularization methods, such as the methods considered by
\cite{Xu:2009fl}, \citet*{Guo:2007te}, and \cite{Mkhadri:1995jp} have been
proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$
so that \eqref{eq:qda} is calculable and has reduced variability in the standard
bias-variance tradeoff. A common form of the covariance-matrix regularization
applies a shrinkage factor $\gamma > 0$, so that
\begin{align}
	\widehat{\bm \Sigma}_k(\gamma) = \widehat{\bm \Sigma}_k + \gamma \bm I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation
\eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance
matrix $\widehat{\bm \Sigma}_k$ toward $\bm I_p$, thereby increasing the
eigenvalues of $\widehat{\bm \Sigma}_k$ by $\gamma$. Specifically, the zero
eigenvalues are replaced with $\gamma$, so that \eqref{eq:ridge-estimator} is
positive definite. For additional covariance-matrix regularization methods, see
\cite{Ramey:2013ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\section{High-Dimensional Regularized Discriminant Analysis}
\label{sec:rda}

Here, we define the \emph{HDRDA} classifier by first formulating the
covariance-matrix estimator $\widehat{\bm \Sigma}_k(\lambda)$ and demonstrating
its clear interpretation as a linear combination of the crossproducts of the
training observations centered by their respective class sample means. We define
the convex combination
\begin{align}
  \widehat{\bm \Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm \Sigma}_k + \lambda
  \widehat{\bm \Sigma}, \quad k = 1, \ldots, K,\label{eq:sig-lambda-alternative}
\end{align}
where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. By rewriting
\eqref{eq:sig-lambda-alternative} in terms of the observations $ \bm x_i$ $(i =
1, \ldots, N)$, each centered by its class sample mean, we attain a clear
interpretation of $\widehat{\bm \Sigma}_k(\lambda)$. That is,
\begin{align}
	\widehat{\bm \Sigma}_k(\lambda)
  &= \left( 1 - \lambda + \frac{\lambda n_k}{N} \right) \widehat{\bm \Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}}^K n_{k'} \widehat{\bm \Sigma}_{k'} \nonumber \\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N
  I(y_i = k) \bm x_i \bm x_i^{T} +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k)
  \bm x_i \bm x_i^{T} \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i^{T},\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of
each of the $N$ observations in estimating $\bm \Sigma_k$ from all $K$ classes
rather than $n_k$ observations from a single class. As a result, we can
interpret \eqref{eq:sig-lambda-alternative2} as a covariance-matrix estimator
that borrows from $\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov} to estimate $
\bm \Sigma_k$.

Below, we will show that the pooling operation is advantageous in increasing the
rank of each $\widehat{\bm \Sigma}_k(\lambda)$ from rank$(\widehat{\bm
  \Sigma}_k)$ to rank$(\widehat{\bm \Sigma})$ for $0 < \lambda \le 1$. Notice
that if $\lambda = 0$, then the observations from the remaining $K - 1$ classes
do not contribute to the estimation of $\bm \Sigma_k$, corresponding to
$\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights
$c_{ik}(\lambda)$ in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$,
corresponding to $\widehat{\bm \Sigma}$. For brevity, when $\lambda = 1$, we
define $\bm X = [\sqrt{c_{1k}(1)} \bm x_1^{T}, \ldots, \sqrt{c_{Nk}(1)} \bm
  x_N^{T}]^{T}$ such that $\widehat{\bm \Sigma} = N^{-1} \bm X^{T} \bm X$. Similarly,
for $\lambda = 0$, we define $\bm X_k = [\sqrt{c_{1k}(0)} \bm x_1^{T}, \ldots,
  \sqrt{c_{Nk}(0)} \bm x_N^{T}]^{T}$ such that $\widehat{\bm \Sigma}_k = n_k^{-1} \bm
X_k^{T} \bm X_k$. In Figure \ref{fig:hdrda-contours} we plot the contours of
five multivariate normal populations for $\lambda = 0$ with unequal covariance
matrices. As $\lambda$ approaches 1, the contours become more similar, resulting
in identical contours for $\lambda = 1$.

\begin{figure}
<<hdrda_contours, fig.width=8, fig.height=8, out.width='.33\\linewidth', fig.show='hold'>>=
rda_contours(lambda = 0)
rda_contours(lambda = 0.2)
rda_contours(lambda = 0.4)
rda_contours(lambda = 0.6)
rda_contours(lambda = 0.8)
rda_contours(lambda = 1)
@
\caption{Contours of five multivariate normal populations as a function of the
  pooling parameter $\lambda$.}
\label{fig:hdrda-contours}
\end{figure}

As we have discussed above, several eigenvalue adjustment methods have been
proposed to increase eigenvalues (approximately) equal to 0. To further improve
the estimation of $\bm \Sigma_k$ and to stabilize the estimator's inverse, we
define the eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\bm \Sigma}_k = \alpha_k \widehat{\bm \Sigma}_k(\lambda) + \gamma \bm I_p,\label{eq:hdrda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\gamma \ge 0$ is an eigenvalue-shrinkage
constant. Thus, the \emph{pooling} parameter $\lambda$ controls the amount
borrowed from $\widehat{\bm \Sigma}$ to estimate $\bm \Sigma_k$, and the
\emph{shrinkage} parameter $\gamma$ determines the degree of eigenvalue
shrinkage. The choice of $\alpha_k$ allows for a flexible formulation of
covariance-matrix estimators. For instance, if $\alpha_k = 1$, $k = 1, \ldots,
K$, then \eqref{eq:hdrda-cov} resembles \eqref{eq:ridge-estimator}. Similarly,
if $\alpha_k = 1 - \gamma$, then \eqref{eq:hdrda-cov} has a form similar to the
\emph{RDA} classifier from \cite{Friedman:1989tm}. Substituting
\eqref{eq:hdrda-cov} into \eqref{eq:qda}, we define the \emph{HDRDA} classifier
\begin{align}
	D_{HDRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)^{T}\tilde{\bm \Sigma}_k^{+}(\bm x - \xbar_k)  + \log |\tilde{\bm \Sigma}_k|. \label{eq:hdrda}
\end{align}
For $\gamma > 0$, $\tilde{\bm \Sigma}_k$ is nonsingular such that $\tilde{\bm
  \Sigma}_k^{-1}$ can be substituted for $\tilde{\bm \Sigma}_k^{+}$ in
\eqref{eq:hdrda}. If $\gamma = 0$, we explicitly set $|\tilde{\bm \Sigma}_k|$
equal to the product of the positive eigenvalues of $\tilde{\bm
  \Sigma}_k$. Following \cite{Friedman:1989tm}, we select $\lambda$ and $\gamma$
from a grid of candidate models via cross-validation. We provide an
implementation of \eqref{eq:hdrda} in the {\tt hdrda} function in the {\tt
  sparsediscrim} R package, available on CRAN.

The choice of $\alpha_k$ in \eqref{eq:hdrda-cov} is one of convenience and
allows the flexibility of various covariance-matrix estimators proposed in the
literature. In practice, we generally are not interested in estimating
$\alpha_k$ as it would incorporate $K$ additional tuning parameters to estimate
via cross-validation, which is counterproductive to our goal of improving the
\emph{RDA} classifier's efficiency. By setting $\alpha_k$ to an appropriate
constant, the \emph{HDRDA} covariance-matrix estimator includes or resembles a
large family of estimators. Notice that if $\alpha_k = 1$ and $\lambda = 1$,
\eqref{eq:hdrda-cov} is equivalent to the standard ridge-like covariance-matrix
estimator in \eqref{eq:ridge-estimator}, and various estimators proposed in the
literature can be obtained by selecting $\gamma$ accordingly. For instance, with
$\gamma = \tr\{ \widehat{\bm \Sigma} \} / \min(N, p)$, we obtain the estimator
from \cite{Srivastava:2007ww}.

Perhaps the most well-known of the ridge-like estimators is from
\cite{Friedman:1989tm}, who proposed regularized discriminant analysis (RDA) by
incorporating a weighted average of the sample covariance matrix $\widehat{ \bm
  \Sigma}_k$ and the pooled sample covariance matrix $\widehat{\bm \Sigma}$ to
estimate $\bm \Sigma_k$ with
\begin{align}
    \widehat{\bm \Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda) \quad
    (k =
      1, \ldots, K),\label{eq:sig-lambda}
      \end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda N$, $
\bm S_k = n_k \widehat{\bm \Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $ \bm
S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. If $\lambda = 0$,
\eqref{eq:sig-lambda} is equal to $\widehat{\bm \Sigma}_k$ used in
\eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces
to \eqref{eq:pooled-cov}. To further improve the estimation of $\bm \Sigma_k$
and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm}
has proposed the biased covariance-matrix estimator
\begin{align}
          \widehat{\bm \Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm
            \Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm
              \Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
          \end{align}
where $\gamma \in [0, 1]$ is a regularization parameter that controls the
shrinkage of \eqref{eq:sig-rda} towards $\bm I_p$ and is weighted by the average
of the eigenvalues of \eqref{eq:sig-lambda}. Finally, the RDA classifier is
established by substituting \eqref{eq:sig-rda} into \eqref{eq:qda}.

Our proposed \emph{HDRDA} classifier resembles \eqref{eq:sig-rda} but improves
the interpretation of the pooling parameter $\lambda$ as discussed
above. Moreover, as we discuss below, our formulation of \emph{HDRDA} classifier
lends itself to rapid estimation in high-dimensional classification.

\section{Properties of the HDRDA Classifier}
\label{sec:hdrda-properties}

Next, we establish properties of the covariance-matrix estimator and the
decision rule employed in the \emph{HDRDA} classifier. By doing so, we
demonstrate that \eqref{eq:hdrda} lends itself to a more efficient
calculation. We decompose \eqref{eq:hdrda} into a sum of two components, where
the first summand consists of matrix operations applied to low-dimensional
matrices and the second summand corresponds to the null space of $\widehat{\bm
  \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix operations
performed on the null space of $\widehat{\bm \Sigma}$ yield constant quadratic
forms across all classes and can be omitted. For $p \gg N$, the constant
component involves determinants and inverses of high-dimensional matrices, and
by ignoring these calculations, a substantial reduction in computational costs
is achieved. Furthermore, a byproduct is that adjustments to the associated
eigenvalues have no effect on \eqref{eq:hdrda}. Lastly, we utilize the singular
value decomposition to calculate efficiently the eigenvalue decomposition of
$\widehat{\bm \Sigma}$, further reducing the computational costs of the
\emph{HDRDA} classifier.

First, we require the following relationship regarding the null spaces of
$\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{ \bm
  \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the MLEs of $\bm
\Sigma_k$ and $\bm \Sigma$, respectively. Let $\widehat{\bm \Sigma}_k(\lambda)$
be defined as in \eqref{eq:sig-lambda-alternative}. Then,
$\mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\} \subset \mathcal{N}(\widehat{
  \bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$, $k = 1, \ldots, K$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\}$ for some $k = 1,
\ldots, K$. Hence, $0 = \bm z^{T} \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 -
\lambda) \bm z^{T} \widehat{\bm \Sigma}_k \bm z + \lambda \bm z^{T} \widehat{
  \bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in
\mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{
  \bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular,
we have that $\mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\} \subset
\mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in
\mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z^{T}
\widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z^{T} \widehat{
  \bm \Sigma}_k \bm z$, which implies that $\bm z \in \mathcal{N}(\widehat{
  \bm \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times
  p}^{\ge}$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset
\mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative
expression for $\tilde{\bm \Sigma}_k$ in terms of the matrix of eigenvectors of
$\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U^{T}$ be the
eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p
  \times p}^{\ge}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$
with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, $\bm D_q \in \mathbb{R}_{q \times
  q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of
$\widehat{\bm \Sigma}$, the columns of $\bm U \in \mathbb{R}_{p \times p}$ are
the corresponding orthonormal eigenvectors of $\widehat{\bm \Sigma}$, and
rank$(\widehat{\bm \Sigma}) = q$. Then, we partition $\bm U = (\bm U_1,
\bm U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in
\mathbb{R}_{p \times (p - q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U^{T}$ be the eigendecomposition of
$\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) =
q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \gamma \bm I_{p-q})\bm U^{T}, \quad
  k = 1, \ldots, K,\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= \alpha_k \{(1 - \lambda) \bm U_1^{T} \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm I_{q}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $\bm U_2$ span the null space
of $\widehat{\bm \Sigma}_k$, which implies that $\widehat{\bm \Sigma}_k \bm U_2
= \bm 0_{p \times (p - q)}$. Hence, $\bm U^{T} \widehat{\bm \Sigma}_k \bm U =
\bm U_1^{T} \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm 0_{p-q}$, $k = 1, \ldots,
K$. Thus, $\bm U^{T} \tilde{\bm \Sigma}_k \bm U = \alpha_k \{(1 - \lambda) \bm
U^{T} \widehat{\bm \Sigma}_k \bm U + \lambda \bm D\} + \gamma \bm I_p$, and
\eqref{eq:rda-matrix} holds because $\bm U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we
have the following corollary.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$,
  rank$\{\widehat{\bm \Sigma}_k(\lambda)\} = q$, $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we
increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ to $q \approx N$ if
$\lambda \ne 0$. Next, we provide an essential result that enables us to prove
that \eqref{eq:hdrda} is invariant to adjustments to the eigenvalues of $\tilde{
  \bm \Sigma}_k$ corresponding to the null space of $\widehat{\bm \Sigma}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times
  1}$, $\bm U_2^{T} (\bm x - \xbar_k) = \bm U_2^{T} (\bm x - \xbar_{k'})$ $(1 \le k,
k' \le K)$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$.
Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that
$\bm U_2^{T} \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma
  1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{ \bm
  \Sigma})$ $(i = 1, \ldots, N)$, $\bm U_2^{T} \bm x_i = \bm 0_{p-q}$.  Hence,
$\bm 0_{p-q} = \sum_{i=1}^N \beta_i \bm U_2^{T}\bm x_i = \bm U_2^{T}(\xbar_k -
\xbar_{k'})$, where $\beta_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i =
k') n_k \}$. Therefore, $\bm U_2^{T} (\bm x - \xbar_k) = \bm U_2^{T} (\bm x -
\xbar_{k'})$.
\end{proof}

We now present our main result where we decompose \eqref{eq:hdrda} and show that
the term requiring the largest computational costs does not contribute to the
classification of an unlabeled observation performed using Lemma
\ref{lemma:RDA-constant-term}. Hence, we reduce \eqref{eq:hdrda} to an
equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ and $\bm W_k$ be defined as in \eqref{eq:rda-matrix}
and \eqref{eq:Wk}, respectively, and let $\bm U_1$ be defined as above. Then,
the decision rule in \eqref{eq:hdrda} is equivalent to
	\begin{align}
		D_{HDRDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x - \xbar_k) + \log | \bm W_k |. \label{eq:hdrda-decomposed}
	\end{align}
\end{thm}
\begin{proof}
From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{+} = \bm U(
\bm W_k^{-1} \oplus \gamma^{+} \bm I_{p-q} )\bm U^{T}$ and $|\tilde{\bm \Sigma}_k| =
\gamma^{p-q}| \bm W_k |$, $k = 1, \ldots, K$. Therefore, for all $\bm x \in
\mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)^{T} \tilde{\bm \Sigma}_k^{+}(\bm x - \xbar_k)  + \log |\tilde{\bm \Sigma}_k| &= (\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x - \xbar_k)\\
	&+ \gamma^{+} (\bm x - \xbar_k)^{T} \bm U_2 \bm U_2^{T} (\bm x - \xbar_k) + \log | \bm W_k |\\
  &+ (p - q) \log \gamma.
	\end{align*}
Because $\gamma$ is constant for $k = 1, \ldots, K$, we can omit the $(p - q)
\log \gamma$ term and particularly avoid the calculation of $\log 0$ for $\gamma
= 0$. Then, the proof follows from Lemma \ref{lemma:RDA-constant-term} because
$\bm U_2^{T} (\bm x - \xbar_k)$ is constant for $k = 1, \ldots, K$.
\end{proof}

As we can see, we are able to avoid the time-consuming inverses and determinants
of $p \times p$ covariance matrices in \eqref{eq:hdrda} and instead calculating
these same operations on $\bm W_k \in \mathbb{R}_{q \times q}$ in
\eqref{eq:hdrda-decomposed}. The substantial computational improvements arise
because our proposed classifier in \eqref{eq:hdrda} is invariant to the term
$\bm U_2$, thus yielding an equivalent classifier in \eqref{eq:hdrda-decomposed}
with a substantial reduction in computational complexity. Here, we demonstrate
that the computational efficiency in calculating the inverse and determinant of
$\bm W_k$ can be further improved via standard matrix operations by showing that
the inverses and determinants of $\bm W_k$ can be performed on matrices of size
$n_k \times n_k$.

\begin{proposition}\label{proposition:hdrda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$
and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - n_k^{-1} \alpha_k(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1^{T} \bm X_k^{T} \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},\label{eq:W_k_inv}
    \intertext{where}
    \bm Q_k &= \bm I_{n_k} + n_k^{-1} \alpha_k(1 - \lambda) \bm X_k \bm U_1 \bm \Gamma_k^{-1} \bm U_1^{T} \bm X_k^{T}\label{eq:Q_k}
    \intertext{and}
    \bm \Gamma_k &= \alpha_k \lambda \bm D_q + \gamma \bm I_q\label{eq:Gamma_k}.
	\end{align}
\end{proposition}
\begin{proof}
First, we write $\bm W_k = n_k^{-1} \alpha_k (1 - \lambda) \bm U_1^{T} \bm
X_k^{T} \bm X_k \bm U_1 + \bm \Gamma_k$. To calculate $|\bm W_k|$, we apply
Theorem 18.1.1 from \cite{Harville:2008wja}, which states that $|\bm A + \bm B
\bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} + \bm C \bm A^{-1} \bm B|$. Thus,
setting $\bm A = \bm \Gamma_k$, $\bm B = \alpha_k (1 - \lambda) \bm U_1^{T} \bm
X_k^{T}$, $\bm T = \bm I_{n_k}$, and $\bm C = \bm X_k \bm U_1$, we have $|\bm
W_k| = |\bm \Gamma_k| |\bm Q_k|$. Similarly, \eqref{eq:W_k_inv} follows from the
well-known Sherman-Woodbury formula \citep[Theorem 18.2.8]{Harville:2008wja} by
noting that $(\bm A + \bm B \bm T \bm C)^{-1} = \bm A^{-1} - \bm A^{-1} \bm B
(\bm T^{-1} + \bm C \bm A^{-1} \bm B)^{-1} \bm C \bm A^{-1}$.
\end{proof}

The application of the Sherman-Woodbury formula requires that $\bm \Gamma_k$ be
nonsingular. This assumption is not met when $(\lambda, \gamma) = (0, 0)$
because $\bm \Gamma_k = \bm 0_q$. In this case we instead use the formulation in
\eqref{eq:hdrda-decomposed}. Also, notice that if $\alpha_k$ is constant across
the $K$ classes, then $\bm \Gamma_k$ in \eqref{eq:Gamma_k} is independent of
$k$. Consequently, $|\bm \Gamma_k|$ is constant across the $K$ classes and need
not be calculated in \eqref{eq:hdrda-decomposed}.

\subsection{Model Selection}

Thus far, we have presented the \emph{HDRDA} classifier and its properties that
facilitate an efficient calculation of the decision rule. Here, we describe an
efficient model-selection procedure along with pseudocode in Algorithm
\ref{alg:hdrda_model_selection} to select the optimal tuning-parameter estimates
from the Cartesian product of candidate values $\{\lambda_g\}_{g=1}^G \times
\{\gamma_h\}_{h=1}^H$. We estimate the $V$-fold cross-validation error rate for
each candidate pair and select $(\widehat{\lambda}, \widehat{\gamma})$ that
attains the minimum error rate. To calculate the $V$-fold cross-validation, we
partition the original training data into $V$ mutually exclusive and exhaustive
folds that have approximately the same number of observations. Then, for $v = 1,
\ldots, V$, we classify the observations in the $v$th fold by training a
classifier on the remaining $V - 1$ folds. We then calculate the
cross-validation error as the proportion of misclassified observations across
the $V$ folds.

\IncMargin{1em}
\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}

  \Input{
    Data matrix $\bm X$\\
    Parameter grid $\{\lambda_g\}_{g=1}^G \times \{\gamma_h\}_{h=1}^H$
  }
  \Output{Optimal Estimates $(\hat{\lambda}, \hat{\gamma})$}
  \BlankLine
  \For{$v \leftarrow 1$ \KwTo $V$} {
    Partition $\bm X$ into $\bm X_{train} \in \mathbb{R}_{N \times p}$ and
    $\bm X_{test} \in \mathbb{R}_{N_T \times p}$

    \For{$k\leftarrow 1$ \KwTo $K$}{
      Extract $\bm X_k \in \mathbb{R}_{n_k \times p}$ from $\bm X_{train}$

      Compute sample mean $\xbar_k$ from $\bm X_k$

      Center $\bm X_k \leftarrow \bm X_k - \bm 1_{n_k} \xbar_k^T$
    }

    $\bm X_c \leftarrow [\bm X_1^T, \ldots, \bm X_K^T]^T$

    Compute the compact SVD $\bm X_c = \bm M_q \bm D_q \bm U_1^T$

    Transform $\bm X_c \leftarrow \bm X_c \bm U_1$

    Transform $\bm X_{test} \leftarrow \bm X_{test} \bm U_1$

    \For{$k \leftarrow 1$ \KwTo $K$} {
      Extract $\bm X_k \in \mathbb{R}_{n_k \times q}$ from $\bm X_c$

      Recompute sample mean $\xbar_k$ from $\bm X_k$
    }

    \For{$(\lambda, \gamma) \in \{\lambda_g\}_{g=1}^G \times \{\gamma_h\}_{h=1}^H$} {
      \For{$k \leftarrow 1$ \KwTo $K$} {
        Compute $\bm Q_k$ using \eqref{eq:Q_k}

        Compute $\bm \Gamma_k$ using \eqref{eq:Gamma_k}

        Compute $\bm W_k^{-1}$ using \eqref{eq:W_k_inv}

        Compute $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$

        Compute $(\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x -
        \xbar_k) + \log | \bm W_k |$ for each row $\bm x$ of $\bm X_{test}$
      }
      Classify test observations $\bm X_{test}$ using \eqref{eq:hdrda-decomposed}

      Compute the number of misclassified test observations $\#\{\text{Error}_v(\lambda, \gamma)\}$
    }
   }
  Compute $\widehat{\text{Error}}(\lambda, \gamma) = N^{-1} \sum_{v=1}^V \#\{\text{Error}_v(\lambda, \gamma)\}$

  Report optimal $(\hat{\lambda}, \hat{\gamma}) \leftarrow \argmin_{(\lambda, \gamma)} \widehat{\text{Error}}(\lambda, \gamma)$
  \caption{Model selection for the \emph{HDRDA} classifier}
  \label{alg:hdrda_model_selection}
\end{algorithm}
\DecMargin{1em}

A primary contributing factor to the efficiency of Algorithm
\ref{alg:hdrda_model_selection} is our usage of the compact singular value
decomposition (SVD) (sometimes called the reduced or fast SVD). Rather than
computing the eigenvalue decomposition of $\widehat{\bm \Sigma}$ to obtain $\bm
U_1$, we instead obtain $\bm U_1$ by computing the eigendecomposition of a much
smaller $N \times N$ matrix when $p \gg N$
\cite[Chapter~18.3.5]{Hastie:2008dt}. Applying the SVD, we decompose $\bm X_c =
\bm M \bm \Delta \bm U^{T}$, where $\bm M \in \mathbb{R}_{N \times p}$ is
orthogonal, $\bm \Delta \in \mathbb{R}_{p \times p}^{\ge}$ is a diagonal matrix
consisting of the singular values of $\bm X_c$, and $\bm U \in \mathbb{R}_{p
  \times p}$ is orthogonal. Recalling that $\widehat{\bm \Sigma} = N^{-1} \bm
X_c^{T} \bm X_c$, we have the eigendecomposition $\widehat{\bm \Sigma} = \bm U
\bm D \bm U^{T}$, where $\bm U$ is the matrix of eigenvectors of $\widehat{\bm
  \Sigma}$ and $\bm D = N^{-1} \bm \Delta$ is the diagonal matrix of eigenvalues
of $\widehat{\bm \Sigma}$. Now, we can obtain $\bm M$ and $\bm D$ efficiently
from the eigenvalue decomposition of the $N \times N$ matrix $\bm X_c \bm
X_c^{T} = \bm M \bm D \bm M^{T}$. Next, we compute $\bm U = \bm X_c^{T} \bm M
\bm D^{+/2}$, where $\bm D^{+/2} = \bm D_q^{-1/2} \oplus \bm 0_{N-q}$. We then
determine $q$, the number of numerically nonzero eigenvalues present in $\bm D$,
by calculating the number of eigenvalues that exceeds some tolerance value, say,
$1 \times 10^{-6}$. After calculating $q$, we extract $\bm U_1$ by retaining
only the first $q$ columns of $\bm U$.

As a result of the compact SVD, we need calculate $\bm X_c \bm U_1$ only once
per cross-validation fold, requiring $O(p q N) \approx (p N^2)$ calculations.
Hence, the computational costs of expensive calculations, such as matrix
inverses and determinants, are greatly reduced because they are performed in the
$q$-dimensional subspace. Similarly, we reduce the dimension of the test data
set by calculating $\bm X_{test} \bm U_1$ once per fold. Conveniently, we see
that the most costly computation involved in $\bm Q_k$ and $\bm W_k^{-1}$ is
$\bm X_k \bm U_1$, which can be extracted from $\bm X_c \bm U_1$. Thus, after
the initial calculation of $\bm X_c \bm U_1$ per cross-validation fold, $\bm
Q_k$ requires $O(n_k q^2)$ operations. Because $\bm Q_k \in \mathbb{R}_{n_k
  \times n_k}$, both its determinant and inverse require $O(n_k^3)$
operations. Consequently, $\bm W_k^{-1}$ requires $O(n_k q^2)$ operations. Also,
the inverse of the diagonal matrix $\bm \Gamma_k^{-1} \in \mathbb{R}_{q \times
  q}$ requires $O(q)$ operations. Finally, $| \bm W_k |$ requires $O(n_k^3)$
operations.

The expressions given in Proposition \ref{proposition:hdrda-W_k} also expedite
the selection of $\lambda$ and $\gamma$ via cross-validation because the most
time-consuming matrix operation involved in computing $\bm W_k^{-1}$ and $|\bm
W_k|$ is $\bm X_k \bm U_1 \in \mathbb{R}_{n_k \times q}$, which is independent
of $\lambda$ and $\gamma$. The subsequent operations in calculating $\bm
W_k^{-1}$ and $|\bm W_k|$ can be simply updated for different pairs of $\lambda$
and $\gamma$ without repeating the costly computations. When calculating the
discriminant scores of each test observation in $\bm X_{test}$, rather than
calculating each score individually, we can more efficiently calculate the
quadratic forms via a single matrix operation. That is, rather than calculating
$(\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x - \xbar_k)$
individually for each row $\bm x$ of $\bm X_{test}$, we can instead calculate
$(\bm X_{test} - \xbar_k \bm 1_k')' \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm
X_{test} - \xbar_k \bm 1_k')$. The diagonal elements of the resulting matrix
contain the individual quadratic form of each test observation, $\bm x_t$.

\section{Classification Study}
\label{sec:sims}

In this section, we compare our proposed classifier with four classifiers
recently proposed for small-sample, high-dimensional data along with the random
forest from \cite{Breiman:2001fb} using version 3.2.2 of the open-source
statistical software {\tt R}. Within our study, we included penalized linear
discriminant analysis from \cite{Witten:2011kc}, implemented in the {\tt
  penalizedLDA} package. We also considered shrunken centroids regularized
discriminant analysis from \cite{Guo:2007te} in the {\tt rda} package. Because
the {\tt rda} package does not perform the authors' ``Min-Min'' rule
automatically, we applied this rule within our {\tt R} code. We included two
modifications of diagonal linear discriminant analysis from \cite{Tong:2012hw}
and \cite{Pang:2009ik}, where the former employs an improved mean estimator and
the latter utilizes an improved variance estimator. Both classifiers are
available in the {\tt sparsediscrim} package. Finally, we incorporated the
random forest as a benchmark based on the findings of
\cite{FernandezDelgado:2014ul}, who concluded that the random forest is often
superior to other classifiers in benchmark studies. We used the implementation
of the random-forest classifier from the {\tt randomForest} package with 250
trees and 100 maximum nodes. For each classifer we explicitly set prior
probabilities as equal, if applicable. All other classifier options were set to
their default settings. Below, we refer to each classifier by the first
author's surname.

For the \emph{HDRDA} classifier in \eqref{eq:hdrda-decomposed}, we examined the
classification performance of two models. For the first \emph{HDRDA} model, we
set $\alpha_k = 1$, $k = 1, \ldots, K$, so that the covariance-matrix estimator
\eqref{eq:hdrda-cov} resembled \eqref{eq:ridge-estimator}. We estimated
$\lambda$ from a grid of 21 equidistant candidate values between 0 and 1,
inclusively. Similarly, we estimated $\gamma$ from a grid consisting of the
values $10^{-1}, \ldots, 10^4$, and $10^5$. We selected optimal estimates of
$\lambda$ and $\gamma$ using $V$-fold cross-validation with $V = 10$
\citep{Hastie:2008dt}. For the second model, we set $\alpha_k = 1 - \gamma$, $k
= 1, \ldots, K$, to resemble \citeauthor{Friedman:1989tm}'s
parameterization. We estimated both $\lambda$ and $\gamma$ from a grid of 21
equidistant candidate values between 0 and 1, inclusively.

\subsection{Simulation Study}

In this section we compare the competing classifiers using the simulation design
from \cite{Guo:2007te}. This design is widely used within the high-dimensional
classification literature including the studies by \cite{Ramey:2013ji} and
\cite{Witten:2011kc}. First, we consider the block-diagonal covariance
matrix from from \cite{Guo:2007te}
\begin{align}
  \bm\Sigma_k = \bm\Sigma^{(\rho_k)} \oplus \bm\Sigma^{(-\rho_k)} \oplus \ldots \oplus \bm\Sigma^{(\rho_k)},\label{eq:block-sigma}
\end{align}
where the $(i,j)$th entry of the block matrix $\bm\Sigma^{(\rho_k)} \in
\mathbb{R}_{100 \times 100}$ is
\begin{align*}
\bm\Sigma_{ij}^{(\rho_k)} = \{ \rho_k^{|i - j|} \}_{1 \le i,j \le 100}.
\end{align*}
The block-diagonal covariance structure in \eqref{eq:block-sigma} resembles
gene-expression data: within each block of pathways, genes are correlated, and
the correlation decays as a function of the distance between any two genes. The
autocorrelation $\rho_k$ alternates between a positive and a negative value from block
to block. The original design from \cite{Guo:2007te} comprised two $p$-dimensional
multivariate normal populations with a common block-diagonal covariance
matrix where $\bm\Sigma_1 = \bm\Sigma_2$.

Although the design is indeed standard, the simulation configuration lacks
artifacts observed commonly in real data, such as skewness and extreme
outliers. As a result, we wished to investigate the effect of outliers on the
high-dimensional classifiers. To do this, we generalized the block-diagonal
simulation configuration by sampling from a $p$-dimensional multivariate
contaminated normal distribution.  Denoting the PDF of the $p$-dimensional
multivariate normal distribution by $N_p(\bm x | \bm \mu, \bm \Sigma)$, we write
the PDF of the $k$th class as
\begin{align}
  p(\bm x | \omega_k) = (1 - \epsilon) N_p(\bm x | \bm \mu_k, \bm \Sigma_k) + \epsilon N_p(\bm x | \bm \mu_k, \omega \bm \Sigma_k),\label{eq:contaminated-pdf}
\end{align}
where $\epsilon \in [0, 1]$ is the probability that an observation is
contaminated (i.e., drawn from a distribution with larger variance) and $\omega
> 1$ scales the covariance matrix $\bm \Sigma_k$ to increase the extremity of
outliers. For $\epsilon = 0$, we have the benchmark block-diagonal simulation
design from \cite{Guo:2007te}. As $\epsilon$ is increased, the average number of
outliers is increased. In our simulation, we considered the values of $\epsilon
= 0$, $0.05$, $\ldots$, $0.50$. We also set $\omega = 100$.

We generated $K=3$ populations from \eqref{eq:contaminated-pdf} with $\bm
\Sigma_k$ given in \eqref{eq:block-sigma}. We set the mean vector of class 1 to
$\bm \mu_1 = \bm 0_p$. Next, comparable to \cite{Guo:2007te}, the first 100
features of $\bm \mu_2$ were 1/2, while the rest were set to 0, i.e., $\bm \mu_2
= (\underbrace{1/2, \ldots, 1/2}_{100}, \underbrace{0, \ldots, 0}_{p -
  100})$. For simplicity, we defined $\bm \mu_3 = -\bm \mu_2$. The three
populations differed in their mean vectors in the first 100 features
corresponding to the first block. No difference in the means occurred in the
remaining blocks.

From each of the $K=3$ populations, we sampled 25 training observations ($n_k =
25$ for all $k$) and 10,000 test observations. After training each classifier on
the training data, we classified the test data sets and computed the proportion
of mislabeled test observations to estimate the classification error rate for
each classifier. Repeating this process 500 times, we computed the average of
the error-rate estimates for each classifier. We allowed the number of features
to vary from $p = 100$ to $p = 500$ in increments of 100 to examine the
classification accuracy as the feature dimension increased while maintaining a
small sample size. Furthermore, although \cite{Guo:2007te} consider $\rho_k =
0.9$ for all $k$, we considered $\rho_1 = 0.1$, $\rho_2 = 0.5$, and $\rho_3 = 0.9$
to explore the more realistic assumption of unequal covariance matrices.

\subsubsection{Results}

As expected, regardless of the values of $p$, $\rho$, or $\bm \mu$, as the
contaminated probability increases, the average error rate increases.  Also, as
can be expected, the average error rates were typically higher for $\bm \mu=0.5$
than for $\bm \mu=1$. The difference in the average error rates for the methods
of Pang, Tong, and Witten all yielded increasingly higher average error rates
than HDRDA ridge, HDRDA complex, Guo, or random forests as the contaminated
probability increased. The correlation coefficient appears to have had little,
if any, effect on the average error rates for any of the considered classifiers.

Regardless of the value of $rho$, $p$, or $\bm \mu$, random forests consistently
yielded the highest average error rate, and the average error rate for random
forests was much more pronounced when $\bm \mu=0.5$. However, as the
contaminated probability increased, random forests became increasingly
competitive, and, in some cases, e.g, $\bm \mu=0.5$, $p=100$, and contaminated
probability greater than 0.05 yielded the lowest average error rate.

Pang and Tong's methods yielded practically the same and consistently the worst
average error rates when the contaminated probability was greater than 0,
regardless of the value of $\bm \mu$, $p$, or $\rho$. Witten's method performed
only slightly better as the contaminated probability increased, however, when
$\bm \mu = 0.5$, Witten's method became increasingly competitive with Pang and
Tong for yielding the highest average error rate as $p$ increased.

Guo's method always outperformed Pang, Witten, and Tong, but never outperformed
the HDRDA methods. For the considered parameter configurations, Guo's method
performed similarly to random forests when the contaminated probability was
greater than 0 and yielded similar results to HDRDA ridge, HDRDA convex when the
contaminated probability was equal to 0. However, as the contaminated
probability increased, Guo's method was often outperformed by HDRDA ridge, HDRDA
convex, and random forests.

HDRDA ridge and HDRDA complex yielded the lowest average error rates, regardless
of the value of $\bm \mu$, $p$, or $\rho$, with HDRDA ridge typically
outperforming HDRDA complex when the contaminated probability was greater than
0. HDRDA ridge only seemed to have competition with random forests when the
contaminated probability was 0.20. Moreover, neither HDRDA ridge nor HDRDA
complex seem to be affected by $p$.

\subsection{Application to Gene Expression Data}

We compared the \emph{HDRDA} classifer to the five competing classifiers on four
benchmark gene-expression microarray data sets. We evaluated the classification
performance of each classifier by randomly partitioning the data set under
consideration such that $2/3$ of the observations were allocated as training
data and the remaining $1/3$ of the observations were allocated as a test data
set. After training each classifier on the training data, we classified the test
data sets and computed the proportion of mislabeled test observations to
estimate the classification error rate for each classifier. Repeating this
process 250 times, we computed the average of the error-rate estimates for each
classifier. Here, we provide a concise description of each high-dimensional
data set examined in our classification study.

\subsubsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired peripheral blood mononuclear cells (PBMC)
through hybridization to microarrays from 127 individuals resulting in 22,283
sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD),
and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to
improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using
the PBMC-based gene expression signature of a patient.

\subsubsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired 105 gene-expression samples of 10 types of soft
tissue tumors through an oligonucleotide microarray, including 16 samples of
synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma (MLS), 3
samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS), 15
samples of dedifferentiated liposarcoma (DDLS), 15 samples of myxofibrosarcoma
(MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of malignant nerve sheathe
tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21 samples of malignant
fibrous histiocytoma (MFH). \cite{Nakayama:2007fl} determined from their data
that these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma,
WDLS, and part of DDLS; (4) Spindle cell and pleomorophic sarcomas including
DDLS, MFS, LMS, MPNST, FS, and MFH. Following \cite{Witten:2011kc}, we restrict
our analysis to the five tumor types having at least 15 observations.

\subsubsection{\cite{Shipp:2002ka} Data Set}

Approximately 30-40\% of adult non-Hodgkin lymphomas are diffuse large B-cell
lymphomas (DLBCLs) \cite{Shipp:2002ka}.  However, only a small proportion of
DLBCL patients are cured with modern chemotherapeutic regimens.  Several models
have been proposed, such as the International Prognostic Index (IPI), to
determine a patient's curability.  These models rely on clinical covariates,
such as age, to determine if the patient can be cured and are often
ineffective. \cite{Shipp:2002ka} have argued that more effective means are
desired to determine a patient's curability. The authors measured 6,817 gene
expression levels from 58 DLBCL patient samples with customized cDNA
(lymphochip) microarrays to investigate the curability of patients treated with
cyclophosphamide, adriamycin, vincristine, and prednisone (CHOP)-based
chemotherapy.  Among the 58 DLBCL patient samples, 32 are from cured patients
while 26 are from patients with fatal or refractory disease.

\subsubsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from
surgery patients between 1995 and 1997. The authors used oligonucleotide
microarrays containing probes for approximately 12,600 genes and expressed
sequence tags. They have reported that 102 of the radical prostatectomy
specimens are of high quality: 52 prostate tumor samples and 50 non-tumor
prostate samples.

\subsection{Classification Results}

Despite the claims of \cite{FernandezDelgado:2014ul} that the random forest
classifier is typically superior, we observed that the classifiers designed for
high-dimensional data were indeed superior.


For the microarray data sets, we computed error-rate estimates for the competing
classifiers after reducing the dimension of the data sets with feature selection
to $d = 100, 200, \ldots, 1000$, similar to the values of $d$ considered by
\cite{Xu:2009fl}. To compare misclassification rates between different values of
$d$, we applied each value of $d$ considered to the same random allocation of
training and test data. Hence, for a given random partition, the set of features
selected for a smaller value of $d$ is a proper subset of the features selected
for a larger value of $d$. We report the average of the error-rate estimates for
each value of $d$ in Figure \ref{fig:error-rates}. Approximate standard errors
were no greater than \Sexpr{max_se}.

The \emph{HDRDA} classifier was superior in classification performance for the
majority of the simulations. For the Burczysnki, Shipp, and Singh data sets, our
classifier outperformed its competitors for all values of $d$ examined. For the
Nakayama data set, the \emph{HDRDA} classifier \eqref{eq:hdrda-decomposed} was
slightly outperformed by the Dudoit, Pang, and Tong classifiers for small values
of $d$ but attained classification rates superior to its competitors for larger
values of $d$. Furthermore, Figure \ref{fig:error-rates} emphasizes two
strengths of our classifier: reliability of the classifier for relatively small
$d$ and stability of the classifier for large $d$. In contrast to the other
classifiers, the \emph{HDRDA} classifier exhibited less variability in its
misclassification estimates for small values of $d$ and yielded superior
classification accuracy as $d$ increased and was hardly effected by the
inclusion of more variables in the data sets. We attribute the apparent
stability of the error-rate estimates of our classifier for large values of $d$
to its novel dimension-reduction technique.

The average error-rate estimates for the Dudoit, Pang, Tong, and Witten
classifiers were comparable for all values of $d$ examined. Furthermore, the
error-rate estimates of the Dudoit, Pang, and Tong classifiers were
approximately equal throughout our study, and the Witten classifier yielded
approximately equal error-rate estimates for the Burczysnki, Shipp, and Singh
data sets. Furthermore, the error-rate estimates for the Dudoit and Tong
classifiers were nearly identical, which suggests that the improved mean
estimator employed by \cite{Tong:2012hw} provided little improvement to the
classification performance of diagonal linear discriminant analysis.

The classification performance of the the Dudoit, Pang, Tong, and Witten
classifiers for the Shipp and Singh data sets revealed the inability of these
methods to handle larger values of $d$. After closer examination of the
classification studies performed in the authors' original papers, we noticed
that classification accuracy estimates were reported only for small values of
$d$, thereby ignoring the inclusion of a larger number of variables. Hence, we
advise practioners to consider filtering a large number of features before
employing these methods.

The popular Guo classifier is most competitive over large values of $d$ for the
Shipp and Singh data sets. However, for the Burczynski and Nakayama data sets,
the implementation of the Guo method in the {\tt rda} package often classified
every test observation into a single class, thus yielding larger error rates
than its competitors. Furthermore, the Guo classifier exhibited unreliable
classification performance for smaller values of $d$ and required a larger
subset of the features to perform competitively in our study.
\begin{figure}
<<error_rates, fig.width=8, fig.height=8, out.width='.65\\linewidth'>>=
print(NULL)
@
\caption{Averages of the 1000 error-rate estimates of the competing classifiers
  for each data set as a function of $d$. Approximate standard errors are no
  greater than \Sexpr{max_se}.}
\label{fig:error-rates}
\end{figure}
Next, we examine the optimal estimates of the tuning parameters for the
\emph{HDRDA} classifier. For brevity, we provide graphical summaries of the
optimal estimates of $\lambda$ and $\gamma$ in \ref{suppB}. For each of the four
data sets, the distribution of the optimal values of $\lambda$ selected through
cross-validation often had a notable bathtub shape, resembling a Beta
distribution with both shape parameters less than unity. Specifically, the
optimal values of $\lambda$ were typically near 0 or near 1, corresponding to
quadratic and linear discriminant analysis, respectively. For instance, with
smaller values of $d$ applied to the Shipp data set, the range of optimal values
of $\lambda$ was typically no larger than 0.05, indicating there were a large
number of training data sets for which we can safely assume the covariance
matrices are unequal. As $d$ increased, the majority of the optimal values of
$\lambda$ were selected as unity, thereby an increased number of variables
resulted in the \emph{HDRDA} classifier preferring the more parsimonious
assumption of equal covariance matrices. In this instance we can see the
flexibility of our proposed classifier to employ the more realistic assumption
of unequal class covariance matrices, but if optimal, the \emph{HDRDA}
classifier \eqref{eq:hdrda-decomposed} can utilize the more parsimonious
assumption of equal covariance matrices.

\section{Discussion}
\label{sec:discussion}

We have compared our proposed \emph{HDRDA} classifier to several recently
proposed classifiers, including the Dudoit, Pang, Tong, and Guo classifiers,
each of which assumes independence of the features conditional on class
membership. Despite the rapid computational performance of diagonal classifiers
and the reduction in the number of parameters to estimate, \cite{Mai:2012bf} and
\cite{Fan:2012iq} have noted that diagonal classifiers can often yield inferior
classification performance to other classification methods. We have shown that
the \emph{HDRDA} classifier often yields superior classification accuracy to the
diagonal classifiers considered, confirming the assertions of \cite{Mai:2012bf}
and \cite{Fan:2012iq}. Furthermore, the implementation of
\eqref{eq:hdrda-decomposed} in the {\tt sparsediscrim} R package was comparable
to that of the diagonal classifiers in terms of computational runtime. Moreover,
when preliminary variable selection was applied to the data in our
classification study, the time to perform model selection with our proposed
classifier was nearly instantaneous. Hence, our proposed \emph{HDRDA} classifier
is competitive in terms of classification accuracy for modern small-sample,
high-dimensional data sets and requires substantially less computing time than
standard regularized classifiers, such as regularized discriminant analysis
proposed by \cite{Friedman:1989tm}.

Given the \emph{RDA} classifier has been shown to have excellent performance in
the high-dimensional setting \citep{Webb:2011vu} but is limited by its
computationally intense model selection procedure, our work enables the
\emph{RDA} classifier to be employed with small-sample, high-dimensional data in
practice. This is reassuring because the \emph{RDA} classifier remains widely
popular in the literature. In fact, variants of the RDA classifier have been
applied to microarray data \citep{Ching:2012fu,Li:2012ev,Tai:2007bk,Guo:2007te},
facial recognition
\citep{Ching:2012fu,Zhang:2010va,Dai:2007vd,Lu:2005hq,Pima:2004vw,Lu:2003we},
handwritten digit recognition \citep{Bouveyron:2007gx}, remote sensing
\citep{Tadjudin:1999fk}, seismic detection \citep{Anderson:2002kg}, and chemical
spectra \citep{Wu:1996us,Aeberhard:1993fp}.

The dimension reduction that we have employed in this paper has reduced the
dimension to the rank$(\widehat{\bm \Sigma})$, $q$. An interesting extension of
our work would reduce the dimension $q$ further to a lower dimension $q_L < q$
using some criterion proposed, perhaps similar to that employed in principal
components analysis. While it is not clear whether classification performance
would improve via such a method, the efficiency of the training and model
selection would certainly improve. Moreover, if $q_L = 2$ or $3$ was selected,
the potential for low-dimensional graphical displays of high-dimensional could
be accomplished.

\bibliographystyle{plainnat}
\bibliography{rda}

\end{document}
