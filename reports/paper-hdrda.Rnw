\documentclass{article}

\usepackage{graphicx, amsmath, amssymb, bm, mathtools, amsthm}
\usepackage{natbib, setspace}

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.5in
\textheight 9.5in

\begin{document}

\title{High-Dimensional Regularized Discriminant Analysis}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

\maketitle

\doublespacing

\begin{abstract}

\citeauthor{Friedman:1989tm} proposed the popular regularized discriminant
analysis (RDA) classifier that utilizes a biased covariance-matrix estimator
that partially pools the sample covariance matrices from linear and quadratic
disriminant analysis before shrinking the resulting estimator towards a scaled
identity matrix. The RDA classifier's two tuning parameters are typically
estimated via a computationally burdensome cross-validation procedure from a
grid of candidate values. We reformulate the RDA classifier in the
high-dimensional setting and then show that the classification decision rule is
equivalent to one in a subspace having a much lower dimension. As a result, the
dimension reduction involved yields a substantial reduction in computation
during model selection. Our parameterization also offers interpretability
previously lacking with the RDA classifier. We demonstrate with three artificial
and four real high-dimensional data sets that our proposed classifier is often
superior to several recently proposed sparse and regularized classifiers in
terms of classification accuracy. Finally, we provide an implementation of our
proposed classifier in the {\tt sparsediscrim} R package, available on CRAN.

\end{abstract}

<<knitr_setup, include=FALSE, echo=FALSE, cache=FALSE>>=
# Round digits throughout document including \Sexpr statements.
# See Yihui's response here:
# http://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr
options(digits = 3)

opts_chunk$set(fig.align = 'default', dev = 'png', message = FALSE,
               warning = FALSE, cache = TRUE, echo = FALSE, autodep = TRUE,
               fig.width = 12, fig.height = 12, out.width = "\\linewidth")
@ 
  
<<setup>>=
setwd("..")
library(ProjectTemplate)
load.project()

results <- lapply(results, function(res) {
  res <- lapply(res, function(res_d) {
    errors <- do.call(cbind, res_d$errors)
    res_d[["errors"]] <- NULL
    cbind(do.call(cbind, res_d), errors)
  })
  do.call(rbind, res)
})
results <- data.frame(do.call(rbind, results), stringsAsFactors = FALSE)

results$lambda <- as.numeric(results$lambda)
results$gamma <- as.numeric(results$gamma)
results$Dudoit <- as.numeric(results$Dudoit)
results$HDRDA <- as.numeric(results$HDRDA)
results$Guo <- as.numeric(results$Guo)
results$Pang <- as.numeric(results$Pang)
results$Tong <- as.numeric(results$Tong)
results$Witten <- as.numeric(results$Witten)

results$d <- factor(results$d, labels = sort(unique(as.numeric(results$d))))

# Summary of the classification error rates for each data set and number of
# variables selected, d.
error_rates <- subset(results, select = -c(lambda, gamma))
m_error <- melt(error_rates, variable.name = "Classifier", value.name = "Error")
error_rates <- ddply(m_error, .(data_set, d, Classifier), plyr:::summarize,
                     Avg_Error = mean(Error), Num_Reps = length(Error))
error_rates <- ddply(error_rates, .(data_set, d, Classifier), transform,
                     SE_Error = sqrt(Avg_Error * (1 - Avg_Error) / Num_Reps))
error_rates <- subset(error_rates, select = -Num_Reps)
error_rates$Classifier <- factor(as.character(error_rates$Classifier))

max_se <- max(error_rates[["SE_Error"]])
@ 


\section{Introduction}

As machine-learning algorithms are increasingly utilized to automate data-driven
decisions, researchers continue to develop a wealth of \emph{classifiers} to
construct automated pattern-recognition systems in a variety of disciplines,
such as medical research, facial recognition, remote sensing, and chemical
analysis \citep{Ching:2012fu,Li:2012ev,Zhang:2010va}. Despite the advances in
classification to facilitate technological advancements, well-established
classifiers often degrade in accuracy with modern, high-dimensional data sets
when the number of features $p$ exceeds the total sample size $N$, due to the
\emph{curse of dimensionality} \citep{Bellman:1961tn}. For instance, linear
discriminant analysis (LDA) and quadratic discriminant analysis (QDA) are
popular because of their simplicity, speed of computation, and
interpretability. However, both methods are incalculable when $p > N$ because
the class sample covariance matrices are singular \citep*{Bouveyron:2007gx}.

As a result, researchers have emphasized three standard approaches to modifying
the \emph{LDA} and \emph{QDA} classifiers for application to high-dimensional
data: 1) covariance-matrix regularization, 2) dimension reduction, and 3) model
parsimony. Covariance-matrix regularization methods are often based on the
well-known ridge-regression approach of
\cite{Hoerl:1970cd}. \cite{Ching:2012fu}, \cite{Zhang:2010va}, \cite{Ye:2009gd},
\cite{Ji:2008wp}, \cite*{Guo:2007te}, \cite*{Srivastava:2007ud},
\cite{Ye:2006vx}, and \cite{Ye:2005uu} have proposed regularization methods to
improve the estimation of the class covariance matrices by adjusting the
eigenvalues of the sample covariance matrices to ensure positive
definiteness. Alternatively, dimension reduction typically transforms the
original data in the high-dimensional space to a subspace having a much smaller
dimension such that redundant and noisy information is removed
\citep{Fan:2012iq,Ye:2009gd,Bouveyron:2007gx,Ye:2006vx,Ye:2005hd,Antoniadis:2003bk}. Finally,
\cite{Mai:2012bf}, \cite{Fan:2012iq}, \cite{Merchante:2012vk},
\cite{Clemmensen:2011kr}, \cite{Huang:2010ju}, and \cite{Dudoit:2002ev} have
proposed restrictions on the population covariance matrices to reduce the number
of parameters that must be estimated. For instance, \cite{Dudoit:2002ev} have
assumed the features within each class are independent so that the population
covariance matrices are diagonal, resulting in positive-definite sample
covariance matrices. For excellent overviews of the regularization problem in
discriminant analysis and other techniques, see \cite{Murphy:2012uq} and
\cite{Mkhadri:1997gy}.

We consider regularized discriminant analysis (\emph{RDA}) from a classic paper
by \cite{Friedman:1989tm}, who proposed improvements to the \emph{QDA}
classifier by employing a covariance matrix estimator that partially pools the
individual sample covariance matrices with the pooled sample covariance matrix
from the \emph{LDA} classifier. Given that the assumption of equal population
covariance matrices for the \emph{LDA} classifier is often too stringent in
practice \citep{Clemmensen:2011kr}, the \emph{RDA} classifier's partial pooling
can relax the linearity assumption by instead incorporating unequal covariance
matrices while retaining as special cases the \emph{LDA} and QDA
classifiers. Covariance-matrix regularization is then applied to the partially
pooled covariance-matrix estimator by shrinking it towards a scaled identity
matrix to ensure positive definiteness.

We reparameterize the \emph{RDA} classifier similar to \cite{Hastie:2008dt} and
\cite{Halbe:2007ea} and refer to our parameterization as the high-dimensional
\emph{RDA} (\emph{HDRDA}) classifier. First, we show the pooling parameter in
the \emph{HDRDA} classifier determines the contribution of each training
observation to the estimation of each class covariance matrix, enabling
interpretability that has been previously lacking with the \emph{RDA} classifier
\citep{Bensmail:1996fw}. Furthermore, the \emph{HDRDA} classifier includes a
large family of shrinkage methods, such as those from \cite{Srivastava:2007ww},
\cite{Rao:1971ul}, and other methods studied by \cite{Ramey:2013ji} and
\cite{Xu:2009fl}. Next, applying a reasoning similar to \cite{Ye:2006tq}, we
show that the matrix operations corresponding to the null space of the pooled
sample covariance matrix are redundant and can be discarded from the
\emph{HDRDA} decision rule without loss of classificatory information. Moreover,
because the \emph{HDRDA} decision rule is invariant to adjustments to the zero
eigenvalues, the decision rule in the original feature space is equivalent to a
decision rule in a much lower dimension (i.e., the intrinsic dimension), such
that the matrix inverses and determinants of relatively small matrices can be
rapidly computed. Thus, the dimension reduction inherent in the \emph{HDRDA}
classifier yields a substantial reduction in computation during model selection.

We provide an efficient algorithm that expedites the model-selection process for
the \emph{HDRDA} classifier and then compare its computational runtime with an
implementation of the \emph{RDA} classifier from the {\tt klaR} R package to
illustrate that the \emph{HDRDA} classifier greatly reduces computation when $p
\gg N$. We then study the classification performance of the \emph{HDRDA}
classifier on three artificial and four real high-dimensional data sets. We
demonstrate that the \emph{HDRDA} classifier often attains superior
classification accuracy to several recent classifiers designed for small-sample,
high-dimensional data from \cite{Tong:2012hw}, \cite{Witten:2011kc},
\cite{Pang:2009ik}, \cite{Guo:2007te}, and \cite{Dudoit:2002ev}. We also include
as benchmarks support vector machines (\emph{SVM}) and $k$-nearest neigbors
($k$-NN) in our classification study.\footnote{TODO: If we change any of the
  classifiers in the simulations, they should be updated here.}

The remainder of this document is organized as follows. In Section
\ref{sec:preliminaries} we present the discrimination problem and the necessary
notation to describe our contributions. In Section \ref{sec:rda} we present the
\emph{HDRDA} classifier along with its interpretation, properties, and
computationally efficient model-selection procedure. Section \ref{sec:sims}
describes our simulation studies of artificial and real data sets and also
describes the experimental results. We conclude with a brief discussion in
Section \ref{sec:discussion}.

\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Notation}

To facilitate our discussion of covariance-matrix regularization and regularized
discriminant analysis, we require the following notation. Let $\mathbb{R}_{a
  \times b}$ denote the matrix space of all $a \times b$ matrices over the real
field $\mathbb{R}$. Denote by $\bm I_m$ the $m \times m$ identity matrix, and
let $\bm 0_{m \times p}$ be the $m \times p$ matrix of zeros, such that $\bm
0_m$ is understood to denote $\bm 0_{m \times m}$. Let $\bm A \oplus \bm B$
denote the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in
\mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Let $\bm A^{T}$,
$\bm A^+$, and $\mathcal{N}(\bm A) = \{\bm z \in \mathbb{R}_{p \times 1} : \bm A
\bm z = \bm 0_{m \times 1}\}$ denote the transpose, the Moore-Penrose
pseudoinverse, and the null space of $\bm A \in \mathbb{R}_{m \times p}$,
respectively. Denote by $\mathbb{R}_{p \times p}^{>}$ the cone of real $p \times
p$ positive-definite matrices. Similarly, let $\mathbb{R}_{p \times p}^{\ge}$
denote the cone of real $p \times p$ positive-semidefinite matrices. Let
$V^{\perp} = \{\bm y \in \mathbb{R}_{p \times 1} : \bm y^{T} \bm v = 0$ for all
$\bm v \in V \}$ denote the orthogonal complement of a vector space $V \subset
\mathbb{R}_{p \times 1}$. For $c \in \mathbb{R}$, let $c^+ = 1/c$ if $c \ne 0$
and $0$ otherwise.

\subsection{Discriminant Analysis}

In discriminant analysis we wish to assign correctly an unlabeled vector $\bm x
\in \mathbb{R}_{p \times 1}$ to one of $K$ unique, known classes by constructing
a classifier from $N$ training observations. Let $\bm x_i = (x_{i1}, \ldots,
x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots,
N)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$. We
assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm
x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$
is the probability density function of the $k$th class and $p(\omega_k)$ is the
prior probability of class membership of the $k$th class. We further assume that
each of $p(\omega_k)$ is equal, $k = 1, \ldots, K$.

Quadratic discriminant analysis is the optimal Bayesian decision rule with
respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the probability
density function of the multivariate normal distribution with known mean vectors
$\bm \mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrices $\bm
\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$. Because $\bm
\mu_k$ and $\bm \Sigma_k$ are typically unknown, we assign an unlabeled
observation $\bm x$ to class $\omega_k$ with the sample quadratic discriminant
analysis classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k} (\bm x - \xbar_k)^{T}\widehat{\bm
    \Sigma}_k^{-1}(\bm x - \xbar_k) + \log |\widehat{\bm \Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm \Sigma}_k$ are the maximum likelihood
estimators of $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further
that $\bm \Sigma_k = \bm \Sigma$, $k = 1, \ldots, K$, then the pooled sample
covariance matrix $\widehat{\bm \Sigma}$ is substituted for $\widehat{ \bm
  \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm \Sigma} = N^{-1} \sum_{k=1}^K n_k \widehat{\bm \Sigma}_k \label{eq:pooled-cov}
\end{align}
is the maximum likelihood estimator for $\bm \Sigma$. In this case \eqref{eq:qda}
reduces to the sample linear discriminant analysis classifier, and the
log-determinant is omitted because it is constant across the $K$ classes.

The smallest eigenvalues and the directions associated with their eigenvectors
can highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{ \bm
  \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest
eigenvalues are underestimated \citep{Seber:2004uh}. Moreover, if $p > n_k$,
then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p -
n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are zero. Furthermore, although
more feature information is available to discriminate among the $K$ classes, if
$p > n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$
does not exist.

Several regularization methods, such as the methods considered by
\citet*{Xu:2009fl}, \citet*{Guo:2007te}, and \cite{Mkhadri:1995jp} have been
proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$
so that \eqref{eq:qda} is calculable and has reduced variability in the standard
bias-variance tradeoff. A common form of the covariance-matrix regularization
applies a shrinkage factor $\gamma > 0$, so that
\begin{align}
	\widehat{\bm \Sigma}_k(\gamma) = \widehat{\bm \Sigma}_k + \gamma \bm I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation
\eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance
matrix $\widehat{\bm \Sigma}_k$ toward $\bm I_p$, thereby increasing the
eigenvalues of $\widehat{\bm \Sigma}_k$ by $\gamma$. Specifically, the zero
eigenvalues are replaced with $\gamma$, so that \eqref{eq:ridge-estimator} is
positive definite. For additional covariance-matrix regularization methods, see
\cite{Ramey:2013ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\section{High-Dimensional Regularized Discriminant Analysis}
\label{sec:rda}

Here, we define the \emph{HDRDA} classifier by first formulating the
covariance-matrix estimator $\widehat{\bm \Sigma}_k(\lambda)$ and demonstrating
its clear interpretation as a linear combination of the crossproducts of the
training observations centered by their respective class sample means. We define
the convex combination
\begin{align}
  \widehat{\bm \Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm \Sigma}_k + \lambda
  \widehat{\bm \Sigma}, \quad k = 1, \ldots, K,\label{eq:sig-lambda-alternative}
\end{align}
where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. By rewriting
\eqref{eq:sig-lambda-alternative} in terms of the observations $ \bm x_i$ $(i =
1, \ldots, N)$, each centered by its class sample mean, we attain a clear
interpretation of $\widehat{\bm \Sigma}_k(\lambda)$. That is, from
\eqref{eq:pooled-cov} we have
\begin{align}
	\widehat{\bm \Sigma}_k(\lambda)
  &= \left( 1 - \lambda + \frac{\lambda n_k}{N} \right) \widehat{\bm \Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}}^K n_{k'} \widehat{\bm \Sigma}_{k'} \nonumber \\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N
  I(y_i = k) \bm x_i \bm x_i^{T} +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k)
  \bm x_i \bm x_i^{T} \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i^{T},\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$. We
can interpret \eqref{eq:sig-lambda-alternative2} as a covariance-matrix
estimator that borrows from $\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov} to
better estimate $ \bm \Sigma_k$. From \eqref{eq:sig-lambda-alternative2},
$\lambda$ weights the contribution of each of the $N$ observations in estimating
$\bm \Sigma_k$. Hence, for $0 < \lambda \le 1$, we estimate $\bm \Sigma_k$ with
$N$ rather than $n_k$ observations.\footnote{TODO: Add more to the
  interpretation here and reference Figure \ref{fig:hdrda-weights}.}

\begin{figure}
<<rda_weights>>=
n <- c(50, 25, 15, 10)
N <- sum(n)
lambda <- seq(0, 1, length = 1000)

rda_weights <- sapply(seq_along(n), function(k) {
      lambda / N + (1 - lambda) / n[k]
      })
rda_weights <- data.frame(lambda = lambda, rda_weights)
rda_weights <- melt(rda_weights, id = 'lambda')
colnames(rda_weights) <- c("lambda", "Class", "Weight")
levels(rda_weights$Class) <- seq_along(n)

p <- ggplot(rda_weights, aes(lambda, Weight, linetype = Class, color = Class))
p <- p + geom_line(size = 1)
p <- p + theme_bw() + xlab(expression(lambda))
p <- p + theme(axis.text = element_text(size = 10))
p <- p + theme(strip.text.x = element_text(size = 16))
p <- p + theme(legend.title = element_text(size = 16, face = "bold"))
p <- p + theme(legend.text = element_text(size = 15))
p <- p + theme(legend.key.size = unit(1, "cm"))
p
@ 
\caption{Weights $c_{ik}(\lambda)$ of \emph{RDA} classifier as a function of the
  pooling parameter $\lambda$ when $n_1 = 50$, $n_2 = 25$, $n_3 = 15$, and $n_4$
  = 10. TODO: Scale down figure.}
\label{fig:hdrda-weights}
\end{figure}


Below, we will show that the pooling
operation is advantageous in increasing the rank of each $\widehat{\bm
  \Sigma}_k(\lambda)$ from rank$(\widehat{\bm \Sigma}_k)$ to rank$(\widehat{\bm
  \Sigma})$ for $0 < \lambda \le 1$. Notice that if $\lambda = 0$, then the
observations from the remaining $K - 1$ classes do not contribute to the
estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm
  \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in
\eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to
$\widehat{\bm \Sigma}$. For brevity, when $\lambda = 1$, we define $\bm X =
      [\sqrt{c_{1k}(1)} \bm x_1^{T}, \ldots, \sqrt{c_{Nk}(1)} \bm x_N^{T}]^{T}$
      such that $\widehat{\bm \Sigma} = \bm X^{T} \bm X$. Similarly, for
      $\lambda = 0$, we define $\bm X_k = [\sqrt{c_{1k}(0)} \bm x_1^{T}, \ldots,
        \sqrt{c_{Nk}(0)} \bm x_N^{T}]^{T}$ such that $\widehat{\bm \Sigma}_k =
      \bm X_k^{T} \bm X_k$. In Figure \ref{fig:hdrda-contours} we plot the
      contours of five multivariate normal populations for $\lambda = 0$ with
      unequal covariance matrices. As $\lambda$ approaches 1, the contours
      become more similar, resulting in identical contours for $\lambda = 1$.
\begin{figure}
<<hdrda_contours, fig.width=8, fig.height=8, out.width='.33\\linewidth', fig.show='hold'>>=
rda_contours(lambda = 0)
rda_contours(lambda = 1/4)
rda_contours(lambda = 3/4)
rda_contours(lambda = 1)
@ 
\caption{Contours of five multivariate normal populations as a function of the
  pooling parameter $\lambda$.}
\label{fig:hdrda-contours}
\end{figure}
As we have discussed above, several eigenvalue adjustment methods have been
proposed to increase eigenvalues (approximately) equal to 0. To further improve
the estimation of $\bm \Sigma_k$ and to stabilize the estimator's inverse, we
define the eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\bm \Sigma}_k = \alpha_k \widehat{\bm \Sigma}_k(\lambda) + \gamma \bm I_p,\label{eq:hdrda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\gamma \ge 0$ is an eigenvalue-shrinkage
constant. Thus, the \emph{pooling} parameter $\lambda$ controls the amount
borrowed from $\widehat{\bm \Sigma}$ to estimate $\bm \Sigma_k$, and the
\emph{shrinkage} parameter $\gamma$ determines the degree of eigenvalue
shrinkage. The choice of $\alpha_k$ allows for a flexible formulation of
covariance-matrix estimators. For instance, if $\alpha_k = 1$, $k = 1, \ldots,
K$, then \eqref{eq:hdrda-cov} resembles \eqref{eq:ridge-estimator}. Similarly,
if $\alpha_k = 1 - \gamma$, then \eqref{eq:hdrda-cov} has a form similar to the
\emph{RDA} classifier from \cite{Friedman:1989tm}. Now, substituting
\eqref{eq:hdrda-cov} into \eqref{eq:qda}, we define the \emph{HDRDA} classifier
\begin{align}
	D_{HDRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)^{T}\tilde{\bm \Sigma}_k^{+}(\bm x - \xbar_k)  + \log |\tilde{\bm \Sigma}_k|. \label{eq:hdrda}
\end{align}
For $\gamma > 0$, $\tilde{\bm \Sigma}_k$ is nonsingular such that $\tilde{\bm
  \Sigma}_k^{-1}$ can be substituted for $\tilde{\bm \Sigma}_k^{+}$ in
\eqref{eq:hdrda}. If $\gamma = 0$, we explicitly set $|\tilde{\bm \Sigma}_k|$
equal to the product of the positive eigenvalues of $\tilde{\bm
  \Sigma}_k$. Following \cite{Friedman:1989tm}, we select $\lambda$ and $\gamma$
from a grid of candidate models via cross-validation \citep{Hastie:2008dt}. We
provide an implementation of \eqref{eq:hdrda} in the {\tt hdrda} function in the
{\tt sparsediscrim} R package, available on CRAN.

\subsection{Relationship with Friedman's RDA Classifier}

\cite{Friedman:1989tm} proposed regularized discriminant analysis (RDA) by
incorporating a weighted average of the sample covariance matrix $\widehat{ \bm
  \Sigma}_k$ and the pooled sample covariance matrix $\widehat{\bm \Sigma}$ to
estimate $\bm \Sigma_k$ with
\begin{align}
    \widehat{\bm \Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda) \quad
    (k =
      1, \ldots, K),\label{eq:sig-lambda}
      \end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda N$, $
\bm S_k = n_k \widehat{\bm \Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $ \bm
S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. If $\lambda = 0$,
\eqref{eq:sig-lambda} is equal to $\widehat{\bm \Sigma}_k$ used in
\eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces
to \eqref{eq:pooled-cov}.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse
of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased
covariance-matrix estimator
\begin{align}
          \widehat{\bm \Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm
            \Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm
              \Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
          \end{align}
where $\gamma \in [0, 1]$ is a regularization parameter that controls the
shrinkage of \eqref{eq:sig-rda} towards $\bm I_p$ and is weighted by the average
of the eigenvalues of \eqref{eq:sig-lambda}. Finally, the RDA classifier is
established by substituting \eqref{eq:sig-rda} into
\eqref{eq:qda}.\footnote{TODO: Discuss $\alpha_k \equiv 1-\gamma$, etc.}

\subsection{Properties of the HDRDA Classifier}
\label{sec:computational-efficiency}

Next, we establish properties of the covariance-matrix estimator and the
decision rule employed in the \emph{HDRDA} classifier. By doing so, we
demonstrate that \eqref{eq:hdrda} lends itself to a more efficient
calculation. We decompose \eqref{eq:hdrda} into a sum of two components, where
the first summand consists of matrix operations applied to low-dimensional
matrices and the second summand corresponds to the null space of $\widehat{\bm
  \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix operations
performed on the null space of $\widehat{\bm \Sigma}$ yield constant quadratic
forms across all classes and can be omitted. For $p \gg N$, the constant
component involves determinants and inverses of high-dimensional matrices, and
by ignoring these calculations, a substantial reduction in computational costs
is achieved. Furthermore, a byproduct is that adjustments to the associated
eigenvalues have no effect on \eqref{eq:hdrda}. Lastly, we utilize the singular
value decomposition to calculate efficiently the eigenvalue decomposition of
$\widehat{\bm \Sigma}$, further reducing the computational costs of the
\emph{HDRDA} classifier.

First, we require the following relationship regarding the null spaces of
$\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{ \bm
  \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the maximum
likelihood estimators of $\bm \Sigma_k$ and $\bm \Sigma$, respectively. Let
$\widehat{\bm \Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, $\mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\} \subset \mathcal{N}(\widehat{
  \bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$, $k = 1, \ldots, K$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\}$ for some $k = 1,
\ldots, K$. Hence, $0 = \bm z^{T} \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 -
\lambda) \bm z^{T} \widehat{\bm \Sigma}_k \bm z + \lambda \bm z^{T} \widehat{
  \bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in
\mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{
  \bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular,
we have that $\mathcal{N}\{\widehat{\bm \Sigma}_k(\lambda)\} \subset
\mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in
\mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z^{T}
\widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z^{T} \widehat{
  \bm \Sigma}_k \bm z$, which implies that $\bm z \in \mathcal{N}(\widehat{
  \bm \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times
  p}^{\ge}$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset
\mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative
expression for $\tilde{\bm \Sigma}_k$ in terms of the matrix of eigenvectors of
$\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U^{T}$ be the
eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p
  \times p}^{\ge}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$
with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, $\bm D_q \in \mathbb{R}_{q \times
  q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of
$\widehat{\bm \Sigma}$, the columns of $\bm U \in \mathbb{R}_{p \times p}$ are
the corresponding orthonormal eigenvectors of $\widehat{\bm \Sigma}$, and
rank$(\widehat{\bm \Sigma}) = q$. Then, we partition $\bm U = (\bm U_1, 
\bm U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in
\mathbb{R}_{p \times (p - q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U^{T}$ be the eigendecomposition of
$\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) =
q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \gamma \bm I_{p-q})\bm U^{T}, \quad
  k = 1, \ldots, K,\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= \alpha_k \{(1 - \lambda) \bm U_1^{T} \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm I_{q}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $\bm U_2$ span the null space
of $\widehat{\bm \Sigma}$, which implies that $\widehat{\bm \Sigma}_k \bm U_2 =
\bm 0_{p \times (p - q)}$. Hence, $\bm U^{T} \widehat{\bm \Sigma}_k \bm U = \bm
U_1^{T} \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm 0_{p-q}$, $k = 1, \ldots,
K$. Thus, $\bm U^{T} \tilde{\bm \Sigma}_k \bm U = \alpha_k \{(1 - \lambda) \bm
U^{T} \widehat{\bm \Sigma}_k \bm U + \lambda \bm D\} + \gamma \bm I_p$, and
\eqref{eq:rda-matrix} holds because $\bm U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we
have the following corollary.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$,
  rank$\{\widehat{\bm \Sigma}_k(\lambda)\} = q$, $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we
increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ to $q$ if $\lambda \ne
0$. The results is that the \emph{HDRDA} classifier has the advantage over
\eqref{eq:qda} in that the rank$(\widehat{\bm \Sigma}_k) \le n_k$. Next, we
provide an essential result that enables us to prove that \eqref{eq:hdrda} is
invariant to adjustments to the eigenvalues of $\tilde{ \bm \Sigma}_k$
corresponding to the null space of $\widehat{\bm \Sigma}$ for any unlabeled
observation $\bm x \in \mathbb{R}_{p \times 1}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times
  1}$, $\bm U_2^{T} (\bm x - \xbar_k) = \bm U_2^{T} (\bm x - \xbar_{k'})$ $(1 \le k,
k' \le K)$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$.
Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that
$\bm U_2^{T} \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma
  1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{ \bm
  \Sigma})$ $(i = 1, \ldots, N)$, $\bm U_2^{T} \bm x_i = \bm 0_{(p-q) \times
  1}$.  Hence, $\bm 0_{(p-q) \times 1} = \sum_{i=1}^N \beta_i \bm U_2^{T}\bm x_i
= \bm U_2^{T}(\xbar_k - \xbar_{k'})$, where $\beta_i = (n_k n_{k'})^{-1} \{
I(y_i = k) n_{k'} - I(y_i = k') n_k \}$. Therefore, $\bm U_2^{T} (\bm x -
\xbar_k) = \bm U_2^{T} (\bm x - \xbar_{k'})$.
\end{proof}

We now present our main result where we decompose \eqref{eq:hdrda} and show that
the term requiring the largest computational costs does not contribute to the
classification of an unlabeled observation performed using Lemma
\ref{lemma:RDA-constant-term}. Hence, we reduce \eqref{eq:hdrda} to an
equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ and $\bm W_k$ be defined as in \eqref{eq:rda-matrix}
and \eqref{eq:Wk}, respectively, and let $\bm U_1$ be defined as above. Then,
the decision rule in \eqref{eq:hdrda} is equivalent to
	\begin{align}
		D_{HDRDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x - \xbar_k) + \log | \bm W_k |. \label{eq:hdrda-decomposed}
	\end{align}
\end{thm}
\begin{proof}
From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{+} = \bm U(
\bm W_k^{-1} \oplus \gamma^{+} \bm I_{p-q} )\bm U^{T}$ and $|\tilde{\bm \Sigma}_k| =
\gamma^{p-q}| \bm W_k |$, $k = 1, \ldots, K$. Therefore, for all $\bm x \in
\mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)^{T} \tilde{\bm \Sigma}_k^{+}(\bm x - \xbar_k)  + \log |\tilde{\bm \Sigma}_k| &= (\bm x - \xbar_k)^{T} \bm U_1 \bm W_k^{-1} \bm U_1^{T} (\bm x - \xbar_k)\\
	&+ \gamma^{+} (\bm x - \xbar_k)^{T} \bm U_2 \bm U_2^{T} (\bm x - \xbar_k) + \log | \bm W_k |\\
  &+ (p - q) \log \gamma.
	\end{align*}
Because $\gamma$ is constant for $k = 1, \ldots, K$, we can omit the $(p - q)
\log \gamma$ term and particularly avoid the calculation of $\log 0$ for $\gamma
= 0$. Then, the proof follows from Lemma \ref{lemma:RDA-constant-term} because
$\bm U_2^{T} (\bm x - \xbar_k)$ is constant for $k = 1, \ldots, K$.
\end{proof}

Thus far, we have shown that our proposed classifier in \eqref{eq:hdrda} is
invariant to the term $\bm U_2$, thus yielding an equivalent classifier in
\eqref{eq:hdrda-decomposed} with a substantial reduction in the computational
complexity. Here, we demonstrate that the computational efficiency in
calculating the inverse and determinant of $\bm W_k$ can be further improved
via standard matrix operations.

\begin{proposition}\label{proposition:hdrda-W_k}
Let $\bm W_k$ be defined as above, $k = 1, \ldots, K$. Then, $|\bm W_k| = |
\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - \alpha_k(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1^{T} \bm X_k^{T} \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},\label{eq:W_k_inv}
	\end{align}
where $\bm Q_k = \bm I_{n_k} + \alpha_k(1 - \lambda) \bm X_k \bm U_1 
\bm \Gamma_k^{-1} \bm U_1^{T} \bm X_k^{T}$ and $\bm \Gamma_k = \alpha_k \lambda \bm D_q +
\gamma \bm I_q$.
\end{proposition}
\begin{proof}
To calculate $|\bm W_k|$, we apply Theorem 18.1.1 from \cite{Harville:2008wja},
which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} +
\bm C \bm A^{-1} \bm B|$. Thus, setting $\bm A = \bm \Gamma_k$, $\bm B =
\alpha_k (1 - \lambda) \bm U_1^{T} \bm X_k^{T}$, $\bm T = \bm I_{n_k}$, and $\bm
C = \bm X_k \bm U_1$, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also,
\eqref{eq:W_k_inv} follows from the well-known Sherman-Woodbury formula
\citep[Theorem 18.2.8]{Harville:2008wja}.
\end{proof}

\subsection{Computationally Efficient Model Selection}

Conveniently, we see that the inverse and determinant of $\bm W_k$ are each in
terms of the inverse and determinant of $\bm Q_k \in \mathbb{R}_{n_k \times
  n_k}$, respectively, as well as the diagonal matrix $\bm \Gamma_k \in
\mathbb{R}_{q \times q}$. Hence, the resulting formulation requires $O(n_k p q)$
calculations to compute both $\bm W_k^{-1}$ and $| \bm W_k |$ and is especially
advantageous when the number of classes $K$ is large because $n_k$ is then
potentially much smaller than $q$. The expressions given in Proposition
\ref{proposition:hdrda-W_k} also expedite the selection of $\lambda$ and
$\gamma$ via cross-validation because the most time-consuming matrix operation
involved in computing $\bm W_k^{-1}$ and $|\bm W_k|$ is $\bm X_k \bm U_1 \in
\mathbb{R}_{n_k \times q}$, which is independent of $\lambda$ and
$\gamma$. Therefore, we need calculate $\bm X_k \bm U_1$ only once for each
cross-validation fold. The subsequent operations in calculating $\bm W_k^{-1}$
and $|\bm W_K|$ can be simply updated for different pairs of $\lambda$ and
$\gamma$.

Thus far, we have constructed each $\tilde{\bm \Sigma}_k$ in terms of the first
$q$ eigenvectors of $\widehat{\bm \Sigma}$. By utilizing a technique similar to
the so-called kernel trick, we can obtain $\bm U_1$ by computing the
eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$
\citep{Hastie:2008dt}. Recall that the pooled sample covariance matrix is
$\widehat{\bm \Sigma} = \bm X^{T} \bm X$. Then, because $\bm X = \bm M \bm
D^{1/2} \bm U^{T}$ by the singular value decomposition, we have the
eigendecomposition $\widehat{\bm \Sigma} = \bm U \bm D \bm U^{T}$, where $\bm M
\in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D^{1/2} \in \mathbb{R}_{N
  \times N}^{\ge}$ is a diagonal matrix consisting of the singular values of
$\bm X$, and $\bm U \in \mathbb{R}_{p \times N}$ is orthogonal. Now, $\bm X \bm
X^{T} = \bm M \bm D \bm M^{T}$ is the eigenvalue decomposition of $\bm X \bm
X^{T}$, where $\bm D$ is the matrix of eigenvalues of $\bm X \bm X^{T}$ and the
columns of $\bm M$ are the corresponding eigenvectors. Hence, we obtain $\bm M$
and $\bm D$ from an eigendecomposition of $\bm X \bm X^{T} \in \mathbb{R}_{N
  \times N}$. Next, we compute $\bm U = \bm X^{T} \bm M \bm D^{+/2}$, where $\bm
D^{+/2} = \bm D_q^{-1/2} \oplus \bm 0_{N-q}$. We then determine $q$, the number
of numerically nonzero eigenvalues present in $\bm D$, by calculating the number
of eigenvalues that exceeds some tolerance value, say, $\epsilon = 1 \times
10^{-6}$. After calculating $q$, we extract $\bm U_1$ by retaining only the
first $q$ columns of $\bm U$.

\section{Classification Study}
\label{sec:sims}

In this section, we describe four high-dimensional microarray data sets and
compare our proposed classifier with five classifiers recently proposed for
small-sample, high-dimensional data. Our competitors include diagonal linear
discriminant analysis from \cite{Dudoit:2002ev}, shrunken centroids regularized
discriminant analysis from \cite{Guo:2007te}, and penalized linear discriminant
analysis from \cite{Witten:2011kc}. We also consider two modifications of
diagonal linear discriminant analysis from \cite{Tong:2012hw} and
\cite{Pang:2009ik}, where the former employs an improved mean estimator and the
latter utilizes an improved variance estimator. Below, we refer to each
classifier by the first author's surname.

We evaluated the classification performance of each classifier by randomly
partitioning the data set under consideration such that $2/3$ of the
observations were allocated as training data and the remaining $1/3$ of the
observations were allocated as a test data set. For each random partition, we
applied the feature-selection method from \cite{Dudoit:2002ev} to the training
data to choose the $d$ features having the largest ratio of their between-group
to within-group sums of squares.  We then filtered the test data set with the
corresponding $d$ features. After training each classifier from the filtered
training data, we classified the filtered test data sets and computed the
proportion of mislabeled test observations to estimate the classification error
rate for each classifier. Repeating this process 1000 times, we computed the
average of the error-rate estimates for each classifier. We remark that our
simulation design avoids the selection bias discussed in \cite{Ambroise:2002fa}.

We used version 3.0.0 of the open-source statistical software {\tt R} for the
classification study. We used the implementations of the Witten and Guo
classifiers from the {\tt penalizedLDA} and {\tt rda} packages, respectively, on
CRAN. Because the {\tt rda} package does not perform the authors' ``Min-Min''
rule automatically, we applied this rule in our {\tt R} code. We also utilized
our implementations of the Dudoit, Pang, and Tong classifiers available in the
{\tt sparsediscrim} package. We used the default settings for each classifier
with the exception that we explicitly set prior probabilities as equal, if the
option was available. For the \emph{HDRDA} classifier in
\eqref{eq:hdrda-decomposed}, we examine the classification performance of two
models. For the first \emph{HDRDA} model, we set $\alpha_k = 1$, $k = 1, \ldots,
K$, so that the covariance-matrix estimator \eqref{eq:hdrda-cov} employed
resembled \eqref{eq:ridge-estimator}. We estimated $\lambda$ from a grid of 21
equidistant candidate values between 0 and 1, inclusively. Similarly, we
estimated $\gamma$ from a grid consisting of the values $10^{-1}, \ldots, 10^4$,
and $10^5$. We selected optimal estimates of $\lambda$ and $\gamma$ using
10-fold cross-validation \citep{Hastie:2008dt}. For the second model, we set
$\alpha_k = 1 - \gamma$, $k = 1, \ldots, K$, to resemble
\citeauthor{Friedman:1989tm}'s parameterization. We estimated both $\lambda$ and
$\gamma$ from a grid of 21 equidistant candidate values between 0 and 1,
inclusively. Each of the microarray data sets listed below can be found in
\ref{suppA}.

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC)
through hybridization to microarrays from 127 individuals resulting in 22,283
sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD),
and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to
improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using
the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide
microarray from 105 samples of 10 types of soft tissue tumors. This included 16
samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma
(MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS),
15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of
myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of
malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21
samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl}
determined from their data that these 10 types fell into 4 broader groups: (1)
SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and
pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following
\cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at
least 15 observations.

\subsection{\cite{Shipp:2002ka} Data Set}

Approximately 30-40\% of adult non-Hodgkin lymphomas are diffuse large B-cell
lymphomas (DLBCLs) \cite{Shipp:2002ka}.  However, only a small proportion of
DLBCL patients are cured with modern chemotherapeutic regimens.  Several models
have been proposed, such as the International Prognostic Index (IPI), to
determine a patient's curability.  These models rely on clinical covariates,
such as age, to determine if the patient can be cured and are often
ineffective. \cite{Shipp:2002ka} have argued that more effective means are
desired to determine a patient's curability. The authors measured 6,817 gene
expression levels from 58 DLBCL patient samples with customized cDNA
(lymphochip) microarrays to investigate the curability of patients treated with
cyclophosphamide, adriamycin, vincristine, and prednisone (CHOP)-based
chemotherapy.  Among the 58 DLBCL patient samples, 32 are from cured patients
while 26 are from patients with fatal or refractory disease.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from
surgery patients between 1995 and 1997. The authors used oligonucleotide
microarrays containing probes for approximately 12,600 genes and expressed
sequence tags. They have reported that 102 of the radical prostatectomy
specimens are of high quality: 52 prostate tumor samples and 50 non-tumor
prostate samples.
 
\subsection{Classification Results}

For the microarray data sets, we computed error-rate estimates for the competing
classifiers after reducing the dimension of the data sets with feature selection
to $d = 100, 200, \ldots, 1000$, similar to the values of $d$ considered by
\cite{Xu:2009fl}. To compare misclassification rates between different values of
$d$, we applied each value of $d$ considered to the same random allocation of
training and test data. Hence, for a given random partition, the set of features
selected for a smaller value of $d$ is a proper subset of the features selected
for a larger value of $d$. We report the average of the error-rate estimates for
each value of $d$ in Figure \ref{fig:error-rates}. Approximate standard errors
were no greater than \Sexpr{max_se}.

The \emph{HDRDA} classifier was superior in classification performance for the
majority of the simulations. For the Burczysnki, Shipp, and Singh data sets, our
classifier outperformed its competitors for all values of $d$ examined. For the
Nakayama data set, the \emph{HDRDA} classifier \eqref{eq:hdrda-decomposed} was
slightly outperformed by the Dudoit, Pang, and Tong classifiers for small values
of $d$ but attained classification rates superior to its competitors for larger
values of $d$. Furthermore, Figure \ref{fig:error-rates} emphasizes two
strengths of our classifier: reliability of the classifier for relatively small
$d$ and stability of the classifier for large $d$. In contrast to the other
classifiers, the \emph{HDRDA} classifier exhibited less variability in its
misclassification estimates for small values of $d$ and yielded superior
classification accuracy as $d$ increased and was hardly effected by the
inclusion of more variables in the data sets. We attribute the apparent
stability of the error-rate estimates of our classifier for large values of $d$
to its novel dimension-reduction technique.

The average error-rate estimates for the Dudoit, Pang, Tong, and Witten
classifiers were comparable for all values of $d$ examined. Furthermore, the
error-rate estimates of the Dudoit, Pang, and Tong classifiers were
approximately equal throughout our study, and the Witten classifier yielded
approximately equal error-rate estimates for the Burczysnki, Shipp, and Singh
data sets. Furthermore, the error-rate estimates for the Dudoit and Tong
classifiers were nearly identical, which suggests that the improved mean
estimator employed by \cite{Tong:2012hw} provided little improvement to the
classification performance of diagonal linear discriminant analysis.

The classification performance of the the Dudoit, Pang, Tong, and Witten
classifiers for the Shipp and Singh data sets revealed the inability of these
methods to handle larger values of $d$. After closer examination of the
classification studies performed in the authors' original papers, we noticed
that classification accuracy estimates were reported only for small values of
$d$, thereby ignoring the inclusion of a larger number of variables. Hence, we
advise practioners to consider filtering a large number of features before
employing these methods.

The popular Guo classifier is most competitive over large values of $d$ for the
Shipp and Singh data sets. However, for the Burczynski and Nakayama data sets,
the implementation of the Guo method in the {\tt rda} package often classified
every test observation into a single class, thus yielding larger error rates
than its competitors. Furthermore, the Guo classifier exhibited unreliable
classification performance for smaller values of $d$ and required a larger
subset of the features to perform competitively in our study.
\begin{figure}
<<error_rates, fig.width=8, fig.height=8, out.width='.65\\linewidth'>>=
error_rates$data_set <- Hmisc:::capitalize(error_rates$data_set)
p <- ggplot(error_rates, aes(x = d, y = Avg_Error, group = Classifier))
p <- p + geom_line(aes(color = Classifier), size = 1, linetype = 2)
p <- p + geom_point(aes(color = Classifier, shape = Classifier), size = 3)
p <- p + facet_wrap(~ data_set) + ylim(c(0, max(error_rates$Avg_Error))) + theme_bw()
p <- p + xlab("Number of Variables Selected, d") + ylab("Average Error Rate")
p <- p + theme(axis.text = element_text(size = 10))
p <- p + theme(axis.title.x = element_text(size = 17))
p <- p + theme(axis.title.y = element_text(size = 17))
p <- p + theme(strip.text.x = element_text(size = 16))
p <- p + theme(legend.title = element_text(size = 16, face = "bold"))
p <- p + theme(legend.text = element_text(size = 15))
p <- p + theme(legend.key.size = unit(1, "cm"))
p
@ 
\caption{Averages of the 1000 error-rate estimates of the competing classifiers
  for each data set as a function of $d$. Approximate standard errors are no
  greater than \Sexpr{max_se}.}
\label{fig:error-rates}
\end{figure}
Next, we examine the optimal estimates of the tuning parameters for the
\emph{HDRDA} classifier. For brevity, we provide graphical summaries of the
optimal estimates of $\lambda$ and $\gamma$ in \ref{suppB}. For each of the four
data sets, the distribution of the optimal values of $\lambda$ selected through
cross-validation often had a notable bathtub shape, resembling a Beta
distribution with both shape parameters less than unity. Specifically, the
optimal values of $\lambda$ were typically near 0 or near 1, corresponding to
quadratic and linear discriminant analysis, respectively. For instance, with
smaller values of $d$ applied to the Shipp data set, the range of optimal values
of $\lambda$ was typically no larger than 0.05, indicating there were a large
number of training data sets for which we can safely assume the covariance
matrices are unequal. As $d$ increased, the majority of the optimal values of
$\lambda$ were selected as unity, thereby an increased number of variables
resulted in the \emph{HDRDA} classifier preferring the more parsimonious
assumption of equal covariance matrices. In this instance we can see the
flexibility of our proposed classifier to employ the more realistic assumption
of unequal class covariance matrices, but if optimal, the \emph{HDRDA}
classifier \eqref{eq:hdrda-decomposed} can utilize the more parsimonious
assumption of equal covariance matrices.

\section{Discussion}
\label{sec:discussion}

In our simulation study, we have compared our proposed \emph{HDRDA} classifier
to several recently proposed classifiers, including the Dudoit, Pang, Tong, and
Guo classifiers, each of which assumes independence of the features conditional
on class membership. Despite the rapid computational performance of diagonal
classifiers and the reduction in the number of parameters to estimate,
\cite{Mai:2012bf} and \cite{Fan:2012iq} have noted that diagonal classifiers can
often yield inferior classification performance to other classification
methods. We have shown that the \emph{HDRDA} classifier often yields superior
classification accuracy to the diagonal classifiers considered, confirming the
assertions of \cite{Mai:2012bf} and \cite{Fan:2012iq}. Furthermore, the
implementation of \eqref{eq:hdrda-decomposed} in the {\tt sparsediscrim} R
package was comparable to that of the diagonal classifiers in terms of
computational runtime. Moreover, when preliminary variable selection was applied
to the data in our classification study, the time to perform model selection
with our proposed classifier was nearly instantaneous. Hence, our proposed
\emph{HDRDA} classifier is competitive in terms of classification accuracy for
modern small-sample, high-dimensional data sets and requires substantially less
computing time than standard regularized classifiers, such as regularized
discriminant analysis proposed by \cite{Friedman:1989tm}.

Given the \emph{RDA} classifier has been shown to have excellent performance in
the high-dimensional setting \citep{Webb:2011vu} but is limited by its
computationally intense model selection procedure, our work enables the
\emph{RDA} classifier to be employed with small-sample, high-dimensional data in
practice. This is reassuring because the \emph{RDA} classifier remains widely
popular in the literature. In fact, variants of the RDA classifier have been
applied to microarray data \citep{Ching:2012fu,Li:2012ev,Tai:2007bk,Guo:2007te},
facial recognition
\citep{Ching:2012fu,Zhang:2010va,Dai:2007vd,Lu:2005hq,Pima:2004vw,Lu:2003we},
handwritten digit recognition \citep{Bouveyron:2007gx}, remote sensing
\citep{Tadjudin:1999fk}, seismic detection \citep{Anderson:2002kg}, and chemical
spectra \citep{Wu:1996us,Aeberhard:1993fp}.

\bibliographystyle{plainnat}
\bibliography{rda}

\end{document} 

