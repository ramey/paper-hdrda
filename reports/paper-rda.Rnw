\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Another Look at Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

<<knitr_setup, include=FALSE, echo=FALSE, cache=FALSE>>=
# Round digits throughout document including \Sexpr statements.
# See Yihui's response here:
# http://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr
options(digits = 3)

opts_chunk$set(fig.align = 'default', dev = 'png', message = FALSE,
               warning = FALSE, cache = TRUE, echo = FALSE, autodep = TRUE,
               fig.width = 10, fig.height = 10, out.width = "\\linewidth")
@ 
  
<<setup>>=
setwd("..")
library(ProjectTemplate)
load.project()

results <- lapply(results, function(res) {
  res <- lapply(res, function(res_d) {
    errors <- do.call(cbind, res_d$errors)
    res_d[["errors"]] <- NULL
    cbind(do.call(cbind, res_d), errors)
  })
  do.call(rbind, res)
})
results <- data.frame(do.call(rbind, results), stringsAsFactors = FALSE)

results$lambda <- as.numeric(results$lambda)
results$gamma <- as.numeric(results$gamma)
results$Dudoit <- as.numeric(results$Dudoit)
results$HDRDA <- as.numeric(results$HDRDA)
results$Guo <- as.numeric(results$Guo)
results$Pang <- as.numeric(results$Pang)
results$Tong <- as.numeric(results$Tong)
results$Witten <- as.numeric(results$Witten)

# To abide by Biometrika's requirements of no abbreviations, we rename the
# results of our HDRDA classifier as simply "Ours."
results$Ours <- results$HDRDA
results <- subset(results, select = -HDRDA)

results$d <- factor(results$d, labels = sort(unique(as.numeric(results$d))))

# Summary of the classification error rates for each data set and number of
# variables selected, d.
error_rates <- subset(results, select = -c(lambda, gamma))
m_error <- melt(error_rates, variable.name = "Classifier", value.name = "Error")
m_error$Classifier <- factor(m_error$Classifier)
error_rates <- ddply(m_error, .(data_set, d, Classifier), plyr:::summarize,
                     Avg_Error = mean(Error), Num_Reps = length(Error))
error_rates <- ddply(error_rates, .(data_set, d, Classifier), transform,
                     SE_Error = sqrt(Avg_Error * (1 - Avg_Error) / Num_Reps))
error_rates <- subset(error_rates, select = -Num_Reps)

# Candidate values of tuning parameters.
seq_lambda <- seq(0, 1, by = 0.05)
seq_gamma <- 10^seq.int(-1, 5)

# Summary of the optimal tuning-parameter estimates for each data set and number
# of variables selected, d.
hdrda_optimal <- subset(results, select = c(data_set, d, lambda, gamma))
hdrda_optimal$lambda <- factor(hdrda_optimal$lambda, levels = seq_lambda)
hdrda_optimal$gamma <- factor(hdrda_optimal$gamma, levels = seq_gamma)

hdrda_optimal <- ddply(hdrda_optimal, .(data_set, d), function(x) {
  x <- subset(x, select = c(lambda, gamma))
  melt(table(x))
})

@ 

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
 
A reliable supervised classification of tumors from gene expression microarray
data is essential for successful diagnosis and treatment of diseases such as
cancer; however, the analysis of microarray data often presents difficult
challenges because the number of gene expression levels is typically much
greater than the sample size. Consequently, several sparse and regularized
classifiers have been proposed, often with strict covariance assumptions or
eigenvalue adjustments to ensure that the classifiers are well-posed. Here, we
propose a novel modification of the regularized discriminant analysis classifier
popularized by \cite{Friedman:1989tm}. Our proposed classifier provides a
straightforward interpretation and employs a new dimension-reduction technique
to enable a computationally efficient model-selection procedure, thus overcoming
the computational shortcomings of \citeauthor{Friedman:1989tm}'s classifier for
high-dimensional data. Moreover, we demonstrate with several high-dimensional
microarray data sets that our proposed classifier is competitive and often
superior to several recently proposed sparse and regularized classifiers in
terms of classification accuracy. Finally, we provide an implementation of our
proposed classifier in the {\tt sparsediscrim} R package, available on CRAN.

\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of
thousands of genes in a single experiment to identify a subset of genes and
their expression patterns that either characterize a particular cell state or
facilitate an accurate prediction of future cell state.  We focus on the
classification of tissues and other biological samples into one of $K$ disjoint,
predefined cancer types, where our goal is to ascertain the class membership of
a new tissue sample based on gene-expression microarray data. However,
microarray data typically consist of a large number of gene expressions $p$
relative to the total sample size $N$. Moreover, standard benchmark microarray
data sets usually consist of between $p = 10,000$ and $55,000$ probes and fewer
than $N = 100$ observations, in which case standard supervised-learning methods
exhibit poor classification performance because of the \emph{curse of
  dimensionality} \citep{Bellman:1961tn}. Furthermore, some well-known
classifiers, such as linear and quadratic discriminant analysis, are
incalculable when $p > N$ because the sample covariance matrices employed are
singular. As a result, many researchers, including \cite{Mai:2012bf},
\cite{Fan:2012iq}, \cite{Merchante:2012vk}, \cite{Clemmensen:2011kr}, and
\cite{Huang:2010ju}, have introduced restrictions on the population covariance
matrices in linear and quadratic discriminant analysis to reduce number of
covariance parameters that must be estimated to ensure that the resulting
classifiers are calculable. For instance, \cite{Dudoit:2002ev} have modified the
linear and quadratic discriminant analysis classifiers by assuming that the
population covariance matrices are diagonal, resulting in classifiers that are
special cases of the Naive Bayes classifier \citep[Chapter 4]{Hastie:2008dt}.

Alternatively, several researchers, including \cite*{Zhang:2010va},
\cite{Ye:2009gd}, \cite{Ji:2008wp}, \cite*{Guo:2007te},
\cite*{Srivastava:2007ud}, \cite{Ye:2006vx}, and \cite{Ye:2005uu}, have proposed
regularization methods to improve the estimation of the population covariance
matrices before applying either linear or quadratic discriminant
analysis. Covariance-matrix regularization methods often are based on the
well-known ridge-regression approach of \cite{Hoerl:1970cd} and adjust the
eigenvalues of the sample covariance matrices to ensure positive
definiteness. For excellent overviews of the regularization problem in
discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we propose an improvement to the popular regularized discriminant
analysis classifier of \cite{Friedman:1989tm}, who has proposed pooling of the
covariance-matrix estimators from linear and quadratic discriminant analysis and
has additionally incorporated a covariance-matrix shrinkage term to ensure that
the classifier is well-posed. \citeauthor{Friedman:1989tm}'s classifier can
attain improved classification accuracy, compared to linear and quadratic
discriminant analysis when $p < N$. However, in its original form,
\citeauthor{Friedman:1989tm}'s classifier is impractical when $p \gg N$ because
the decision rule requires the calculation of the inverses and determinants of
large $p \times p$ covariance-matrix estimators for each class. Furthermore,
\citeauthor{Friedman:1989tm}'s classifier has two tuning parameters that are
selected via cross-validation, which adds a substantial computational burden to
the model-selection process. \citeauthor{Friedman:1989tm}'s classifier is
ill-advised on modern data sets because the model selection requires $O(Kp^3)$
calculations for each tuning-parameter pair.

In this paper we modify \citeauthor{Friedman:1989tm}'s classifier by first
constructing the pooling matrix as a convex combination of the covariance-matrix
estimators employed in linear and quadratic discriminant analysis and
demonstrate that this simplification improves the interpretation of the pooling
operation of \citeauthor{Friedman:1989tm}'s classifier. Given that the
assumption of equal population covariance matrices within linear discriminant
analysis is often too stringent in practice \citep{Clemmensen:2011kr}, our
proposed classifier can relax the linearity assumption by instead incorporating
unequal covariance matrices while retaining linear and quadratic discriminant
analysis as special cases. We then establish our classifier by applying
covariance-matrix regularization with a parameterization that includes a large
family of shrinkage methods, such as those from \cite{Srivastava:2007ww},
\cite{Rao:1971ul}, and other methods studied by \cite{Ramey:2013ji} and
\cite{Xu:2009fl}.

We show that our proposed classifier is invariant to any adjustments of the
eigenvalues corresponding to the null space of the pooled sample covariance
matrix. Applying a reasoning similar to \cite{Ye:2006tq}, we decompose the
decision rule of our classifier and show that the sum related to the null space
of the pooled sample covariance matrix can be discarded. Hence, no gain in
classification performance can be obtained by adjusting these zero
eigenvalues. This invariance property addresses the computational shortcomings
of \citeauthor{Friedman:1989tm}'s classifier and provides a substantial increase
in computational efficiency and model selection. In fact, we show that the
inverse and determinants can be reduced to calculations of $n_k \times n_k$
matrices, where $n_k$ is the sample size of the $k$th class $(k = 1, \ldots,
K)$. This reduction in calculation is clearly advantageous because $n_k$ is
often quite small in microarray data sets (e.g., 15--20 observations).

We also demonstrate that our proposed classifier is competitive with and often
superior to several recent classifiers designed for small-sample,
high-dimensional data. In particular, we compare our classifier with diagonal
linear discriminant analysis from \cite{Dudoit:2002ev} and the recently proposed
classifiers from \cite{Guo:2007te}, \cite{Pang:2009ik}, \cite{Tong:2012hw}, and
\cite{Witten:2011kc}. We conduct our comparison of the classification accuracy
of the competing classifiers using four small-sample, high-dimensional
benchmark microarray data sets.

\section{Regularized Discriminant Analysis}
\label{sec:rda}

\subsection{Preliminaries}
In discriminant analysis we wish to assign correctly an unlabeled
$p$-dimensional observation vector $x$ to one of $K$ unique, known classes
(or populations) by constructing a classifier from $N$ training
observations. Let $x_i = (x_{i1}, \ldots, x_{ip}) \in \mathbb{R}_{p \times
  1}$ be the $i$th observation $(i = 1, \ldots, N)$ with true, unique membership
$y_i \in \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{a \times b}$
denotes the matrix space of all $a \times b$ matrices over the real field
$\mathbb{R}$. We assume that $(x_i, y_i)$ is a realization from a mixture
distribution $p(x) = \sum_{k=1}^K p(x | \omega_k) p(\omega_k)$, where
$p(x | \omega_k)$ is the probability density function of the $k$th class and
$p(\omega_k)$ is the prior probability of class membership of the $k$th
class. We further assume that each of $p(\omega_k)$ is equal $(k = 1, \ldots,
K)$.

Quadratic discriminant analysis is the optimal Bayesian decision rule with
respect to a $0-1$ loss function when $p(x | \omega_k)$ is the probability
density function of the multivariate normal distribution with known mean vector
$\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\Sigma_k
\in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p
  \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite
matrices. Because $\mu_k$ and $\Sigma_k$ are typically unknown, we assign
an unlabeled observation $x$ to class $\omega_k$ with the sample quadratic
discriminant analysis classifier
\begin{align}
	D_{QDA}(x) = \argmin_{k}  (x - \xbar_k)'\widehat{\Sigma}_k^{-1}(x - \xbar_k)  + \log |\widehat{\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\Sigma}_k$ are the maximum likelihood
estimators for $\mu_k$ and $\Sigma_k$, respectively. If we assume
further that $\Sigma_k = \Sigma$ $(k = 1, \ldots, K)$, then the pooled
sample covariance matrix $\widehat{\Sigma}$ is substituted for $\widehat{
  \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\Sigma} = N^{-1} \sum_{k=1}^K n_k \widehat{\Sigma}_k \label{eq:pooled-cov}
\end{align}
is the maximum likelihood estimator for $\Sigma$. In this case \eqref{eq:qda}
reduces to the sample linear discriminant analysis classifier, and the
log-determinant is omitted because it is constant across all classes.

Throughout the document we also require the following additional notation. Let
$I_m$ be the $m \times m$ identity matrix, and let $0_{m \times p}$ be
the $m \times p$ matrix of zeros, such that $0_m$ is understood to denote
$0_{m \times m}$. Lastly, for $c \in \mathbb{R}$, let $c^+ = 1/c$ if $c \ne
0$ and $0$ otherwise. Let $A \oplus B$ denote the direct sum of $A \in
\mathbb{R}_{r \times r}$ and $B \in \mathbb{R}_{s \times s}$ \citep[Chapter
  1]{Lutkepohl:1996uz}. Let $A^+$ and $\mathcal{N}(A) = \{z \in \mathbb{R}_{p
  \times 1} : A z = 0_{m \times 1}\}$ denote the Moore-Penrose pseudoinverse and null space
of $A \in \mathbb{R}_{m \times p}$, respectively. Let $V^{\perp} = \{y
\in \mathbb{R}_{p \times 1} : y'v = 0\ \forall v \in V \}$ denote
the orthogonal complement of a vector space $V \subset \mathbb{R}_{p \times 1}$.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors
highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{
  \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest
eigenvalues are underestimated \citep{Seber:2004uh}, Moreover, if $p > n_k$,
then rank$(\widehat{\Sigma}_k) \le n_k$, which implies that at least $p -
n_k$ eigenvalues of $\widehat{\Sigma}_k$ are 0. Furthermore, although more
feature information is available to discriminate among the $K$ classes, if $p >
n_k$, \eqref{eq:qda} is incalculable because $\widehat{\Sigma}_k^{-1}$ does
not exist.

Several regularization methods, such as the methods considered by
\citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been
proposed in the literature to adjust the eigenvalues of $\widehat{\Sigma}_k$
so that \eqref{eq:qda} is calculable and has reduced variability in the standard
bias-variance tradeoff. A common form of the covariance-matrix regularization
applies a shrinkage factor $\gamma > 0$, so that
\begin{align}
	\widehat{\Sigma}_k(\gamma) =  \widehat{\Sigma}_k + \gamma I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation
\eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance
matrix $\widehat{\Sigma}_k$ towards $I_p$, thereby increasing the
eigenvalues of $\widehat{\Sigma}_k$ by $\gamma$. Specifically, the zero
eigenvalues are replaced with $\gamma$, so that \eqref{eq:ridge-estimator} is
positive definite. For additional covariance-matrix regularization methods, see
\cite{Ramey:2013ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\subsection{Friedman's Regularized Discriminant Analysis Classifier}

\cite{Friedman:1989tm} has proposed regularized discriminant analysis by
incorporating a weighted average of the sample covariance matrix $\widehat{
  \Sigma}_k$ and the pooled sample covariance matrix $\widehat{\Sigma}$ to
estimate $\Sigma_k$ with
\begin{align}
  \widehat{\Sigma}_k(\lambda) = n_k^{-1}(\lambda) S_k(\lambda) \quad (k =
  1, \ldots, K),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda N$, $
S_k = n_k \widehat{\Sigma}_k$, $S = \sum_{k=1}^K S_k$, and $
S_k(\lambda) = (1 - \lambda) S_k + \lambda S$. We can interpret
\eqref{eq:sig-lambda} as a covariance-matrix estimator that borrows from
$\widehat{\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $
\Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to
$\widehat{\Sigma}_k$ used in \eqref{eq:qda}. Alternatively, if $\lambda = 1$,
\eqref{eq:sig-lambda} reduces to \eqref{eq:pooled-cov}.

To further improve the estimation of $\Sigma_k$ and to stabilize the inverse
of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased
covariance-matrix estimator
\begin{align}
	\widehat{\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\Sigma}_k(\lambda)\}}{p} I_p,\label{eq:sig-rda}
\end{align}
where $\gamma \in [0, 1]$ is a regularization parameter that controls the
shrinkage of \eqref{eq:sig-rda} towards $I_p$ and is weighted by the average
of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter
$\lambda$ controls the amount borrowed from $\widehat{\Sigma}$ to estimate
$\Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the
amount of eigenvalue shrinkage applied. Finally, by substituting
\eqref{eq:sig-rda} into \eqref{eq:qda}, we have the regularized discriminant
analysis classifier
\begin{align}
	D_{RDA}(x) = \argmin_{k}  (x - \xbar_k)'\widehat{\Sigma}_k(\lambda, \gamma)^{-1}(x - \xbar_k)  + \log |\widehat{\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}
If $\gamma$ is sufficiently small, \eqref{eq:sig-rda} can be singular or
near-singular. For this case \cite{Friedman:1989tm} has suggested replacing the
zero eigenvalues of \eqref{eq:sig-rda} with a small positive constant that
permits a numerically stable matrix inversion.

\section{High-Dimensional Regularized Discriminant Analysis}

\subsection{Improvements to \citeauthor{Friedman:1989tm}'s Classifier}

Here, we define our adaptation of \citeauthor{Friedman:1989tm}'s classifier by
first simplifying the formulation of $\widehat{\Sigma}_k(\lambda)$ in
\eqref{eq:sig-lambda} and demonstrating its clear interpretation as a linear
combination of the crossproducts of the training observations centered by their
respective class sample means. We define the convex combination
\begin{align}
  \widehat{\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\Sigma}_k + \lambda
  \widehat{\Sigma}, \quad (k = 1, \ldots, K)\label{eq:sig-lambda-alternative}
\end{align}
where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we avoid the
convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By
rewriting \eqref{eq:sig-lambda-alternative} in terms of the observations $
x_i$ $(i = 1, \ldots, N)$, each centered by its class sample mean, we attain a
clear interpretation of $\widehat{\Sigma}_k(\lambda)$. That is,
\begin{align}
	\widehat{\Sigma}_k(\lambda) &= \widehat{\Sigma}_k + \lambda (\widehat{\Sigma} - \widehat{\Sigma}_k) \nonumber\\
%	&= \widehat{\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\Sigma}_{k'}  - \widehat{\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\Sigma}_{k'} \nonumber \\
%	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) x_i x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) x_i x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) x_i x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of
each of the $N$ observations to estimate $\Sigma_k$. Hence, for $0 < \lambda
\le 1$, we estimate $\Sigma_k$ with $N$ rather than $n_k$
observations. Below, we will show that the pooling operation is advantageous in
increasing the rank of each $\widehat{\Sigma}_k(\lambda)$ from
rank$(\widehat{\Sigma}_k)$ to rank$(\widehat{\Sigma})$ for $0 < \lambda
\le 1$. Notice that if $\lambda = 0$, then the observations from the remaining
$K - 1$ classes do not contribute to the estimation of $\Sigma_k$,
corresponding to $\widehat{\Sigma}_k$. Furthermore, if $\lambda = 1$, the
weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to
$\widehat{\Sigma}$. For brevity, when $\lambda = 1$, we define $X =
[\sqrt{c_{1k}(1)} x_1', \ldots, \sqrt{c_{Nk}(1)} x_N']'$ such that
$\widehat{\Sigma} = X' X$. Similarly, for $\lambda = 0$, we define
$X_k = [\sqrt{c_{1k}(0)} x_1', \ldots, \sqrt{c_{Nk}(0)} x_N']'$ such
that $\widehat{\Sigma}_k = X_k' X_k$. In Figure
\ref{fig:hdrda-contours} we plot the contours of five multivariate normal
populations for $\lambda = 0$ with unequal covariance matrices. As $\lambda$
approaches 1, the contours become more similar, resulting in identical contours
for $\lambda = 1$.

\[ \left[\text{Figure \ref{fig:hdrda-contours} goes about here }\right]\]

As we have discussed above, several eigenvalue adjustment methods have been
proposed to increase eigenvalues (approximately) equal to 0. Here, we define the
eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\Sigma}_k = \alpha_k \widehat{\Sigma}_k(\lambda) + \gamma I_p,\label{eq:hdrda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\gamma \ge 0$ is an eigenvalue-shrinkage
constant. The choice of $\alpha_k$ allows for a flexible formulation of
covariance-matrix estimators. For instance, if $\alpha_k = 1$, $k = 1, \ldots,
K$, then \eqref{eq:hdrda-cov} resembles \eqref{eq:ridge-estimator}. Similarly,
if $\alpha_k = 1 - \gamma$, then \eqref{eq:hdrda-cov} has a form similar to
\eqref{eq:sig-rda}. Now, substituting \eqref{eq:hdrda-cov} into \eqref{eq:qda},
we define the high-dimensional regularized discriminant analysis classifier
\begin{align}
	D_{HDRDA}(x) = \argmin_{k}  (x - \xbar_k)'\tilde{\Sigma}_k^{+}(x - \xbar_k)  + \log |\tilde{\Sigma}_k|. \label{eq:hdrda}
\end{align}
For $\gamma > 0$, $\tilde{\Sigma}_k$ is nonsingular such that
$\tilde{\Sigma}_k^{-1}$ can be substituted for $\tilde{\Sigma}_k^{+}$ in
\eqref{eq:hdrda}. If $\gamma = 0$, we explicitly set $|\tilde{\Sigma}_k|$
equal to the product of the positive eigenvalues of $\tilde{\Sigma}_k$.

\subsection{Enhanced Computational Efficiency}

Next, we demonstrate that our proposed classifier in \eqref{eq:hdrda} lends
itself to a more efficient calculation and a rapid model selection of the
parameters $\lambda$ and $\gamma$\footnote{TODO: Briefly mention the updating
  trick employed in the R package}. We decompose \eqref{eq:hdrda} into a sum of
two components, where the first summand consists of matrix operations applied to
low-dimensional matrices and the second summand corresponds to the null space of
$\widehat{\Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix
operations performed on the null space of $\widehat{\Sigma}$ yield constant
quadratic forms across all classes and can be omitted. For $p \gg N$, the
constant component involves determinants and inverses of high-dimensional
matrices, and by ignoring these calculations, a substantial reduction in
computational costs is achieved relative to Friedman's classifier in
\eqref{eq:rda}\footnote{TODO: Add a brief comparison of the computations of
  \emph{RDA} and \emph{HDRDA} in terms of Big Oh notation}. Furthermore, a
byproduct is that adjustments to the associated eigenvalues have no effect on
\eqref{eq:hdrda}. Lastly, we utilize the singular value decomposition to
calculate efficiently the eigenvalue decomposition of $\widehat{\Sigma}$,
further reducing the computational costs of our proposed classifier.

First, we require the following relationship regarding the null spaces of
$\widehat{\Sigma}_k(\lambda)$, $\widehat{\Sigma}$, and $\widehat{
  \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\Sigma}_k$ and $\widehat{\Sigma}$ be the maximum
likelihood estimators of $\Sigma_k$ and $\Sigma$, respectively. Let
$\widehat{\Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, $\mathcal{N}\{\widehat{\Sigma}_k(\lambda)\} \subset \mathcal{N}(\widehat{
  \Sigma}) \subset \mathcal{N}(\widehat{\Sigma}_k)$ $(k = 1, \ldots, K)$.
\end{lemma}
\begin{proof}
Let $z \in \mathcal{N}\{\widehat{\Sigma}_k(\lambda)\}$ for some $k = 1,
\ldots, K$. Hence, $0 = z' \widehat{\Sigma}_k(\lambda) z = (1 -
\lambda) z' \widehat{\Sigma}_k z + \lambda z' \widehat{
  \Sigma} z$. Because $\widehat{\Sigma}_k, \widehat{\Sigma}\in
\mathbb{R}_{p \times p}^{\ge}$, we have $z \in \mathcal{N}(\widehat{
  \Sigma})$ and $z \in \mathcal{N}(\widehat{\Sigma}_k)$. In particular,
we have that $\mathcal{N}\{\widehat{\Sigma}_k(\lambda)\} \subset
\mathcal{N}(\widehat{\Sigma})$. Now, suppose $z \in
\mathcal{N}(\widehat{\Sigma})$. Similarly, we have that $0 = z'
\widehat{\Sigma} z = N^{-1} \sum_{k = 1}^K n_k z' \widehat{
  \Sigma}_k z$, which implies that $z \in \mathcal{N}(\widehat{
  \Sigma}_k)$ because $\widehat{\Sigma}_k \in \mathbb{R}_{p \times
  p}^{\ge}$. Therefore, $\mathcal{N}(\widehat{\Sigma}) \subset
\mathcal{N}(\widehat{\Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative
expression for $\tilde{\Sigma}_k$ in terms of the matrix of eigenvectors of
$\widehat{\Sigma}$. Let $\widehat{\Sigma} = U D U'$ be the
eigendecomposition of $\widehat{\Sigma}$ such that $D \in \mathbb{R}_{p
  \times p}^{\ge}$ is the diagonal matrix of eigenvalues of $\widehat{\Sigma}$
with $D = D_q \oplus 0_{p-q}$, $D_q \in \mathbb{R}_{q \times
  q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of
$\widehat{\Sigma}$, the columns of $U \in \mathbb{R}_{p \times p}$ are
the corresponding orthonormal eigenvectors of $\widehat{\Sigma}$, and
rank$(\widehat{\Sigma}) = q$. Then, we partition $U = (U_1, 
U_2)$ such that $U_1 \in \mathbb{R}_{p \times q}$ and $U_2 \in
\mathbb{R}_{p \times (p - q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\Sigma} = U D U'$ be the eigendecomposition of
$\widehat{\Sigma}$ as above, and suppose that rank$(\widehat{\Sigma}) =
q \le p$. Then, we have
\begin{align}
	\tilde{\Sigma}_k &= U(W_k \oplus \gamma I_{p-q})U' \quad
  (k = 1, \ldots, K),\label{eq:rda-matrix}
\intertext{where}
W_k &= \alpha_k \{(1 - \lambda) U_1' \widehat{\Sigma}_k U_1 + \lambda D_q\} + \gamma I_{q}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $U_2$ span the null space
of $\widehat{\Sigma}$, which implies that $\widehat{\Sigma}_k U_2 =
0_{p \times (p - q)}$. Hence, $U' \widehat{\Sigma}_k U = 
U_1' \widehat{\Sigma}_k U_1 \oplus 0_{p-q}$ $(k = 1, \ldots,
K)$. Thus, $U' \tilde{\Sigma}_k U = \alpha_k \{(1 - \lambda) U'
\widehat{\Sigma}_k U + \lambda D\} + \gamma I_p$, and
\eqref{eq:rda-matrix} follows because $U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we
have the following corollary.

\begin{cor}
Let $\widehat{\Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$,
  rank$\{\widehat{\Sigma}_k(\lambda)\} = q$ $(k = 1, \ldots, K)$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $x_i$ into the estimation of $\Sigma_k$, we
increase the rank of $\widehat{\Sigma}_k(\lambda)$ to $q$ if $\lambda \ne
0$. Next, we provide an essential result that enables us to prove that
\eqref{eq:hdrda} is invariant to adjustments to the eigenvalues of $\tilde{
  \Sigma}_k$ corresponding to the null space of $\widehat{\Sigma}$ for any
unlabeled observation $x \in \mathbb{R}_{p \times 1}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $U_2$ be defined as above. Then, for all $x \in \mathbb{R}_{p \times
  1}$, $U_2' (x - \xbar_k) = U_2' (x - \xbar_{k'})$ $(1 \le k,
k' \le K)$.
\end{lemma}
\begin{proof}
Let $x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$.
Recall that $U_2 \in \mathcal{N}(\widehat{\Sigma})$, which implies that
$U_2' \in \mathcal{C}(\widehat{\Sigma})^{\perp}$ \citep[Lemma
  1.2.5]{Kollo:2005vp}. Now, because $x_i \in \mathcal{C}(\widehat{
  \Sigma})$ $(i = 1, \ldots, N)$, $U_2' x_i = 0_{(p-q) \times 1}$.
Hence, $0_{(p-q) \times 1} = \sum_{i=1}^N \beta_i U_2'x_i = U_2'(\xbar_k -
\xbar_{k'})$, where $\beta_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i =
k') n_k \}$. Therefore, $U_2' (x - \xbar_k) = U_2' (x - \xbar_{k'})$.
\end{proof}

We now present our main result, where we decompose the decision rule of our
proposed classifier and show that the term requiring the largest computational
costs does not contribute to the classification of an unlabeled observation
performed using Lemma \ref{lemma:RDA-constant-term}. Hence, we reduce
\eqref{eq:hdrda} to an equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\Sigma}_k$ and $W_k$ be defined as in \eqref{eq:rda-matrix}
and \eqref{eq:Wk}, respectively, and let $U_1$ be defined as above. Then,
the decision rule in \eqref{eq:hdrda} is equivalent to
	\begin{align}
		D_{HDRDA}(x) &= \argmin_k  (x - \xbar_k)' U_1 W_k^{-1} U_1' (x - \xbar_k) + \log | W_k |. \label{eq:hdrda-decomposed}
	\end{align}
\end{thm}
\begin{proof}
From \eqref{eq:rda-matrix}, we have that $\tilde{\Sigma}_k^{+} = U(
W_k^{-1} \oplus \gamma^{+} I_{p-q} )U'$ and $|\tilde{\Sigma}_k| =
\gamma^{p-q}| W_k |$ $(k = 1, \ldots, K)$. Therefore, for all $x \in
\mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(x - \xbar_k)' \tilde{\Sigma}_k^{+}(x - \xbar_k)  + \log |\tilde{\Sigma}_k| &= (x - \xbar_k)' U_1 W_k^{-1} U_1' (x - \xbar_k)\\
	&+ \gamma^{+} (x - \xbar_k)' U_2 U_2' (x - \xbar_k) + \log | W_k | + (p - q) \log \gamma.
	\end{align*}
Because $\gamma$ is constant for $k = 1, \ldots, K$, we can omit the $(p - q)
\log \gamma$ term and particularly avoid the calculation of $\log 0$ for $\gamma
= 0$. Then, the proof follows from Lemma \ref{lemma:RDA-constant-term} because
$U_2' (x - \xbar_k)$ is constant for $k = 1, \ldots, K$.
\end{proof}

\subsection{Model Selection}

Thus far, we have shown that our classifier in \eqref{eq:hdrda} is invariant to
the term $U_2$, thus yielding an equivalent classifier in
\eqref{eq:hdrda-decomposed} with a substantial reduction in the computational
complexity. Here, we provide two\footnote{TODO: Two? Perhaps, I mean one and
  then the Fast SVD below. If so, clarify this.} additional results that further
improve the computational performance of our classifier. First, we demonstrate
that the inverse and determinants of $W_k$ can be written so that they are
performed on $n_k \times n_k$ matrices rather than the larger $q \times q$
matrices, resulting in further improvements to the computational efficiency.

\begin{proposition}\label{proposition:rda-W_k}
Let $W_k$ be defined as above $(k = 1, \ldots, K)$. Then, $|W_k| = |
\Gamma_k| |Q_k|$ and
	\begin{align}
		W_k^{-1} &= \Gamma_k^{-1} - \alpha_k(1 - \lambda) \Gamma_k^{-1} U_1' X_k' Q_k^{-1} X_k U_1 \Gamma_k^{-1},\label{eq:W_k_inv}
	\end{align}
where $Q_k = I_{n_k} + \alpha_k(1- \lambda) X_k U_1 
\Gamma_k^{-1} U_1' X_k'$ and $\Gamma_k = \alpha_k \lambda D_q +
\gamma I_q$.
\end{proposition}
\begin{proof}
To calculate $|W_k|$, we apply Theorem 18.1.1 from \cite{Harville:2008wja},
which states that $|A + B T C| = |A| |T| |T^{-1} +
C A^{-1} B|$. Thus, setting $A = \Gamma_k$, $B =
\alpha_k (1 - \lambda) U_1' X_k'$, $T = I_{n_k}$, and $C =
X_k U_1$, we have $|W_k| = |\Gamma_k| |Q_k|$. Also,
\eqref{eq:W_k_inv} follows from the well-known Sherman-Woodbury formula
\citep[Theorem 18.2.8]{Harville:2008wja}.
\end{proof}

Thus far, we have constructed each $\tilde{\Sigma}_k$ in terms of the first
$q$ eigenvectors of $\widehat{\Sigma}$. By utilizing a technique similar to
the so-called kernel trick, we can obtain $U_1$ by computing the
eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$
\citep{Hastie:2008dt}. Recall that the pooled sample covariance matrix
$\widehat{\Sigma} = X' X$. Then, because $X = M D^{1/2}
U'$ by the singular value decomposition, we have the eigendecomposition
$\widehat{\Sigma} = U D U'$, where $M \in \mathbb{R}_{N
  \times N}$ is orthogonal, $D^{1/2} \in \mathbb{R}_{N \times N}^{\ge}$ is a
diagonal matrix consisting of the singular values of $X$, and $U \in
\mathbb{R}_{p \times N}$ is orthogonal. Now, $X X' = M D M'$
is the eigenvalue decomposition of $X X'$, where $D$ is the matrix
of eigenvalues of $X X'$ and the columns of $M$ are the
corresponding eigenvectors. Hence, we obtain $M$ and $D$ from an
eigendecomposition of $X X' \in \mathbb{R}_{N \times N}$. Next, we
compute $U = X' M D^{+/2}$, where $D^{+/2} = D_q^{-1/2}
\oplus 0_{N-q}$. We then determine $q$ by computing the number of
numerically nonzero eigenvalues present in $D$, where $q$ is the number of
eigenvalues that exceeds some tolerance value, say, $\epsilon = 1 \times
10^{-6}$. After calculating $q$, we extract $U_1$ by retaining only the
first $q$ columns of $U$.

\section{High-Dimensional Microarray Data Sets}

In this section, we describe four high-dimensional microarray data sets and
compare our proposed classifier with five classifiers recently proposed for
small-sample, high-dimensional data. Our competitors include diagonal linear
discriminant analysis from \cite{Dudoit:2002ev}, shrunken centroids regularized
discriminant analysis from \cite{Guo:2007te}, and penalized linear discriminant
analysis from \cite{Witten:2011kc}. We also consider two modifications of
diagonal linear discriminant analysis from \cite{Tong:2012hw} and
\cite{Pang:2009ik}, where the former employs an improved mean estimator and the
latter utilizes an improved variance estimator. Below, we refer to each
classifier by the first author's surname.

We evaluated the classification performance of each classifier by randomly
partitioning the data such that $2/3$ of the observations were allocated as
training data and the remaining $1/3$ of the observations were allocated as a
test data set. For each random partition, we applied the feature-selection
method from \cite{Dudoit:2002ev} to the training data to choose the $d$ features
having the largest ratio of their between-group to within-group sums of squares.
We then filtered the test data set with the corresponding $d$ features. After
training each classifier from the filtered training data, we classified the
filtered test data sets and computed the proportion of mislabeled test
observations to estimate the classification error rate for each
classifier. Repeating this process 1000 times, we computed the average of the
error-rate estimates for each classifier. We remark that our simulation design
avoids the selection bias discussed in \cite{Ambroise:2002fa}. We did not
include \citeauthor{Friedman:1989tm}'s classifier in our studies to avoid its
excessive computations. In Table \ref{tab:data-summary}, we summarize the
attributes of each microarray data set.

We used version 3.0.0 of the open-source statistical software {\tt R} for the
classification study. For each classifier's {\tt R} implementation, we used the
default settings with the exception that we explicitly set prior probabilities
as equal. For our classifier, we set $\alpha_k = 1$ $(k = 1, \ldots, K)$, so
that the covariance-matrix estimator \eqref{eq:hdrda-cov} employed had a form
resembling \eqref{eq:ridge-estimator}. We estimated $\lambda$ from a grid of 21
equidistant candidate values between 0 and 1, inclusively. Similarly, we
estimated $\gamma$ from a grid consisting of the values $10^{-1}, \ldots, 10^4$,
and $10^5$. We selected optimal estimates of $\lambda$ and $\gamma$ using
10-fold cross-validation \citep{Hastie:2008dt}.

\[ \left[\text{Table 1 goes about here }\right]\]

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC)
through hybridization to microarrays from 127 individuals resulting in 22,283
sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD),
and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to
improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using
the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide
microarray from 105 samples of 10 types of soft tissue tumors. This included 16
samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma
(MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS),
15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of
myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of
malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21
samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl}
determined from their data that these 10 types fell into 4 broader groups: (1)
SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and
pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following
\cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at
least 15 observations.

\subsection{\cite{Shipp:2002ka} Data Set}

Approximately 30-40\% of adult non-Hodgkin lymphomas are diffuse large B-cell
lymphomas (DLBCLs) \cite{Shipp:2002ka}.  However, only a small proportion of
DLBCL patients are cured with modern chemotherapeutic regimens.  Several models
have been proposed, such as the International Prognostic Index (IPI), to
determine a patient's curability.  These models rely on clinical covariates,
such as age, to determine if the patient can be cured and are often
ineffective. \cite{Shipp:2002ka} have argued that more effective means are
desired to determine a patient's curability. The authors measured 6,817 gene
expression levels from 58 DLBCL patient samples with customized cDNA
(lymphochip) microarrays to investigate the curability of patients treated with
cyclophosphamide, adriamycin, vincristine, and prednisone (CHOP)-based
chemotherapy.  Among the 58 DLBCL patient samples, 32 are from cured patients
while 26 are from patients with fatal or refractory disease.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from
surgery patients between 1995 and 1997. The authors used oligonucleotide
microarrays containing probes for approximately 12,600 genes and expressed
sequence tags. They have reported that 102 of the radical prostatectomy
specimens are of high quality: 52 prostate tumor samples and 50 non-tumor
prostate samples.
 
\subsection{Classification Results}

For the microarray data sets, we computed error-rate estimates for the competing
classifiers after reducing the dimension of the data sets with feature selection
to $d = 100, \ldots, 1000$, similar to the values of $d$ considered by
\cite{Xu:2009fl}. To compare misclassification rates between different values of
$d$, we applied each value of $d$ considered to the same random allocation of
training and test data. Hence, for a given random partition, the set of features
selected for a small value of $d$ is a proper subset of the features selected
for a larger value of $d$. We report the average of the error-rate estimates for
each value of $d$ in Figure \ref{fig:error-rates}. Approximate standard errors
were no greater than \Sexpr{max(error_rates[["SE_Error"]])}.

Our classifier was superior in classification performance for the majority of
the simulations. For the Burczysnki, Shipp, and Singh data sets, our classifier
outperformed its competitors for all values of $d$ examined. For the Nakayama
data set, our classifier was outperformed by the Dudoit, Pang, and Tong
classifiers for small values of $d$ but attained classification rates superior
to its competitors for larger values of $d$. Furthermore, Figure
\ref{fig:error-rates} emphasizes two strengths of our classifier: reliability of
the classifier for small $d$ and stability of the classifier for large $d$. In
contrast to the other classifiers, our classifier exhibited less variability in
its misclassification estimates for small values of $d$. Also, our classifier
yielded superior classification accuracy as $d$ increased and was hardly
effected by the inclusion of more variables in the data sets. We attribute the
apparent stability of the error-rate estimates of our classifier for large
values of $d$ to its novel dimension-reduction technique.

The error-rate estimates for the Dudoit, Pang, Tong, and Witten classifiers were
comparable for all values of $d$ examined. Furthermore, the error-rate estimates
of the Dudoit, Pang, and Tong classifiers were approximately equal throughout
our study, and the Witten classifier yielded approximately equal error-rate
estimates for the Burczysnki, Shipp, and Singh data sets. Furthermore, the
error-rate estimates for the Dudoit and Tong classifiers were nearly identical,
which suggests that the improved mean estimator employed by \cite{Tong:2012hw}
provided little improvement to the classification performance of diagonal linear
discriminant analysis.

The classification performance of the the Dudoit, Pang, Tong, and Witten
classifiers for the Shipp and Singh data sets revealed the inability of these
methods to handle larger values of $d$. After closer examination of the
classification studies performed in the authors' original papers, we noticed
that classification accuracy estimates were reported only for small values of
$d$, thereby ignoring the inclusion of a larger number of variables. Hence, we
advise that practioners be aware of the necessity to filter a large number of
features before employing these methods, lest their classification performance
can degrade.

The popular Guo classifier is most competitive over large values of $d$ for the
Shipp and Singh data sets. However, for the Burczynski and Nakayama data sets,
the implementation of the Guo method in the {\tt rda} package often classified
every test observation into a single class, thus yielding larger error rates
than its competitors. Furthermore, the Guo classifier exhibited unreliable
classification performance for smaller values of $d$. The Guo classifier
required a larger subset of the features to perform competitively in our study.

\[ \left[\text{Figure \ref{fig:error-rates} goes about here }\right]\]

Next, we examine the optimal estimates of the tuning parameters for our
classifier. For brevity, we provide graphical summaries of the optimal estimates
of $\lambda$ and $\gamma$ in the Supplementary Material. For each of the four
data sets, the distribution of the optimal values of $\lambda$ selected through
cross-validation often had a notable bathtub shape, resembling a Beta
distribution with both shape parameters less than unity. Specifically, the
optimal values of $\lambda$ were typically near 0 or near 1, corresponding to
quadratic and linear discriminant analysis, respectively. For instance, with
smaller values of $d$ applied to the Shipp data set, the range of optimal values
of $\lambda$ was typically no larger than 0.05, indicating there were a large
number of training data sets for which we can safely assume the covariance
matrices are unequal or nearly unequal. As $d$ increased, the majority of the
optimal values of $\lambda$ were selected as unity, thereby an increased number
of variables resulted in our classifier preferring the more parsimonious
assumption of equal covariance matrices. In this instance we can see the
flexibility of our classifier to employ the more realistic assumption that the
class covariance matrices are unequal, but if optimal, we can employ the more
parsimonious assumption of equal covariance matrices.

\section{Discussion}

In our simulation study, we compared our classifier to several recently proposed
classifiers, including the Dudoit, Pang, Tong, and Guo classifiers, each of
which assumes independence of the features conditional on class
membership. Despite the rapid computational performance of diagonal classifiers
and the reduction in the number of parameters to estimate, \cite{Mai:2012bf} and
\cite{Fan:2012iq} have noted that diagonal classifiers can often yield inferior
classification performance to other classification methods. We have shown that
our proposed classifier often yields superior classification accuracy to the
diagonal classifiers considered, confirming the assertions of \cite{Mai:2012bf}
and \cite{Fan:2012iq}. Furthermore, the implementation of our classifier in the
{\tt sparsediscrim} R package available on CRAN was comparable to that of the
diagonal classifiers in terms of computational runtime. Moreover, when
preliminary variable selection was applied to the data in our classification
study, the time to perform model selection with our proposed classifier was
nearly instantaneous.


\bibliographystyle{plainnat}
\bibliography{rda}

\clearpage

<<data_summary, results="asis">>=
# Table containing a summary of the data sets
data_summary <- subset(describe_data(), author %in% unique(results$data_set))
data_summary$K[data_summary$author == "nakayama"] <- 5
rownames(data_summary) <- NULL
colnames(data_summary)[1:3] <- c("Author", "Year", "N")
data_summary$Author <- Hmisc:::capitalize(as.character(data_summary$Author))
data_summary$Author <- with(data_summary, paste0(Author, " et al. (", Year, ")"))
data_summary <- subset(data_summary, select = -Year)
print(xtable(data_summary, caption = "Summary of high-dimensional microarray data sets.",
             label = "tab:data-summary", digits = 0), include.rownames = FALSE)

@ 

\begin{figure}
<<hdrda_contours, fig.width=8, fig.height=8, out.width='.49\\linewidth', fig.show='hold'>>=
rda_contours(lambda = 0)
rda_contours(lambda = 1/4)
rda_contours(lambda = 3/4)
rda_contours(lambda = 1)
@ 
\caption{Contours of five multivariate normal populations as a function of the
  pooling parameter $\lambda$.}
\label{fig:hdrda-contours}
\end{figure}

\begin{figure}
<<error_rates>>=
error_rates$data_set <- Hmisc:::capitalize(error_rates$data_set)
p <- ggplot(error_rates, aes(x = d, y = Avg_Error, group = Classifier))
p <- p + geom_line(aes(color = Classifier, linetype = Classifier), size = 1.5)
p <- p + facet_wrap(~ data_set) + ylim(c(0, max(error_rates$Avg_Error))) + theme_bw()
p <- p + xlab("Number of Variables Selected") + ylab("Average Error Rate")
p  + theme(strip.text.x = element_text(size = 12))
@ 
\caption{Average error-rate estimates of the competing classifiers as a function
  of $d$. Approximate standard errors are no greater than \Sexpr{max(error_rates[["SE_Error"]])}.}
\label{fig:error-rates}
\end{figure}

\begin{figure}
<<optimal_estimates>>=
hdrda_optimal$lambda <- factor(hdrda_optimal$lambda)
hdrda_optimal$gamma <- factor(hdrda_optimal$gamma)
p <- ggplot(subset(hdrda_optimal, data_set == "shipp" & d %in% c(100, 500, 1000)), aes(lambda))
p <- p + geom_bar(aes(fill = gamma, weight = value))
p <- p + facet_grid(d ~ ., labeller = label_bquote(d == .(x))) + theme_bw()
p + theme(strip.text.y = element_text(size = 16))
@ 
\caption{Counts of the optimal tuning-parameter estimates for the \emph{HDRDA}
  classifier for the Shipp data set for $d = 100$, $500$, and $1000$.}
\label{fig:optimal-shipp}
\end{figure}



\end{document} 
