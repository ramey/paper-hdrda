\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Another Look at Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

<<knitr_setup, include=FALSE, echo=FALSE, cache=FALSE>>=
# Round digits throughout document including \Sexpr statements.
# See Yihui's response here:
# http://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr
options(digits = 3)

opts_chunk$set(fig.align = 'default', dev = 'png', message = FALSE,
               warning = FALSE, cache = TRUE, echo = FALSE, autodep = TRUE,
               fig.width = 10, fig.height = 10, out.width = "\\linewidth")
@ 
  
<<setup>>=
setwd("..")
library(ProjectTemplate)
load.project()

results <- lapply(results, function(res) {
  res <- lapply(res, function(res_d) {
    errors <- do.call(cbind, res_d$errors)
    res_d[["errors"]] <- NULL
    cbind(do.call(cbind, res_d), errors)
  })
  do.call(rbind, res)
})
results <- data.frame(do.call(rbind, results), stringsAsFactors = FALSE)

results$lambda <- as.numeric(results$lambda)
results$gamma <- as.numeric(results$gamma)
results$Dudoit <- as.numeric(results$Dudoit)
results$HDRDA <- as.numeric(results$HDRDA)
results$Guo <- as.numeric(results$Guo)
results$Pang <- as.numeric(results$Pang)
results$Tong <- as.numeric(results$Tong)
results$Witten <- as.numeric(results$Witten)

results$d <- factor(results$d, labels = sort(unique(as.numeric(results$d))))

# Summary of the classification error rates for each data set and number of
# variables selected, d.
error_rates <- subset(results, select = -c(lambda, gamma))
m_error <- melt(error_rates, variable.name = "Classifier", value.name = "Error")
error_rates <- ddply(m_error, .(data_set, d, Classifier), plyr:::summarize,
                     Avg_Error = mean(Error), Num_Reps = length(Error))
error_rates <- ddply(error_rates, .(data_set, d, Classifier), transform,
                     SE_Error = sqrt(Avg_Error * (1 - Avg_Error) / Num_Reps))
error_rates <- subset(error_rates, select = -Num_Reps)

# Candidate values of tuning parameters.
seq_lambda <- seq(0, 1, by = 0.05)
seq_gamma <- 10^seq.int(-1, 5)

# Summary of the optimal tuning-parameter estimates for each data set and number
# of variables selected, d.
hdrda_optimal <- subset(results, select = c(data_set, d, lambda, gamma))
hdrda_optimal$lambda <- factor(hdrda_optimal$lambda, levels = seq_lambda)
hdrda_optimal$gamma <- factor(hdrda_optimal$gamma, levels = seq_gamma)

hdrda_optimal <- ddply(hdrda_optimal, .(data_set, d), function(x) {
  x <- subset(x, select = c(lambda, gamma))
  melt(table(x))
})

@ 

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
 
A reliable supervised classification of tumors from gene expression microarray
data is essential for successful diagnosis and treatment of diseases such as
cancer; however, the analysis of microarray data often presents difficult
challenges because the number of gene expression levels is typically much
greater than the sample size. Consequently, several sparse and regularized
classifiers have been proposed, often with strict covariance assumptions or
eigenvalue adjustments to ensure that the classifiers are well-posed. Here, we
consider an improved version of the regularized discriminant analysis
(\emph{RDA}) classifier popularized by \cite{Friedman:1989tm}. That is, we
propose the high-dimensional \emph{RDA} (\emph{HDRDA}) classifier, a novel
modification of the \emph{RDA} classifier, that provides a straightforward
interpretation and employs a new dimension-reduction technique to enable a
computationally efficient model-selection procedure, thus overcoming the
computational shortcomings of the \emph{RDA} classifier for high-dimensional
data. Moreover, we demonstrate with several high-dimensional microarray data
sets that our proposed classifier is competitive and often superior to several
recently proposed sparse and regularized classifiers in terms of classification
accuracy. Finally, we provide an implementation of the \emph{HDRDA} classifier in
the {\tt sparsediscrim} R package, available on CRAN.

\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of
thousands of genes in a single experiment to identify a subset of genes and
their expression patterns that either characterize a particular cell state or
facilitate an accurate prediction of future cell state.  We focus on the
classification of tissues and other biological samples into one of $K$ disjoint,
predefined cancer types, where our goal is to ascertain the class membership of
a new tissue sample based on gene-expression microarray data. However,
microarray data typically consist of a large number of gene expressions $p$
relative to the total sample size $N$. Moreover, standard benchmark microarray
data sets usually consist of between $p = 10,000$ and $55,000$ probes and fewer
than $N = 100$ observations, in which case standard supervised-learning methods
exhibit poor classification performance because of the \emph{curse of
  dimensionality} \citep{Bellman:1961tn}. Furthermore, well-known classifiers,
including linear discriminant analysis \emph{(LDA)} and quadratic discriminant
analysis \emph{(QDA)}, are incalculable when $p > N$ because the sample
covariance matrices employed in both classifiers are singular. As a result, many
researchers, including \cite*{Mai:2012bf}, \cite*{Fan:2012iq},
\cite*{Merchante:2012vk}, \cite*{Clemmensen:2011kr}, and \cite*{Huang:2010ju},
have introduced restrictions on the population covariance matrices in the
\emph{LDA} and \emph{QDA} models to reduce number of covariance parameters that
must be estimated to ensure that the resulting classifiers are calculable.  For
instance, \cite{Dudoit:2002ev} have proposed the diagonal \emph{LDA}
(\emph{DLDA}) classifier, a modification of the \emph{LDA} classifier where the
class covariance matrices are assumed to be diagonal.

Alternatively, several researchers, including \cite*{Zhang:2010va},
\cite{Ye:2009gd}, \cite{Ji:2008wp}, \cite*{Guo:2007te},
\cite*{Srivastava:2007ud}, \cite{Ye:2006vx}, and \cite{Ye:2005uu}, have proposed
regularization methods to improve the estimation of the population covariance
matrices before applying the \emph{LDA} and \emph{QDA}
classifiers. Covariance-matrix regularization methods often are based on the
well-known ridge-regression approach of \cite{Hoerl:1970cd} and adjust the
eigenvalues of the sample covariance matrices to ensure positive
definiteness. For excellent overviews of the regularization problem in
discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we propose a generalization of the popular \emph{RDA} classifier of
\cite{Friedman:1989tm}, who has proposed pooling of the covariance-matrix
estimators from the \emph{LDA} and \emph{QDA} classifiers and has additionally
incorporated a covariance-matrix shrinkage term to ensure that the \emph{RDA}
classifier is well-posed. \cite{Friedman:1989tm} has shown that the \emph{RDA}
classifier can attain improved classification accuracy, compared to the
\emph{LDA} and \emph{QDA} classifiers when $p < N$. However, in its original
form, the \emph{RDA} classifier is impractical when $p \gg N$ because the
\emph{RDA} decision rule requires the calculation of the inverses and
determinants of the $p \times p$ covariance-matrix estimators for each
class. Furthermore, the \emph{RDA} classifier has two tuning parameters that are
selected via cross-validation, which adds a substantial computational burden to
the model-selection process. The \emph{RDA} classifier is ill-advised on modern
data sets because the model selection requires $O(Kp^3)$ calculations for each
tuning-parameter pair.

In this paper we modify the \emph{RDA} classifier by first simplifying the
pooling matrix employed in the \emph{RDA} classifier. We construct a convex
combination of the covariance-matrix estimators employed in the \emph{LDA} and
\emph{QDA} classifiers and demonstrate that this simplification improves the
interpretation of the pooling operation of the \emph{RDA} classifier.
\cite{Clemmensen:2011kr} have argued that the assumption of equal population
covariance matrices within the \emph{LDA} classifier is often too stringent in
practice. The \emph{HDRDA} classifier allows us to relax the linearity
assumption by including unequal covariance matrices in the model. The
\emph{HDRDA} classifier is then a compromise between the \emph{LDA} and
\emph{QDA} classifiers, so that the \emph{HDRDA} classifier includes both the
\emph{LDA} and \emph{QDA} classifiers as special cases. We then establish the
\emph{HDRDA} classifier by applying covariance-matrix regularization with a
parameterization that includes a large family of shrinkage methods, such as the
methods from \cite{Srivastava:2007ww}, \cite{Rao:1971ul}, and a wide variety of
other shrinkage methods studied by \cite{Ramey:2013ji} and \cite{Xu:2009fl}.

We show that the \emph{HDRDA} classifier is invariant to any adjustments of the
eigenvalues corresponding to the null space of the pooled sample covariance
matrix. Applying a reasoning similar to \cite{Ye:2006tq}, we decompose the
\emph{RDA} classifier and show that the sum related to the null space of the
pooled sample covariance matrix can be discarded. Hence, no gain in
classification performance can be obtained by adjusting these zero
eigenvalues. This invariance property addresses the computational shortcomings
of the \emph{RDA} classifier and provides a substantial increase in
computational efficiency and model selection. In fact, we show that the inverse
and determinants can be reduced to calculations of $n_k \times n_k$ matrices,
where $n_k$ is the sample size of the $k$th class, $k = 1, \ldots, K$. This
reduction in calculation is clearly advantageous because $n_k$ is often quite
small, say, 15--20, in microarray data sets.

We also demonstrate that our \emph{HDRDA} classifier is competitive with and
often superior to several recent classifiers designed for small-sample,
high-dimensional data. In particular, we compare our \emph{HDRDA} classifier
with the \emph{DLDA} classifier from \cite{Dudoit:2002ev} and the recently
proposed classifiers from \cite{Guo:2007te}, \cite{Pang:2009ik},
\cite{Tong:2012hw}, and \cite{Witten:2011kc}. We conduct our comparison of the
classification accuracy of the competing classifiers using four small-sample,
high-dimensional microarray data sets.

We have organized the remainder of the paper as follows. In Section 2 we review
the \emph{RDA} classifier from \cite{Friedman:1989tm} and discuss our
generalization in Section 3. In Section 4 we describe four small-sample,
high-dimensional microarray data sets and then demonstrate that our proposed
\emph{HDRDA} classifier is often superior to the competing classifiers in terms
of classification accuracy. We then conclude with a brief discussion in Section
5.

\section{Regularized Discriminant Analysis}
\label{sec:rda}

In discriminant analysis we wish to assign correctly an unlabeled
$p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes
(or populations) by constructing a classifier from $n$ training observations
that can accurately predict the class membership of $\bm x$. Let $\bm x_i =
(x_{i1}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation
$(i = 1, \ldots, N)$ with true, unique membership $y_i \in \{\omega_1, \ldots,
\omega_K\}$, where $\mathbb{R}_{a \times b}$ denotes the matrix space of all $a
\times b$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i,
y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K
p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability
density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of
class membership of the $k$th class.

The \emph{QDA} classifier is the optimal Bayesian decision rule with respect to
a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate
normal distribution with known mean vector $\bm\mu_k \in \mathbb{R}_{p \times
  1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$,
$k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of
real $p \times p$ positive-definite matrices. Because the parameters are
typically unknown, we estimate the unknown parameters $\bm \mu_k$ and
$\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the
MLEs into the \emph{QDA} classifier. Assuming that the prior probabilities of
class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an
unlabeled observation $\bm x$ to class $\omega_k$ with the sample \emph{QDA}
classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and
$\bm \Sigma_k$, respectively. If we assume further that the population
covariance matrices are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$
for all $k$), then the pooled sample covariance matrix $\widehat{\bm\Sigma}$ is
substituted for $\widehat{\bm \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{N} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}
is the MLE for $\bm \Sigma$. In this case \eqref{eq:qda} reduces to Fisher's
\emph{LDA} classifier, and the log-determinant is omitted because it is constant
across all classes.

We also require the following additional notation throughout the document. Let
$\bm A \oplus \bm B$ denote the direct sum of $\bm A \in \mathbb{R}_{r \times
  r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter
  1]{Lutkepohl:1996uz}. Let $\bm A^+$ and $\mathcal{N}(\bm A) = \{\bm z \in
\mathbb{R}_{p \times 1} : \bm A \bm z = \bm 0\}$ denote the Moore-Penrose
pseudoinverse and null space of $\bm A \in \mathbb{R}_{m \times p}$,
respectively. Let $V^{\perp} = \{\bm y \in \mathbb{R}_{p \times 1} : \bm y'\bm v
= 0\ \forall \bm v \in V \}$ denote the orthogonal complement of a vector space
$V \subset \mathbb{R}_{p \times 1}$. Let $\bm I_m$ be the $m \times m$ identity
matrix, and let $\bm 0_{m \times p}$ be the $m \times p$ matrix of zeros, such
that $\bm 0_m$ is understood to denote $\bm 0_{m \times m}$. Lastly, for $c \in
\mathbb{R}$, let $c^+ = 1/c$ if $c \ne 0$ and $0$ otherwise.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors
highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{\bm
  \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest
eigenvalues are underestimated \citep{Seber:2004uh}, Moreover, if $p > n_k$,
then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p -
n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are 0. Furthermore, although more
feature information is available to discriminate among the $K$ classes, if $p >
n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does
not exist.

Several regularization methods, such as the methods considered by
\citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been
proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$
so that \eqref{eq:qda} is calculable and has reduced variability in the standard
bias-variance tradeoff.  A common form of the covariance-matrix regularization
applies a shrinkage factor $\gamma_k > 0$, so that
\begin{align}
	\widehat{\bm \Sigma}_k(\gamma_k) =  \widehat{\bm \Sigma}_k + \gamma_k \bm I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation
\eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance
matrix $\widehat{\bm\Sigma}_k$ towards $\bm I_p$, thereby increasing the
eigenvalues of $\widehat{\bm\Sigma}_k$ by $\gamma_k$. Specifically, the zero
eigenvalues are replaced with $\gamma_k$, so that \eqref{eq:ridge-estimator} is
positive definite. For additional covariance-matrix regularization methods, see
\cite{Ramey:2013ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\subsection{Friedman's Regularized Discriminant Analysis Classifier}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a
weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for
class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$
to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda N$, $\bm
S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm
S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret
\eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that
borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate
$\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to
$\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in
\eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces
to \eqref{eq:pooled-cov} used in the \emph{LDA} classifier.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse
of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased
\emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter
that controls the shrinkage of \eqref{eq:sig-rda} towards $\bm I_p$, which is
weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus,
the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from
$\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage}
parameter $\gamma$ determines the amount of shrinkage applied.

\cite{Friedman:1989tm} substituted \eqref{eq:sig-rda} into \eqref{eq:qda} to
obtain the \emph{RDA} classifier
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}

\section{The HDRDA Classifier}

Here, we define our adaptation of the \emph{RDA} classifier by first simplifying
the formulation of $\widehat{\bm \Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda}
and demonstrating its clear interpretation as a linear combination of the
crossproducts of the training observations centered by their respective class
sample means. We define
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where
$\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we avoid the convex
combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By rewriting
\eqref{eq:sig-lambda-alternative} in terms of the observations $\bm x_i$, $i =
1, \ldots, N$, each centered by its class sample mean, we attain a clear
interpretation of $\widehat{\bm\Sigma}_k(\lambda)$. We have that
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
%	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'} \nonumber \\
%	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of
each of the $N$ observations to estimate $\bm \Sigma_k$. That is, for $0 <
\lambda \le 1$, we estimate $\bm \Sigma_k$ with $N$ rather than $n_k$
observations. Below, we will show that the pooling operation is advantageous in
increasing the rank of each $\widehat{\bm\Sigma}_k(\lambda)$ from
rank$(\widehat{\bm\Sigma}_k)$ to rank$(\widehat{\bm\Sigma})$ for $0 < \lambda
\le 1$. Notice that if $\lambda = 0$, then the observations from the remaining
$K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$,
corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the
weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to
$\widehat{\bm\Sigma}$. For brevity, when $\lambda = 1$, we define $\bm X =
[\sqrt{c_{1k}(1)} \bm x_1', \ldots, \sqrt{c_{Nk}(1)} \bm x_N']'$ such that
$\widehat{\bm \Sigma} = \bm X' \bm X$. Similarly, for $\lambda = 0$, we define
$\bm X_k = [\sqrt{c_{1k}(0)} \bm x_1', \ldots, \sqrt{c_{Nk}(0)} \bm x_N']'$ such
that $\widehat{\bm \Sigma}_k = \bm X_k' \bm X_k$, $k = 1, \ldots, K$. In Figure
\ref{fig:hdrda-contours} we plot the contours of five multivariate normal
populations with unequal covariance matrices and $\lambda = 0$. As $\lambda$ is
increased towards 1, the contours become more similar, ultimately resulting in
equal covariance matrices and contours for the parsimonious case of $\lambda =
1$.

\[ \left[\text{Figure \ref{fig:hdrda-contours} goes about here }\right]\]

As we have discussed, several eigenvalue adjustment methods have been proposed
to increase eigenvalues (approximately) equal to 0. Here, we define the
eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\bm \Sigma}_k = \alpha_k \widehat{\bm \Sigma}_k(\lambda) + \gamma \bm I_p,\label{eq:hdrda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\gamma \ge 0$ is an eigenvalue-shrinkage
constant. The choice of $\alpha_k$ allows for a flexible formulation of
covariance-matrix estimators. For instance, if $\alpha_k = 1$, $k = 1, \ldots,
K$, then \eqref{eq:hdrda-cov} resembles \eqref{eq:ridge-estimator}. Similarly,
if $\alpha_k = 1 - \gamma$, then \eqref{eq:hdrda-cov} has a form similar to
\eqref{eq:sig-rda}. Now, substituting \eqref{eq:hdrda-cov} into \eqref{eq:qda},
we define the \emph{HDRDA} classifier as
\begin{align}
	D_{HDRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\tilde{\bm\Sigma}_k^{+}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k|. \label{eq:hdrda}
\end{align}
For $\gamma > 0$, $\tilde{\bm\Sigma}_k$ is nonsingular such that
$\tilde{\bm\Sigma}_k^{-1}$ can be substituted for $\tilde{\bm\Sigma}_k^{+}$ in
\eqref{eq:hdrda}. If $\gamma = 0$, we explicitly set $|\tilde{\bm\Sigma}_k|$
equal to the product of the positive eigenvalues of $\tilde{\bm\Sigma}_k$.

Next, we demonstrate that the \emph{HDRDA} classifier lends itself to an
efficient calculation of \eqref{eq:hdrda} and a rapid model selection of the
pooling parameter $\lambda$. We decompose \eqref{eq:hdrda} into a sum of two
components, where the first summand consists of matrix operations applied to
low-dimensional matrices and the second summand corresponds to the null space of
$\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix
operations performed on the null space of $\widehat{\bm \Sigma}$ yield constant
quadratic forms across all classes and can be omitted under mild regularity
conditions. For $p \gg N$, the constant component involves determinants and
inverses of high-dimensional matrices, and by ignoring these calculations, a
substantial reduction in computational costs is achieved relative to Friedman's
\emph{RDA} classifier in \eqref{eq:rda}. Furthermore, a byproduct is that
adjustments to the associated eigenvalues have no effect on
\eqref{eq:hdrda}. Lastly, we utilize the singular value decomposition (SVD) to
calculate efficiently the eigenvalue decomposition of $\widehat{\bm \Sigma}$,
further reducing the computational costs of our proposed \emph{RDA} classifier.

First, we require the following relationship regarding the null spaces of
$\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm
  \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the MLEs of $\bm
\Sigma_k$ and $\bm \Sigma$ as discussed in Section \ref{sec:rda}, and let
$\widehat{\bm \Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, for $k = 1, \ldots, K$,
$\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm
  \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k(\lambda))$ for some $k = 1,
\ldots, K$. Hence, $0 = \bm z' \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 -
\lambda) \bm z' \widehat{\bm \Sigma}_k \bm z + \lambda \bm z' \widehat{\bm
  \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in
\mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{\bm
  \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular,
we have that $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset
\mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in
\mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z'
\widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z' \widehat{\bm
  \Sigma}_k \bm z$, which implies that $\bm z \in \mathcal{N}(\widehat{\bm
  \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times p}^{\ge}$
for $k = 1, \ldots, K$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset
\mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative
expression for $\tilde{\bm \Sigma}_k$ in terms of the matrix of eigenvectors of
$\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the
eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p
  \times p}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$
with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, $\bm D_q \in \mathbb{R}_{q \times
  q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of
$\widehat{\bm \Sigma}$, the columns of $\bm U \in \mathbb{R}_{p \times p}$ are
the corresponding orthonormal eigenvectors of $\widehat{\bm \Sigma}$, and
rank$(\widehat{\bm \Sigma}) = q$. Then, we partition $\bm U = (\bm U_1, \bm
U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in
\mathbb{R}_{p \times (p - q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of
$\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) =
q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \gamma \bm I_{p-q})\bm U',\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= \alpha_k \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm I_{q}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $\bm U_2$ span the null space
of $\widehat{\bm \Sigma}$, which implies that $\widehat{\bm \Sigma}_k \bm U_2 =
\bm 0_{p \times (p - q)}$. Hence, for $k = 1, \ldots, K$, $\bm U' \widehat{\bm
  \Sigma}_k \bm U = \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm
0_{p-q}$. Thus, $\bm U' \tilde{\bm \Sigma}_k \bm U = \alpha_k \{(1 - \lambda)
\bm U' \widehat{\bm \Sigma}_k \bm U + \lambda \bm D\} + \gamma \bm I_p$, and
\eqref{eq:rda-matrix} follows because $\bm U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we
have the following corollary.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in
\eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$,
  rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$ for all $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we
increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ to $q$ if $\lambda \ne
0$. Next, we provide an essential result that enables us to prove that, under
mild regularity conditions, \eqref{eq:hdrda} is invariant to adjustments to the
eigenvalues of $\tilde{\bm \Sigma}_k$ corresponding to the null space of
$\widehat{\bm \Sigma}$ for any unlabeled observation $\bm x \in \mathbb{R}_{p
  \times 1}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times
  1}$, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$, $1 \le k,
k' \le K$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$.
Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that
$\bm U_2' \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma
  1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{\bm
  \Sigma})$, $i = 1, \ldots, N$, $\bm U_2' \bm x_i = \bm 0_{(p-q) \times 1}$.
Hence, $\bm 0_{(p-q) \times 1} = \sum_{i=1}^N \beta_i \bm U_2'\bm x_i = \bm U_2'(\xbar_k -
\xbar_{k'})$, where $\beta_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i =
k') n_k \}$. Therefore, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$.
\end{proof}

We now present our main result, where we decompose the \emph{HDRDA} decision
rule and show that the component requiring the largest computational costs does
not contribute to the classification of an unlabeled observation performed using
Lemma \ref{lemma:RDA-constant-term}. Hence, we reduce \eqref{eq:hdrda} to an
equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ and $\bm W_k$ be defined as in \eqref{eq:rda-matrix}
and \eqref{eq:Wk}, respectively, and let $\bm U_1$ be defined as above. Then,
the \emph{HDRDA} decision rule in \eqref{eq:hdrda} is equivalent to
	\begin{align}
		D_{HDRDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k) + \ln | \bm W_k |. \label{eq:hdrda-decomposed}
	\end{align}
	
\end{thm}
\begin{proof}
From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{+} = \bm U(\bm
W_k^{-1} \oplus \gamma^{+} \bm I_{p-q} )\bm U'$ and $|\tilde{\bm \Sigma}_k| =
\gamma^{p-q}| \bm W_k |$. Therefore, for all $\bm x \in \mathbb{R}_{p \times 1}$,
we have that
	\begin{align*}
	(\bm x - \xbar_k)' \tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k| &= (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)\\
	&+ \gamma^{+} (\bm x - \xbar_k)' \bm U_2 \bm U_2' (\bm x - \xbar_k) + \ln | \bm W_k | + (p - q) \ln \gamma.
	\end{align*}
The proof follows from Lemma \ref{lemma:RDA-constant-term} because $\bm U_2'
(\bm x - \xbar_k)$ is constant for all $k = 1, \ldots, K$.
\end{proof}

\subsection{Model Selection}

Thus far, we have shown that the \emph{HDRDA} classifier in \eqref{eq:hdrda} is
invariant to the term $\bm U_2$, thus yielding an equivalent classifier in
\eqref{eq:hdrda-decomposed} with a substantial reduction in the computational
complexity. While the classifier is much improved in terms of computational
performance compared to the \emph{RDA} classifier originally proposed by
\cite{Friedman:1989tm}, we provide two additional results that further improve
the computational performance of our \emph{HDRDA} classifier. First, we
demonstrate that the inverse and determinants of $\bm W_k$ can be written so
that they are performed on $n_k \times n_k$ matrices rather than the larger $q
\times q$ matrices, resulting in further improvements to the computational
efficiency.

\begin{proposition}\label{proposition:rda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - \alpha_k(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},\label{eq:W_k_inv}
	\end{align}
where $\bm Q_k = \bm I_{n_k} + \alpha_k(1- \lambda) \bm X_k \bm U_1 \bm
\Gamma_k^{-1} \bm U_1' \bm X_k'$ and $\bm \Gamma_k = \alpha_k \lambda \bm D_q +
\gamma \bm I_q$.
\end{proposition}
\begin{proof}
To calculate $|\bm W_k|$, we apply Theorem 18.1.1 from \cite{Harville:2008wja},
which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} +
\bm C \bm A^{-1} \bm B|$. Thus, setting $\bm A = \bm \Gamma_k$, $\bm B =
\alpha_k (1 - \lambda) \bm U_1' \bm X_k'$, $\bm T = \bm I_{n_k}$, and $\bm C =
\bm X_k \bm U_1$, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also,
\eqref{eq:W_k_inv} follows from the well-known Sherman-Woodbury formula
\citep[Theorem 18.2.8]{Harville:2008wja}.
\end{proof}

Thus far, we have constructed each $\tilde{\bm \Sigma}_k$ in terms of the first
$q$ eigenvectors of $\widehat{\bm \Sigma}$. By utilizing a technique similar to
the so-called kernel trick, we can obtain $\bm U_1$ by computing the
eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$
\citep{Hastie:2008dt}. Recall that the pooled sample covariance matrix
$\widehat{\bm\Sigma} = \bm X' \bm X$. Then, because $\bm X = \bm M \bm D^{1/2}
\bm U'$ by the SVD, we have the spectral decomposition $\widehat{\bm\Sigma} =
\bm U \bm D \bm U'$, where $\bm M \in \mathbb{R}_{N \times N}$ is orthogonal,
$\bm D^{1/2} \in \mathbb{R}_{N \times N}^{\ge}$ is a diagonal matrix consisting
of the singular values of $\bm X$, and $\bm U \in \mathbb{R}_{p \times N}$ is
orthogonal. Now, $\bm X \bm X' = \bm M \bm D \bm M'$ is the eigenvalue
decomposition of $\bm X \bm X'$, where $\bm D$ is the matrix of eigenvalues of
$\bm X \bm X'$ and the columns of $\bm M$ are the corresponding
eigenvectors. Hence, we obtain $\bm M$ and $\bm D$ from an eigendecomposition of
$\bm X \bm X' \in \mathbb{R}_{N \times N}$. Next, we compute $\bm U = \bm X' \bm
M \bm D^{+/2}$, where $\bm D^{+/2} = \bm D_q^{-1/2} \oplus \bm 0_{N-q}$. We then
determine $q$ by computing the number of numerically nonzero eigenvalues present
in $\bm D$, where $q$ is the number of eigenvalues that exceeds some tolerance
value, say, $\epsilon = 1 \times 10^{-6}$. After calculating $q$, we extract
$\bm U_1$ by retaining only the first $q$ columns of $\bm U$.

\section{High-Dimensional Microarray Data Sets}

In this section, we describe four high-dimensional microarray data sets and
compare our proposed \emph{HDRDA} classifier with five recently proposed
classifiers designed for small-sample, high-dimensional data: the \emph{DLDA}
classifier from \cite{Dudoit:2002ev}, the shrunken centroids \emph{RDA}
classifier from \cite{Guo:2007te}, the shrinkage-based \emph{DLDA} classifier
from \cite{Pang:2009ik}, the shrinkage-mean-based DLDA classifier from
\cite{Tong:2012hw}, and the penalized \emph{LDA} classifier from
\cite{Witten:2011kc}. The classifiers from \cite{Tong:2012hw} and
\cite{Pang:2009ik} are both modifications of the \emph{DLDA} classifier from
\cite{Dudoit:2002ev}, where the former employs an improved mean estimator and
the latter utilizes an improved variance estimator. Below, we refer to each
classifier by the first author's surname.

We evaluated the classification performance of each classifier by randomly
partitioning the data such that $2/3$ of the observations were allocated as
training data and the remaining $1/3$ of the observations were allocated as a
test data set. For each random partition, we applied the feature-selection
method from \cite{Dudoit:2002ev} to the training data to choose the $d$ features
having the largest ratio of their between-group to within-group sums of squares.
We then filtered the test data set with the corresponding $d$ features. After
training each classifier from the filtered training data, we classified the
filtered test data sets and computed the proportion of mislabeled test
observations to estimate the classification error rate for each
classifier. Repeating this process 1000 times, we computed the average of the
error-rate estimates for each classifier. We remark that our simulation design
avoids the selection bias discussed in \cite{Ambroise:2002fa}. We did not
include the \emph{RDA} classifier from \cite{Friedman:1989tm} in our studies to
avoid its excessive computations. In Table \ref{tab:data-summary}, we summarize
the attributes of each microarray data set.

We used version 3.0.0 of the open-source statistical software {\tt R} for the
classification study. For each classifier's {\tt R} implementation, we used the
default settings with the exception that we explicitly set the prior
probabilities as equal. In our {\tt R} implementation of the \emph{HDRDA}
classifier in the {\tt sparsediscrim} package, we set $\alpha_k = 1$, $k = 1,
\ldots, K$, so that the covariance-matrix estimator \eqref{eq:hdrda-cov}
employed in the \emph{HDRDA} classifier has a form resembling
\eqref{eq:ridge-estimator}. We estimated $\lambda$ from a grid of 21 equidistant
candidate values between 0 and 1, inclusively. Similarly, we estimated $\gamma$
from a grid consisting of the values $10^{-1}, \ldots, 10^4$, and $10^5$. We
selected optimal estimates of $\lambda$ and $\gamma$ using 10-fold
cross-validation \citep{Hastie:2008dt}.

\[ \left[\text{Table 1 goes about here }\right]\]

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC)
through hybridization to microarrays from 127 individuals resulting in 22,283
sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD),
and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to
improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using
the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide
microarray from 105 samples of 10 types of soft tissue tumors. This included 16
samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma
(MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS),
15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of
myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of
malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21
samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl}
determined from their data that these 10 types fell into 4 broader groups: (1)
SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and
pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following
\cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at
least 15 observations.

\subsection{\cite{Shipp:2002ka} Data Set}

Approximately 30-40\% of adult non-Hodgkin lymphomas are diffuse large B-cell
lymphomas (DLBCLs) \cite{Shipp:2002ka}.  However, only a small proportion of
DLBCL patients are cured with modern chemotherapeutic regimens.  Several models
have been proposed, such as the International Prognostic Index (IPI), to
determine a patient's curability.  These models rely on clinical covariates,
such as age, to determine if the patient can be cured and are often
ineffective. \cite{Shipp:2002ka} have argued that more effective means are
desired to determine a patient's curability. The authors measured 6,817 gene
expression levels from 58 DLBCL patient samples with customized cDNA
(lymphochip) microarrays to investigate the curability of patients treated with
cyclophosphamide, adriamycin, vincristine, and prednisone (CHOP)-based
chemotherapy.  Among the 58 DLBCL patient samples, 32 are from cured patients
while 26 are from patients with fatal or refractory disease.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from
surgery patients between 1995 and 1997. The authors used oligonucleotide
microarrays containing probes for approximately 12,600 genes and expressed
sequence tags. They have reported that 102 of the radical prostatectomy
specimens are of high quality: 52 prostate tumor samples and 50 non-tumor
prostate samples.
 
\subsection{Classification Results}

For the microarray data sets, we computed error-rate estimates for the competing
classifiers after reducing the dimension of the data sets with feature selection
to $d = 100, \ldots, 1000$, similar to the values of $d$ considered by
\cite{Xu:2009fl}. To compare misclassification rates between different values of
$d$, we applied each value of $d$ considered to the same random allocation of
training and test data. Hence, for a given random partition, the set of features
selected for a small value of $d$ is a proper subset of the features selected
for a larger value of $d$. We report the average of the error-rate estimates for
each value of $d$ in Figure \ref{fig:error-rates}. Approximate standard errors
were no greater than \Sexpr{max(error_rates[["SE_Error"]])}.

The \emph{HDRDA} classifier was superior in classification performance for the
majority of the simulations. For the Burczysnki, Shipp, and Singh data sets, the
\emph{HDRDA} classifier outperformed its competitors for all values of $d$
examined. For the Nakayama data set, the \emph{HDRDA} classifier was
outperformed by the Dudoit, Pang, and Tong classifiers for small values of $d$
but attained classification rates superior to its competitors for larger values
of $d$. Furthermore, Figure \ref{fig:error-rates} emphasizes two strengths of
the \emph{HDRDA} classifier: reliability of the classifier for small $d$ and
stability of the classifier for large $d$. In contrast to the other classifiers,
the \emph{HDRDA} classifier exhibited less variability in its misclassification
estimates for small values of $d$. Also, the \emph{HDRDA} classifier yielded
superior classification accuracy as $d$ increased and was hardly effected by the
inclusion of more variables in the data sets. We attribute the apparent
stability of the error-rate estimates of the \emph{HDRDA} classifier for large
values of $d$ to its novel dimension-reduction technique.

The error-rate estimates for the Dudoit, Pang, Tong, and Witten classifiers were
comparable for all values of $d$ examined. Furthermore, the error-rate estimates
of the Dudoit, Pang, and Tong classifiers were approximately equal throughout
our study, and the Witten classifier yielded approximately equal error-rate
estimates for the Burczysnki, Shipp, and Singh data sets. Furthermore, the
error-rate estimates for the Dudoit and Tong classifiers were nearly identical,
which suggests that the improved mean estimator employed by \cite{Tong:2012hw}
provided little improvement to the classification performance of the \emph{DLDA}
classifier.

The classification performance of the the Dudoit, Pang, Tong, and Witten
classifiers for the Shipp and Singh data sets revealed the inability of these
methods to handle larger values of $d$. After closer examination of the
classification studies performed in the authors' original papers, we noticed
that classification accuracy estimates were reported only for small values of
$d$, thereby ignoring the inclusion of a larger number of variables. Hence, we
advise that practioners be aware of the necessity to filter a large number of
features before employing these methods, lest their classification performance
can degrade.

The popular Guo classifier is most competitive over large values of $d$ for the
Shipp and Singh data sets. However, for the Burczynski and Nakayama data sets,
the implementation of the Guo method in the {\tt rda} package often classified
every test observation into a single class, thus yielding larger error rates
than its competitors. Furthermore, the Guo classifier exhibited unreliable
classification performance for smaller values of $d$. The Guo classifier
required a larger subset of the features to perform competitively in our study.

\[ \left[\text{Figure \ref{fig:error-rates} goes about here }\right]\]

Next, we examine the optimal estimates of the tuning parameters for the
\emph{HDRDA} classifier. For brevity, we include only a graphical summary of the
optimal estimates of $\lambda$ for the Shipp data set in Figure
\ref{fig:optimal-shipp}. Similar figures for the other three data sets can be
found in the Supplementary Material. In Figure \ref{fig:optimal-shipp}, the
distribution of the optimal values of $\lambda$ selected through
cross-validation has a notable bathtub shape, resembling a Beta distribution
with both shape parameters less than unity. Specifically, the optimal values of
$\lambda$ are typically near 0 or near 1, corresponding to the \emph{QDA} and
\emph{LDA} classifiers, respectively. We noticed a similar distribution of
optimal values of $\lambda$ for the other three data sets.

For the Shipp data set with smaller values of $d$, the range of optimal values
of $\lambda$ was typically no larger than 0.05, indicating there were a large
number of training data sets for which we can safely assume the covariance
matrices are unequal or nearly unequal. As $d$ increased, the majority of the
optimal values of $\lambda$ were selected as unity, thereby an increased number
of variables resulted in the \emph{HDRDA} classifier preferring the more
parsimonious assumption of equal covariance matrices. In this instance we can
see the flexibility of the \emph{HDRDA} classifier to employ the more realistic
assumption that the class covariance matrices are unequal, but if optimal, we
can employ the more parsimonious \emph{LDA} classifier as a special case.

\[ \left[\text{Figure \ref{fig:optimal-shipp} goes about here }\right]\]


\section{Discussion}

The analysis of high-dimensional microarray data sets is important to modern
biomedical studies and presents challenges with supervised classification
because the number of gene expressions greatly outnumbers the number of
collected responses. Consequently, several variants of the well-known \emph{LDA}
classifier have been proposed for improved classification performance. However,
the \emph{LDA} classifier includes the strong assumption that the covariance
matrices of each class are equal. Ideally, we could model the class covariance
matrices as unrestricted and unequal in the \emph{QDA} classifier, but it is
well known to have a large number of parameters that must be estimated and
require a large number of samples, which are often unavailable in the
high-dimensional setting. With this mind, \cite{Friedman:1989tm} proposed the
\emph{RDA} classifier as a compromise between the \emph{LDA} and \emph{QDA}
classifiers, controlling the amount of pooling of the individual class
covariance-matrix estimators using the tuning parameter $\lambda$. However,
despite its success for data sets where $p > N$, the \emph{RDA} classifier
requires excessive calculations and a time-consuming model selection.

We have adapted the \emph{RDA} classifier for usage with modern small-sample,
high-dimensional data sets. Our model formulation facilitates a rapid
classification of high-dimensional data by employing a novel dimension reduction
technique. Moreover, our 10-fold cross-validation procedure typically requires a
small runtime for data sets having large $p$. When preliminary variable
selection is applied to the data as we performed in our classification study,
the time to perform model selection is nearly instantaneous. Additionally, we
have provided an implementation of the \emph{HDRDA} classifier in the {\tt
  sparsediscrim} R package that is easy to use and available on CRAN.

We have parameterized the covariance-matrix regularization employed in the
\emph{HDRDA} classifier to include a large family of shrinkage methods, such as
the methods from \cite{Srivastava:2007ww}, \cite{Rao:1971ul}, and a wide variety
of other shrinkage methods studied by \cite{Ramey:2013ji} and
\cite{Xu:2009fl}. Our formulation of the \emph{HDRDA} classifier in terms of the
nonzero eigenvalues of the pooled sample covariance matrix indicates that
eigenvalue-adjustment methods applied to the \emph{LDA} classifier explains the
similiarities in classification accuracy of several regularization methods
applied to the \emph{LDA} classifier reported by \cite{Ramey:2013ji} in a
high-dimensional classification study. We have provided an invariance property
that extends results of \cite{Ji:2008wp} and \cite{Ye:2006jm}, who have provided
a similar invariance property for the \emph{LDA} decision rule. Additionally,
our model formulation improves the interpretation of the \emph{RDA} classifier
by explicitly defining the covariance-matrix estimator so that each of the $N$
observations contributes to the estimation of the individual class covariance
matrices.

In our simulation study, we compared the \emph{HDRDA} classifier to several
recently proposed classifiers, including the Dudoit, Pang, Tong, and Guo
classifiers, each of which assumes independence of the features conditional on
class membership. Despite the rapid computational performance of diagonal
classifiers and the reduction in the number of parameters to estimate,
\cite{Mai:2012bf} and \cite{Fan:2012iq} have noted that diagonal classifiers
can often yield inferior classification performance to other classification
methods. We have shown that our proposed \emph{HDRDA} classifier often yields
superior classification accuracy to the diagonal classifiers considered,
confirming the assertions of \cite{Mai:2012bf} and
\cite{Fan:2012iq}. Furthermore, the \emph{HDRDA} classifier is able to compete
with the diagonal classifiers in terms of speed by incorporating a new
dimension-reduction technique.


\bibliographystyle{plainnat}
\bibliography{rda}

\clearpage

<<data_summary, results="asis">>=
# Table containing a summary of the data sets
data_summary <- subset(describe_data(), author %in% unique(results$data_set))
data_summary$K[data_summary$author == "nakayama"] <- 5
rownames(data_summary) <- NULL
colnames(data_summary)[1:3] <- c("Author", "Year", "N")
data_summary$Author <- Hmisc:::capitalize(as.character(data_summary$Author))
data_summary$Author <- with(data_summary, paste0(Author, " et al. (", Year, ")"))
data_summary <- subset(data_summary, select = -Year)
print(xtable(data_summary, caption = "Summary of high-dimensional microarray data sets.",
             label = "tab:data-summary", digits = 0), include.rownames = FALSE)

@ 

\begin{figure}
<<hdrda_contours, fig.width=8, fig.height=8, out.width='.49\\linewidth', fig.show='hold'>>=
rda_contours(lambda = 0)
rda_contours(lambda = 1/4)
rda_contours(lambda = 3/4)
rda_contours(lambda = 1)
@ 
\caption{Contours of five multivariate normal populations as a function of the
  pooling parameter $\lambda$.}
\label{fig:hdrda-contours}
\end{figure}

\begin{figure}
<<error_rates>>=
error_rates$data_set <- Hmisc:::capitalize(error_rates$data_set)
p <- ggplot(error_rates, aes(x = d, y = Avg_Error, group = Classifier))
p <- p + geom_line(aes(color = Classifier, linetype = Classifier), size = 1.5)
p <- p + facet_wrap(~ data_set) + ylim(c(0, max(error_rates$Avg_Error))) + theme_bw()
p <- p + xlab("Number of Variables Selected") + ylab("Average Error Rate")
p  + theme(strip.text.x = element_text(size = 12))
@ 
\caption{Average error-rate estimates of the competing classifiers as a function
  of $d$. Approximate standard errors are no greater than \Sexpr{max(error_rates[["SE_Error"]])}.}
\label{fig:error-rates}
\end{figure}

\begin{figure}
<<optimal_estimates>>=
hdrda_optimal$lambda <- factor(hdrda_optimal$lambda)
hdrda_optimal$gamma <- factor(hdrda_optimal$gamma)
p <- ggplot(subset(hdrda_optimal, data_set == "shipp" & d %in% c(100, 500, 1000)), aes(lambda))
p <- p + geom_bar(aes(fill = gamma, weight = value))
p <- p + facet_grid(d ~ ., labeller = label_bquote(d == .(x))) + theme_bw()
p + theme(strip.text.y = element_text(size = 16))
@ 
\caption{Counts of the optimal tuning-parameter estimates for the \emph{HDRDA}
  classifier for the Shipp data set for $d = 100$, $500$, and $1000$.}
\label{fig:optimal-shipp}
\end{figure}



\end{document} 
