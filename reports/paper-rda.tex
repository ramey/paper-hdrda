\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{A Generalization of Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
\cite{Friedman:1989tm} has proposed the popular regularized discriminant analysis (\emph{RDA}) classifier for the small-sample, high-dimensional setting. This \emph{RDA} classifier employs a regularized covariance-matrix estimator, which incorporates a ridge-like shrinkage factor by adding a positive constant to the zero-eigenvalue estimates to ensure positive definiteness. Here, we propose a novel generalization of Friedman's \emph{RDA} classifier that is more computationally efficient than its original counterpart because the inverses and determinants computed in the decision rule can be reduced from the original feature space to a linear subspace with dimensions corresponding to the individual class sample sizes. Additionally, our generalization of the \emph{RDA} (\emph{GRDA}) classifier enables efficient estimation of optimal tuning parameters from a large set of candidate values, simplifies the construction of the regularized covariance-matrix estimator to improve its interpretation, and allows for a more flexible classification model. Moreover, we demonstrate with several high-dimensional microarray data sets that our proposed classifier is competitive and often superior to several recently proposed sparse and regularized classifiers. Finally, we provide an implementation of the \emph{GRDA} classifier in the {\tt regdiscrim} R package, available on CRAN.
\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of thousands of genes in a single experiment to identify a subset of genes and their expression patterns that either characterize a particular cell state or facilitate an accurate prediction of future cell state.  We focus on the classification of tissues and other biological samples into one of $K$ disjoint, predefined cancer types, where our goal is to ascertain the class membership of a new tissue sample based on gene-expression microarray data. However, microarray data typically consist of a large number of gene expressions $p$ relative to the sample size $N$. Moreover, standard benchmark microarray data sets usually consist of between $p = 10,000$ and $55,000$ probes and fewer than $N = 100$ observations, in which case standard supervised-learning methods exhibit poor classification performance, due to the \emph{curse of dimensionality} \citep{Bellman:1961tn}. Furthermore, well-known classifiers, including linear discriminant analysis \emph{(LDA)} and quadratic discriminant analysis \emph{(QDA)}, are incalculable when $p > N$ because the sample covariance matrices employed in both classifiers are singular. As a result, many researchers, including \cite*{Merchante:2012vk}, \cite{Witten:2011kc}, \cite*{Pang:2009ik}, \cite*{Huang:2010ju}, and \cite*{Clemmensen:2011kr}, have introduced restrictions on the population covariance matrices in the \emph{LDA} and \emph{QDA} models to reduce number of covariance parameters that must be estimated to ensure that the resulting classifiers are calculable.  For instance, \cite{Dudoit:2002ev} have proposed the diagonal \emph{LDA} (\emph{DLDA}) classifier, a modification of the \emph{LDA} classifier where the class covariance matrices are assumed to be diagonal.

Alternatively, several researchers, including \cite*{Zhang:2010va}, \cite{Ye:2009gd}, \cite{Ji:2008wp}, \cite*{Guo:2007te}, \cite{Srivastava:2007ud}, \cite{Ye:2006vx}, \cite*{Thomaz:2006ef}, \cite{Ye:2005uu}, \cite{Mkhadri:1995jp}, and \cite{Peck:1982tt}, have proposed regularization methods to improve the estimation of the population covariance matrices before applying the \emph{LDA} and \emph{QDA} classifiers. Covariance-matrix regularization methods often are based on the well-known ridge-regression approach from \cite{Hoerl:1970cd} and adjust the eigenvalues of the sample covariance matrices to ensure that the estimator is positive definite. Typically, a positive shrinkage value is added to each eigenvalue estimate, replacing the zero eigenvalues with the shrinkage constant and stabilizing the eigenvalues near zero to ensure that the inverse of the the sample covariance matrix is well-posed. This common form of covariance-matrix regularization is equivalent to summing the original covariance-matrix estimator with the identity matrix scaled by the shrinkage constant. For an excellent overview of the regularization problem in discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we consider a generalization of the popular \emph{RDA} classifier of \cite{Friedman:1989tm}, who has proposed a weighted average of the covariance-matrix estimators from the \emph{LDA} and \emph{QDA} classifiers. Because the resulting biased covariance-matrix estimator can remain singular or nearly singular, \cite{Friedman:1989tm} has additionally incorporated applied covariance-matrix shrinkage, where the shrinkage constant is a function of the average eigenvalue of the original estimator. \cite{Friedman:1989tm} has shown that the \emph{RDA} classifier can attain improved classification accuracy, compared to the \emph{LDA} and \emph{QDA} classifiers when $p > N$. However, in its original form, the \emph{RDA} classifier is impractical when $p \gg N$ because the \emph{RDA} decision rule requires the calculation of the inverses and determinants of the $p \times p$ covariance-matrix estimators for each class. Furthermore, the \emph{RDA} classifier has two tuning parameters that are selected via cross-validation, which adds a substantial computational burden to the model-selection process. Despite the introduction of a computational shortcut by \cite{Friedman:1989tm} for model selection to alleviate some of the computation, the \emph{RDA} classifier is ill-advised on modern data sets because the model selection requires on the order of $Kp^3$ calculations for each tuning-parameter pair for each fold in the cross-validation scheme.

In this paper we address the computational shortcomings of the \emph{RDA} classifier. Applying a technique similar to \cite{Ye:2006tq}, we decompose the \emph{RDA} classifier and show that the sum related to the null space of the pooled sample covariance matrix can be discarded. In turn, this omission provides a substantial increase in the computational efficiency of the \emph{RDA} classifier and its model-selection. In fact, we show rigorously that the inverse and determinants can be reduced to calculations of $n_k \times n_k$ matrices, where $n_k$ is the sample size of the $k$th class, $k = 1, \ldots, K$. This reduction in calculation is clearly advantageous because $n_k$ is often quite small, say, 15--20, in microarray data sets. Thus, we demonstrate that the calculation of the \emph{RDA} decision rule involving inverses and determinants of extremely large covariance matrices is equivalent a decision rule involving inverses and determinants on much smaller matrices.

Also, for our \emph{GRDA} classifier we first simplify the pooling matrix in the \emph{RDA} classifier by constructing instead a convex combination of the covariance-matrix estimators employed in the \emph{LDA} and \emph{QDA} classifiers. This simplification is advantageous in that it improves the interpretation of the pooling operation of the \emph{RDA} classifier. We show that under mild regularity conditions the \emph{GRDA} classifier is invariant to any adjustments of the eigenvalues corresponding to the null space of the pooled sample covariance matrix. Hence, no gain in classification performance can be obtained by adjusting these zero eigenvalues. Although \cite{Ji:2008wp} and \cite{Ye:2006jm} have provided a similar invariance property for the \emph{LDA} decision rule, the \emph{LDA} classifier involves the assumption that the population covariance matrices are equal, which is often too stringent in practice \citep{Clemmensen:2011kr}. The \emph{GRDA} classifier allows us to relax this assumption by including unrestricted covariance matrices in the model. The \emph{GRDA} classifier is then a compromise between the two approaches, favoring the stronger linear assumption if a larger value of the pooling parameter is selected as optimal. Our formulation provides a general framework from which we can elucidate properties of covariance-regularization methods and the relationships among them in the linear case, the quadratic case, and the cases in between.

Many of the recently proposed classifiers have a variable-selection method integrated. While this is advantageous in potentially determining biomarkers that are correlated with the classification label, these methods often yield poor classification accuracy if a large number of variables are retained. Moreover, these methods can require large computational costs, making these methods undesirable in practice. To study these issues in depth, we consider several simulated data sets, where we show that the classification degrades as the number of features is increased. Contrarily, our proposed \emph{GRDA} classifier handles the large number of features well because our included dimension-reduction method retains only the classificatory information in the covariance-matrix estimators. Moreover, while the competing classifiers exhibit large computational costs as the number of features in the original data set is increased, the \emph{GRDA} classifier handles the larger number of features efficiently.

In addition to the improvement in computational performance of classifiers with regularized covariance matrices, we demonstrate that our \emph{GRDA} classifier is competitive with and often superior to several recent classifiers designed for small-sample, high-dimensional data. In particular, we compare our \emph{GRDA} classifier with an alternatively-defined \emph{RDA} classifier from \cite*{Hastie:2008dt}, the regularized \emph{LDA} classifier from \cite{Guo:2007te}, a recently proposed variant of the \emph{DLDA} classifier from \cite*{Tong:2012hw}, and the penalized \emph{LDA} classifier from \cite{Witten:2011kc}. We conduct our comparison of the classification accuracy of the competing classifiers using three small-sample, high-dimensional microarray data sets and examine the classification accuracy of the competing classifiers using \textbf{three} simulation configurations.

We have organized the remainder of the paper as follows. In Section 2 we present our generalization of the \emph{RDA} classifier after reviewing its original counterpart from \cite{Friedman:1989tm}. In Section 3 we show that our proposed classifier includes several well-known regularized classifiers as special cases and briefly discuss some commonalities among them. In Section 4 we describe the \textbf{three} simulation configurations and three small-sample, high-dimensional microarray data sets and then demonstrate that our two proposed classifiers are superior to the competing classifiers in terms of classification accuracy. We then conclude with a brief discussion in Section 5.

\section{Regularized Discriminant Analysis}
\label{sec:rda}

In discriminant analysis we wish to assign correctly an unlabeled $p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes (or populations) by constructing a classifier from $n$ training observations that can accurately predict the class membership of $\bm x$. Let $\bm x_i = (x_{i1}, x_{i2}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots, n)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{a \times b}$ denotes the matrix space of all $a \times b$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of class membership of the $k$th class.

The \emph{QDA} classifier is the optimal Bayesian decision rule with respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate normal distribution with known mean vector  $\bm\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite matrices. Because the parameters are typically unknown, we estimate the unknown parameters $\bm \mu_k$ and $\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the MLEs into the \emph{QDA} classifier. Assuming that the prior probabilities of class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an unlabeled observation $\bm x$ to class $\omega_k$ with the sample \emph{QDA} classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further that the population covariance matrices are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$ for all $k$), then the pooled sample covariance matrix $\widehat{\bm\Sigma}$ is substituted for $\widehat{\bm \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{n} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}
is the MLE for $\bm \Sigma$. In this case \eqref{eq:qda} reduces to Fisher's \emph{LDA} classifier, and the log-determinant is omitted because it is constant across all classes.

We also require the following additional notation throughout the document. Let $\bm A \oplus \bm B$ denote the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Let $\bm A^+$ and $\mathcal{N}(\bm A) = \{\bm z \in \mathbb{R}_{p \times 1} : \bm A \bm z = \bm 0\}$ denote the Moore-Penrose pseudoinverse and null space of $\bm A \in \mathbb{R}_{m \times p}$, respectively. Let $V^{\perp} = \{\bm y \in \mathbb{R}_{p \times 1} : \bm y'\bm v = 0\ \forall \bm v \in V \}$ denote the orthogonal complement of a vector space $V \subset \mathbb{R}_{p \times 1}$. Let $\bm I_m$ be the $m \times m$ identity matrix, and let $\bm 0_{m \times p}$ be the $m \times p$ matrix of zeros, such that $\bm 0_m$ is understood to denote $\bm 0_{m \times m}$.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{\bm \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest eigenvalues are underestimated \citep{Seber:2004uh}, Moreover, if $p > n_k$, then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p - n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are 0. Furthermore, although more feature information is available to discriminate among the $K$ classes, if $p > n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does not exist.

Several regularization methods, such as the methods considered by \citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$ so that \eqref{eq:qda} is calculable and has reduced variability in the standard bias-variance tradeoff.  A common form of the covariance-matrix regularization applies a shrinkage factor $\gamma_k > 0$, so that
\begin{align}
	\widehat{\bm \Sigma}_k(\gamma_k) =  \widehat{\bm \Sigma}_k + \gamma_k \bm I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation \eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance matrix $\widehat{\bm\Sigma}_k$ towards $\bm I_p$, thereby increasing the eigenvalues of $\widehat{\bm\Sigma}_k$ by $\gamma_k$. Specifically, the zero eigenvalues are replaced with $\gamma_k$, so that \eqref{eq:ridge-estimator} is positive definite. For additional covariance-matrix regularization methods, see \cite{Ramey:2011ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\subsection{Friedman's Regularized Discriminant Analysis Classifier}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$ to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda n$, $\bm S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret \eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in \eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces to \eqref{eq:pooled-cov} used in the \emph{LDA} classifier.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased \emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter that controls the shrinkage of \eqref{eq:sig-rda}  towards $\bm I_p$, which is weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from $\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the amount of shrinkage applied.

\cite{Friedman:1989tm} substituted \eqref{eq:sig-rda} into \eqref{eq:qda} to obtain the \emph{RDA} classifier
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}

\subsection{A Generalization of Regularized Discriminant Analysis}

Here, we define our generalization of the \emph{RDA} classifier by first simplifying the formulation of $\widehat{\bm \Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda} and demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations centered by their respective class sample means. We define
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we avoid the convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By rewriting \eqref{eq:sig-lambda-alternative} in terms of the observations $\bm x_i$, $i = 1, \ldots, N$, each centered by its class sample mean, we attain a clear interpretation of $\widehat{\bm\Sigma}_k(\lambda)$. We have that
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
%	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'} \nonumber \\
%	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of each of the $N$ observations to estimating $\bm \Sigma_k$. That is, for $0 < \lambda \le 1$, we estimate $\bm \Sigma_k$ with $N$ rather than $n_k$ observations. Below, we will show that the pooling operation is advantageous in increasing the rank of each $\widehat{\bm\Sigma}_k(\lambda)$ from rank$(\widehat{\bm\Sigma}_k)$ to rank$(\widehat{\bm\Sigma})$ for $0 < \lambda \le 1$. Notice that if $\lambda  = 0$, then the observations from the remaining $K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to $\widehat{\bm\Sigma}$. For brevity, for $\lambda = 1$, we define $\bm X = [\sqrt{c_{1k}(1)} \bm x_1', \ldots,  \sqrt{c_{Nk}(1)} \bm x_N']'$ such that $\widehat{\bm \Sigma} = \bm X' \bm X$. Similarly, for $\lambda = 0$, we define $\bm X_k = [\sqrt{c_{1k}(0)} \bm x_1', \ldots,  \sqrt{c_{Nk}(0)} \bm x_N']'$ such that $\widehat{\bm \Sigma}_k = \bm X_k' \bm X_k$, $k = 1, \ldots, K$.


As we have discussed, several eigenvalue adjustment methods have been proposed to increase eigenvalues (approximately) equal to 0. Here, we define a more general eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\bm \Sigma}_k = \alpha_k \widehat{\bm \Sigma}_k(\lambda) + \bm \Phi_k,\label{eq:grda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\bm \Phi_k \in \mathbb{R}_{p \times p}^{\ge}$ is a diagonal matrix consisting of the eigenvalue shrinkage constants. Thus, we have the \emph{GRDA} (\emph{GRDA}) classifier
\begin{align}
	D_{GRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k|, \label{eq:grda}
\end{align}
The specification of $\alpha_k$ and $\bm \Phi_k$ in \eqref{eq:grda-cov} allows for a large family of eigenvalue-shrinkage methods and their relationships to be examined in depth.

\section{The GRDA Classifier}
In this section, we demonstrate that the \emph{GRDA} classifier facilitates an efficient calculation of \eqref{eq:grda} and a rapid model selection of the pooling parameter $\lambda$. Our approach is to decompose \eqref{eq:grda} into a sum of two components, where the first summand consists of matrix operations applied to low-dimensional matrices and the second summand corresponds to the null space of $\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix operations performed on the null space of $\widehat{\bm \Sigma}$ yield constant quadratic forms across all classes and can be omitted under mild regularity conditions. For $p \gg N$, the constant component involves determinants and inverses of high-dimensional matrices, and by ignoring these calculations, a substantial reduction in computational costs is achieved relative to Friedman's \emph{RDA} classifier in \eqref{eq:rda}. Furthermore, a byproduct of this omission is that adjustments to the associated eigenvalues have no effect on \eqref{eq:grda}. Lastly, we exploit the so-called Fast singular value decomposition (SVD) to construct the eigenvalue decomposition of $\widehat{\bm \Sigma}$, further reducing the computational costs of our proposed \emph{RDA} classifier.

First, we require the following relationship regarding the null spaces of $\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$ as discussed in Section \ref{sec:rda}, and let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $k = 1, \ldots, K$, $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k(\lambda))$ for some $k = 1, \ldots, K$. Hence, $0 = \bm z' \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 - \lambda) \bm z' \widehat{\bm \Sigma}_k \bm z + \lambda \bm z' \widehat{\bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in \mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular, we have that $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z' \widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z' \widehat{\bm \Sigma}_k \bm z$, which implies that $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times p}^{\ge}$ for $k = 1, \ldots, K$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative expression for $\tilde{\bm \Sigma}_k$ in terms of the matrix of eigenvectors of $\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p \times p}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$ with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, $\bm D_q \in \mathbb{R}_{q \times q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of $\widehat{\bm \Sigma}$,  the columns of $\bm U \in \mathbb{R}_{p \times p}$ are the corresponding orthonormal eigenvectors of $\widehat{\bm \Sigma}$, and rank$(\widehat{\bm \Sigma}) = q$. Then, we partition $\bm U = (\bm U_1, \bm U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in \mathbb{R}_{p \times (p - q)}$. We also partition $\bm \Phi_k = \bm \Phi_{k1} \oplus \bm \Phi_{k2}$ such that $\bm \Phi_{k1} \in \mathbb{R}_{q \times q}$ and $\bm \Phi_{k2} \in \mathbb{R}_{(p-q) \times (p-q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) = q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \bm \Phi_{k2})\bm U',\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= \alpha_k \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \bm \Phi_{k1}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $\bm U_2$ span the null space of $\widehat{\bm \Sigma}$, which implies that $\widehat{\bm \Sigma}_k \bm U_2 = \bm 0_{p \times (p - q)}$. Hence, for $k = 1, \ldots, K$, $\bm U' \widehat{\bm \Sigma}_k \bm U = \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm 0_{p-q}$. Then, notice that $\bm U' \tilde{\bm \Sigma}_k \bm U = \alpha_k \{(1 - \lambda) \bm U' \widehat{\bm \Sigma}_k \bm U + \lambda \bm D\} + \bm U' \bm \Phi_k \bm U$. Thus, \eqref{eq:rda-matrix} follows because $\bm U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we have the following corollary.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$, rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$ for all $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\bm \Phi_k = \bm 0_p$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ to $q$, $k = 1, \ldots, K$ if the pooling parameter $\lambda \ne 0$. Next, we provide an essential result that enables us to prove that, under mild regularity conditions, \eqref{eq:grda} is invariant to adjustments to the eigenvalues of $\tilde{\bm \Sigma}_k$ corresponding to the null space of $\widehat{\bm \Sigma}$ for any unlabeled observation $\bm x \in \mathbb{R}_{p \times 1}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times 1}$, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$, $1 \le k, k' \le K$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$. Then, $\xbar_k - \xbar_{k'} = \sum_{i=1}^N \beta_i \bm x_i$, where $\beta_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i = k') n_k \}$.  Then, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$ is equivalent to $\bm U_2'(\xbar_k - \xbar_{k'}) = \sum_{i=1}^N \beta_i \bm U_2' \bm x_i$. Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that $\bm U_2' \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma 1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{\bm \Sigma})$, for $i = 1, \ldots, N$, $\bm U_2' \bm x_i = \bm 0_{(p-q) \times 1}$, which implies that $\bm U_2'(\xbar_k - \xbar_{k'}) = \bm 0_{(p-q) \times 1}$. Therefore, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$.
\end{proof}

Now, we present our main result, where we decompose the \emph{GRDA} decision rule and show that the component requiring the largest computational costs does not contribute to the classification performed using Lemma \ref{lemma:RDA-constant-term}. To reduce the computational costs of the \emph{GRDA} classifier, we require that $\bm \Phi_{k2} = \bm \Phi_2$, $k = 1, \ldots, K$.
Here, we present our main result that reduces \eqref{eq:grda} to an equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ and $\bm W_k$ be defined as in \eqref{eq:rda-matrix} and \eqref{eq:Wk}, respectively, and let $\bm U_1$ be defined as above. Additionally, let $\bm \Phi_{k2} = \bm \Phi_2$, $k = 1, \ldots, K$. Then, the \emph{GRDA} decision rule in \eqref{eq:grda} is equivalent to the reduced form
	\begin{align}
		D_{GRDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k) + \ln | \bm W_k |. \label{eq:rda-decomposed}
	\end{align}
	
\end{thm}
\begin{proof}
	From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{-1} = \bm U(\bm W_k^{-1} \oplus\bm \Phi_{k2}^{-1})\bm U'$ and $|\tilde{\bm \Sigma}_k| = | \bm W_k | |\bm \Phi_{k2}|$. Therefore, for all $\bm x \in \mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)' \tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k| &= (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)\\
	&+ (\bm x - \xbar_k)' \bm U_2 \bm \Phi_{k2}^{-1} \bm U_2' (\bm x - \xbar_k) + \ln | \bm W_k | + \ln |\bm \Phi_{k2}|.
	\end{align*}
From Lemma \ref{lemma:RDA-constant-term}, $\bm U_2' (\bm x - \xbar_k)$ is constant for all $k = 1, \ldots, K$. Therefore, the proof follows because $\bm \Phi_{k2} = \bm \Phi_2$.
\end{proof}

\subsection{Model Selection}

Thus far we have shown that the \emph{GRDA} classifier in \eqref{eq:rda-decomposed} is invariant to the $\bm U_2$ term, yielding a substantial reduction in the computational costs. While the classifier is much improved in terms of computational performance compared to the \emph{RDA} classifier originally proposed by \cite{Friedman:1989tm}, we provide two additional results that further improve the computational performance of our \emph{GRDA} classifier. First, we demonstrate that the inverse and determinants of $\bm W_k$ can be written so that they are performed on $n_k \times n_k$ matrices rather than the larger $q \times q$ matrices, resulting in further improvements to the computational efficiency of the \emph{GRDA} classifier.

\begin{proposition}\label{proposition:rda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - \alpha_k(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},\label{eq:W_k_inv}
	\end{align}
where $\bm Q_k = \bm I_{n_k} + \alpha_k(1- \lambda) \bm X_k \bm U_1 \bm \Gamma_k^{-1} \bm U_1' \bm X_k'$ and $\bm \Gamma_k = \alpha_k \lambda \bm D_q + \bm \Phi_{k1}$.
\end{proposition}
\begin{proof}
	To calculate $|\bm W_k|$, from \cite{Harville:2008wja} we apply Theorem 18.1.1, which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} + \bm C \bm A^{-1} \bm B|$. Thus, setting $\bm A = \bm \Gamma_k$, $\bm B = \alpha_k (1 - \lambda) \bm U_1' \bm X_k'$, $\bm T = \bm I_{n_k}$, and $\bm C = \bm X_k \bm U_1$, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also,  \eqref{eq:W_k_inv} follows from the well-known Sherman-Woodbury formula \citep[Theorem 18.2.8]{Harville:2008wja}.
\end{proof}

Thus far, we have construction each $\tilde{\bm \Sigma}_k$ in terms of the first $q$ eigenvectors of $\widehat{\bm \Sigma}$. In practice, we would usually require a costly eigendecomposition of $\widehat{\bm \Sigma}$, but by utilizing a technique similar to the so-called kernel trick, we can obtain $\bm U_1$ by computing the eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$ \citep{Hastie:2008dt}. Recall that the pooled sample covariance matrix $\widehat{\bm\Sigma} = \bm X' \bm X$. Then, by writing the SVD of $\bm X = \bm M \bm D^{1/2} \bm U'$, we have the spectral decomposition $\widehat{\bm\Sigma} = \bm U \bm D \bm U'$, where $\bm M \in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D^{1/2} \in \mathbb{R}_{N \times N}$ is a diagonal matrix consisting of the singular values of $\bm X$, and $\bm U \in \mathbb{R}_{p \times N}$ is orthogonal. Now, we see that $\bm X \bm X' = \bm M \bm D \bm M'$ is the eigenvalue decomposition of $\bm X \bm X'$, where $\bm D$ is the matrix of eigenvalues of $\bm X \bm X'$ and the columns of $\bm M$  are the corresponding eigenvectors. Hence, we obtain $\bm M$ and $\bm D$ from an eigendecomposition of $\bm X \bm X' \in \mathbb{R}_{N \times N}$. Next, we compute $\bm U = \bm X' \bm M \bm D^{+/2}$, where $\bm D^{+/2} = \bm D_q^{-1/2} \oplus \bm 0_{N-q}$. We determine $q$ efficiently by computing the number of numerically nonzero eigenvalues present in $\bm D$ such that $q$ is the number of eigenvalues that exceeds some tolerance value, say, $1 \times 10^{-6}$. After calculating $q$, we retain only the first $q$ columns of $\bm U$ to extract $\bm U_1$.

\section{Monte Carlo Simulations}

Our generalization of the \emph{RDA} classifier facilitates a large number of potential models based on the specification of $\alpha_k$ and $\bm \Phi_k$ in \eqref{eq:grda-cov} to shrink the eigenvalues of $\widehat{\bm \Sigma}_k(\lambda)$. In our simulations below, we consider two choices of $\alpha_k$ and $\bm \Phi_k$ based on variants of eigenvalue adjustment methods proposed in the literature. First, ... 

Second, \cite{Srivastava:2007ww} derive an empirical-Bayes covariance-matrix estimator under the assumption that the data are independent and identically distributed multivariate normal observations and then substitute the estimator into the \emph{LDA} classifier to attain the minimum distance empirical Bayes (\emph{MDEB}) classifier. To derive the empirical-Bayes covariance-matrix estimator, \cite{Srivastava:2007ww} first assume that $\bm \Sigma^{-1}$ follows a Wishart distribution \emph{a priori} with mean $\lambda^{-1} \bm I_p$, $\lambda > 0$, and degrees of freedom, $l \ge p$. The resulting posterior estimator for $\bm \Sigma^{-1}$ is $\frac{N + l}{N}(N^{-1}\lambda \bm I_p + \widehat{\bm \Sigma})^{-1}$. Using an empirical Bayes argument to estimate $\lambda$, \cite{Srivastava:2007ww} propose the covariance-matrix estimator
\begin{align*}
	\widehat{\bm \Sigma}_{MDEB} = \widehat{\bm \Sigma} + \frac{\tr \{\widehat{\bm \Sigma}\}}{\min\{N, p\}} \bm I_p,
\end{align*}
where the shrinkage factor is approximately equal to the average of the nonzero eigenvalues of $\widehat{\bm \Sigma}$. We consider a variant of the \emph{MDEB} covariance-matrix estimator because \cite{Srivastava:2007ww} claim that the \emph{MDEB} classifier is the best covariance-matrix regularization method applied to the \emph{LDA} classifier they have encountered in the literature.

\section{High-Dimensional Microarray Data Sets}

In this section, we describe three high-dimensional microarray data sets and compare our proposed \emph{GRDA} classifier with two recently proposed classifiers for small-sample, high-dimensional data: the penalized \emph{LDA} (\emph{PLDA}) classifier from \cite{Witten:2011kc}, the shrinkage-mean-based DLDA (\emph{SmDLDA}) classifier from \cite{Tong:2012hw}, the \emph{RDA} classifier as defined in \cite{Hastie:2008dt}, and the sparse discriminant analysis (\emph{SDA}) classifier from \cite{Clemmensen:2011kr}. We calculated 10-fold cross-validation error rates \citep{Hastie:2008dt} to evaluate the performance of each classifier considered. Because the \emph{PLDA} classifier has a built-in variable selection method, for fair comparison we compared each of the classifiers using the variables selected from each cross-validation fold.

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC) through hybridization to microarrays from 127 individuals resulting in 22,283 sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD), and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide microarray from 105 samples of 10 types of soft tissue tumors. This included 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS), 15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21 samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl} determined from their data that these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following \cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at least 15 observations.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from surgery patients between 1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600 genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are of high quality: 52 prostate tumour samples and 50 non-tumour prostate samples.

\subsection{Classification Results}

As we can see, our competitors attain decent classification accuracy only when the number of variables selected is small. As the number of variables is increased, the classification accuracy of our competitors degrades.

\section{Discussion}

Our formulation of covariance-matrix regularization employed in the \emph{GRDA} classifier includes a large family of shrinkage methods, such as the methods from \cite{Srivastava:2007ww}, \cite{Rao:1971ul}, and a wide variety of other shrinkage methods studied by \cite{Ramey:2011ji} and \cite*{Xu:2009fl}.


\section{Next Actions}

\begin{itemize}
\item Add simulation results
\item Discussion simulation results
\item Show a timing comparison between our implementation and that of the {\tt klaR} package.
\item Discuss choice of $\lambda$ and eigenvalue shrinkage, if necessary
\item Question to answer: What do we do when $\lambda = 0$, corresponding to \emph{QDA}?
\end{itemize}

	

\bibliographystyle{plainnat}
\bibliography{rda}


\end{document} 
