\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{A Generalization of Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
\cite{Friedman:1989tm} has proposed the popular regularized discriminant analysis (\emph{RDA}) classifier for the small-sample, high-dimensional setting. This \emph{RDA} classifier employs a regularized covariance-matrix estimator, which incorporates a ridge-like shrinkage factor by adding a positive constant to the zero-eigenvalue estimates to ensure positive definiteness. Here, we propose a novel generalization of Friedman's \emph{RDA} classifier that is more computationally efficient than its original counterpart because the inverses and determinants computed in the decision rule can be reduced from the original feature space to a linear subspace with dimensions corresponding to the individual class sample sizes. Additionally, our generalization of the \emph{RDA} (\emph{GRDA}) classifier enables efficient estimation of optimal tuning parameters from a large set of candidate values, simplifies the construction of the regularized covariance-matrix estimator to improve its interpretation, and allows for a more flexible classification model. Moreover, we demonstrate with several high-dimensional microarray data sets that our proposed classifier is competitive and often superior to several recently proposed sparse and regularized classifiers. Finally, we provide an implementation of the \emph{GRDA} classifier in the {\tt regdiscrim} R package, available on CRAN.
\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of thousands of genes in a single experiment to identify a subset of genes and their expression patterns that either characterize a particular cell state or facilitate an accurate prediction of future cell state.  We focus on the classification of tissues and other biological samples into one of $K$ disjoint, predefined cancer types, where our goal is to ascertain the class membership of a new tissue sample based on gene-expression microarray data. However, microarray data typically consist of a large number of gene expressions $p$ relative to the sample size $N$. Moreover, standard benchmark microarray data sets usually consist of between $p = 10,000$ and $55,000$ probes and fewer than $N = 100$ observations, in which case standard supervised-learning methods exhibit poor classification performance, due to the \emph{curse of dimensionality} \citep{Bellman:1961tn}. Furthermore, well-known classifiers, including linear discriminant analysis \emph{(LDA)} and quadratic discriminant analysis \emph{(QDA)}, are incalculable when $p > N$ because the sample covariance matrices employed in both classifiers are singular. As a result, many researchers, including \cite*{Mai:2012bf}, \cite*{Merchante:2012vk}, \cite{Witten:2011kc}, \cite*{Pang:2009ik}, \cite*{Huang:2010ju}, and \cite*{Clemmensen:2011kr}, have introduced restrictions on the population covariance matrices in the \emph{LDA} and \emph{QDA} models to reduce number of covariance parameters that must be estimated to ensure that the resulting classifiers are calculable.  For instance, \cite{Dudoit:2002ev} have proposed the diagonal \emph{LDA} (\emph{DLDA}) classifier, a modification of the \emph{LDA} classifier where the class covariance matrices are assumed to be diagonal.

Alternatively, several researchers, including \cite*{Zhang:2010va}, \cite{Ye:2009gd}, \cite{Ji:2008wp}, \cite*{Guo:2007te}, \cite{Srivastava:2007ud}, \cite{Ye:2006vx}, and \cite{Ye:2005uu}, have proposed regularization methods to improve the estimation of the population covariance matrices before applying the \emph{LDA} and \emph{QDA} classifiers. Covariance-matrix regularization methods often are based on the well-known ridge-regression approach from \cite{Hoerl:1970cd} and adjust the eigenvalues of the sample covariance matrices to ensure positive definiteness. For excellent overviews of the regularization problem in discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we consider a generalization of the popular \emph{RDA} classifier of \cite{Friedman:1989tm}, who has proposed a weighted average of the covariance-matrix estimators from the \emph{LDA} and \emph{QDA} classifiers. \cite{Friedman:1989tm} has additionally incorporated a covariance-matrix shrinkage term to ensure that the \emph{RDA} classifier is well-posed. \cite{Friedman:1989tm} has shown that the \emph{RDA} classifier can attain improved classification accuracy, compared to the \emph{LDA} and \emph{QDA} classifiers when $p > N$. However, in its original form, the \emph{RDA} classifier is impractical when $p \gg N$ because the \emph{RDA} decision rule requires the calculation of the inverses and determinants of the $p \times p$ covariance-matrix estimators for each class. Furthermore, the \emph{RDA} classifier has two tuning parameters that are selected via cross-validation, which adds a substantial computational burden to the model-selection process. The \emph{RDA} classifier is ill-advised on modern data sets because the model selection requires $O(Kp^3)$ calculations for each tuning-parameter pair for each fold in the cross-validation scheme.

In this paper we address the computational shortcomings of the \emph{RDA} classifier. Applying a technique similar to \cite{Ye:2006tq}, we decompose the \emph{RDA} classifier and show that the sum related to the null space of the pooled sample covariance matrix can be discarded. In turn, this omission provides a substantial increase in the computational efficiency of the \emph{RDA} classifier and its model-selection. In fact, we show rigorously that the inverse and determinants can be reduced to calculations of $n_k \times n_k$ matrices, where $n_k$ is the sample size of the $k$th class, $k = 1, \ldots, K$. This reduction in calculation is clearly advantageous because $n_k$ is often quite small, say, 15--20, in microarray data sets. Thus, we demonstrate that the calculation of the \emph{RDA} decision rule involving inverses and determinants of extremely large covariance matrices is equivalent a decision rule involving inverses and determinants on much smaller matrices.

Also, for our \emph{GRDA} classifier we first simplify the pooling matrix in the \emph{RDA} classifier by constructing instead a convex combination of the covariance-matrix estimators employed in the \emph{LDA} and \emph{QDA} classifiers. This simplification is advantageous in that it improves the interpretation of the pooling operation of the \emph{RDA} classifier. We show that under mild regularity conditions the \emph{GRDA} classifier is invariant to any adjustments of the eigenvalues corresponding to the null space of the pooled sample covariance matrix. Hence, no gain in classification performance can be obtained by adjusting these zero eigenvalues. Although \cite{Ji:2008wp} and \cite{Ye:2006jm} have provided a similar invariance property for the \emph{LDA} decision rule, the \emph{LDA} classifier involves the assumption that the population covariance matrices are equal, which is often too stringent in practice \citep{Clemmensen:2011kr}. The \emph{GRDA} classifier allows us to relax this assumption by including unrestricted covariance matrices in the model. The \emph{GRDA} classifier is then a compromise between the two approaches, favoring the stronger linear assumption if a larger value of the pooling parameter is selected as optimal. Our formulation provides a general framework from which we can elucidate properties of covariance-regularization methods and the relationships among them in the linear case, the quadratic case, and the cases in between.

Many of the recently proposed classifiers have a variable-selection method integrated. While this is advantageous in potentially determining biomarkers that are correlated with the classification label, these methods often yield poor classification accuracy if a large number of variables are retained. Moreover, these methods can require large computational costs, making these methods undesirable in practice. To study these issues in depth, we consider several simulated data sets, where we show that the classification degrades as the number of features is increased. Contrarily, our proposed \emph{GRDA} classifier handles the large number of features well because our included dimension-reduction method retains only the classificatory information in the covariance-matrix estimators. Moreover, while the competing classifiers exhibit large computational costs as the number of features in the original data set is increased, the \emph{GRDA} classifier handles the larger number of features efficiently.

In addition to the improvement in computational performance of classifiers with regularized covariance matrices, we demonstrate that our \emph{GRDA} classifier is competitive with and often superior to several recent classifiers designed for small-sample, high-dimensional data. In particular, we compare our \emph{GRDA} classifier with an alternatively-defined \emph{RDA} classifier from \cite*{Hastie:2008dt}, the regularized \emph{LDA} classifier from \cite{Guo:2007te}, a recently proposed variant of the \emph{DLDA} classifier from \cite*{Tong:2012hw}, and the penalized \emph{LDA} classifier from \cite{Witten:2011kc}. We conduct our comparison of the classification accuracy of the competing classifiers using four small-sample, high-dimensional microarray data sets.

We have organized the remainder of the paper as follows. In Section 2 we present our generalization of the \emph{RDA} classifier after reviewing its original counterpart from \cite{Friedman:1989tm}. In Section 3 we show that our proposed classifier includes several well-known regularized classifiers as special cases and briefly discuss some commonalities among them. In Section 4 we describe four small-sample, high-dimensional microarray data sets and then demonstrate that our two proposed classifiers are superior to the competing classifiers in terms of classification accuracy. We then conclude with a brief discussion in Section 5.

\section{Regularized Discriminant Analysis}
\label{sec:rda}

In discriminant analysis we wish to assign correctly an unlabeled $p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes (or populations) by constructing a classifier from $n$ training observations that can accurately predict the class membership of $\bm x$. Let $\bm x_i = (x_{i1}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots, N)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{a \times b}$ denotes the matrix space of all $a \times b$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of class membership of the $k$th class.

The \emph{QDA} classifier is the optimal Bayesian decision rule with respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate normal distribution with known mean vector  $\bm\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite matrices. Because the parameters are typically unknown, we estimate the unknown parameters $\bm \mu_k$ and $\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the MLEs into the \emph{QDA} classifier. Assuming that the prior probabilities of class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an unlabeled observation $\bm x$ to class $\omega_k$ with the sample \emph{QDA} classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further that the population covariance matrices are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$ for all $k$), then the pooled sample covariance matrix $\widehat{\bm\Sigma}$ is substituted for $\widehat{\bm \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{N} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}
is the MLE for $\bm \Sigma$. In this case \eqref{eq:qda} reduces to Fisher's \emph{LDA} classifier, and the log-determinant is omitted because it is constant across all classes.

We also require the following additional notation throughout the document. Let $\bm A \oplus \bm B$ denote the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Let $\bm A^+$ and $\mathcal{N}(\bm A) = \{\bm z \in \mathbb{R}_{p \times 1} : \bm A \bm z = \bm 0\}$ denote the Moore-Penrose pseudoinverse and null space of $\bm A \in \mathbb{R}_{m \times p}$, respectively. Let $V^{\perp} = \{\bm y \in \mathbb{R}_{p \times 1} : \bm y'\bm v = 0\ \forall \bm v \in V \}$ denote the orthogonal complement of a vector space $V \subset \mathbb{R}_{p \times 1}$. Let $\bm I_m$ be the $m \times m$ identity matrix, and let $\bm 0_{m \times p}$ be the $m \times p$ matrix of zeros, such that $\bm 0_m$ is understood to denote $\bm 0_{m \times m}$.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{\bm \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest eigenvalues are underestimated \citep{Seber:2004uh}, Moreover, if $p > n_k$, then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p - n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are 0. Furthermore, although more feature information is available to discriminate among the $K$ classes, if $p > n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does not exist.

Several regularization methods, such as the methods considered by \citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$ so that \eqref{eq:qda} is calculable and has reduced variability in the standard bias-variance tradeoff.  A common form of the covariance-matrix regularization applies a shrinkage factor $\gamma_k > 0$, so that
\begin{align}
	\widehat{\bm \Sigma}_k(\gamma_k) =  \widehat{\bm \Sigma}_k + \gamma_k \bm I_p, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression \citep{Hoerl:1970cd}. Equation \eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance matrix $\widehat{\bm\Sigma}_k$ towards $\bm I_p$, thereby increasing the eigenvalues of $\widehat{\bm\Sigma}_k$ by $\gamma_k$. Specifically, the zero eigenvalues are replaced with $\gamma_k$, so that \eqref{eq:ridge-estimator} is positive definite. For additional covariance-matrix regularization methods, see \cite{Ramey:2013ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\subsection{Friedman's Regularized Discriminant Analysis Classifier}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$ to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda N$, $\bm S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret \eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in \eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces to \eqref{eq:pooled-cov} used in the \emph{LDA} classifier.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased \emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter that controls the shrinkage of \eqref{eq:sig-rda}  towards $\bm I_p$, which is weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from $\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the amount of shrinkage applied.

\cite{Friedman:1989tm} substituted \eqref{eq:sig-rda} into \eqref{eq:qda} to obtain the \emph{RDA} classifier
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}

\subsection{A Generalization of Regularized Discriminant Analysis}

Here, we define our generalization of the \emph{RDA} classifier by first simplifying the formulation of $\widehat{\bm \Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda} and demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations centered by their respective class sample means. We define
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we avoid the convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By rewriting \eqref{eq:sig-lambda-alternative} in terms of the observations $\bm x_i$, $i = 1, \ldots, N$, each centered by its class sample mean, we attain a clear interpretation of $\widehat{\bm\Sigma}_k(\lambda)$. We have that
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
%	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'} \nonumber \\
%	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of each of the $N$ observations to estimating $\bm \Sigma_k$. That is, for $0 < \lambda \le 1$, we estimate $\bm \Sigma_k$ with $N$ rather than $n_k$ observations. Below, we will show that the pooling operation is advantageous in increasing the rank of each $\widehat{\bm\Sigma}_k(\lambda)$ from rank$(\widehat{\bm\Sigma}_k)$ to rank$(\widehat{\bm\Sigma})$ for $0 < \lambda \le 1$. Notice that if $\lambda  = 0$, then the observations from the remaining $K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to $\widehat{\bm\Sigma}$. For brevity, for $\lambda = 1$, we define $\bm X = [\sqrt{c_{1k}(1)} \bm x_1', \ldots,  \sqrt{c_{Nk}(1)} \bm x_N']'$ such that $\widehat{\bm \Sigma} = \bm X' \bm X$. Similarly, for $\lambda = 0$, we define $\bm X_k = [\sqrt{c_{1k}(0)} \bm x_1', \ldots,  \sqrt{c_{Nk}(0)} \bm x_N']'$ such that $\widehat{\bm \Sigma}_k = \bm X_k' \bm X_k$, $k = 1, \ldots, K$.


As we have discussed, several eigenvalue adjustment methods have been proposed to increase eigenvalues (approximately) equal to 0. Here, we define a more general eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} as
\begin{align}
	\tilde{\bm \Sigma}_k = \alpha_k \widehat{\bm \Sigma}_k(\lambda) + \bm \Phi_k,\label{eq:grda-cov}
\end{align}
where $\alpha_k \ge 0$ and $\bm \Phi_k \in \mathbb{R}_{p \times p}^{\ge}$ is a diagonal matrix consisting of the eigenvalue shrinkage constants. Thus, we have the \emph{GRDA} (\emph{GRDA}) classifier
\begin{align}
	D_{GRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k|, \label{eq:grda}
\end{align}
The specification of $\alpha_k$ and $\bm \Phi_k$ in \eqref{eq:grda-cov} allows for a large family of eigenvalue-shrinkage methods and their relationships to be examined in depth.

\section{The GRDA Classifier}
In this section, we demonstrate that the \emph{GRDA} classifier facilitates an efficient calculation of \eqref{eq:grda} and a rapid model selection of the pooling parameter $\lambda$. Our approach is to decompose \eqref{eq:grda} into a sum of two components, where the first summand consists of matrix operations applied to low-dimensional matrices and the second summand corresponds to the null space of $\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix operations performed on the null space of $\widehat{\bm \Sigma}$ yield constant quadratic forms across all classes and can be omitted under mild regularity conditions. For $p \gg N$, the constant component involves determinants and inverses of high-dimensional matrices, and by ignoring these calculations, a substantial reduction in computational costs is achieved relative to Friedman's \emph{RDA} classifier in \eqref{eq:rda}. Furthermore, a byproduct of this omission is that adjustments to the associated eigenvalues have no effect on \eqref{eq:grda}. Lastly, we exploit the so-called Fast singular value decomposition (SVD) to construct the eigenvalue decomposition of $\widehat{\bm \Sigma}$, further reducing the computational costs of our proposed \emph{RDA} classifier.

First, we require the following relationship regarding the null spaces of $\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$ as discussed in Section \ref{sec:rda}, and let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $k = 1, \ldots, K$, $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k(\lambda))$ for some $k = 1, \ldots, K$. Hence, $0 = \bm z' \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 - \lambda) \bm z' \widehat{\bm \Sigma}_k \bm z + \lambda \bm z' \widehat{\bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in \mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular, we have that $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z' \widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z' \widehat{\bm \Sigma}_k \bm z$, which implies that $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times p}^{\ge}$ for $k = 1, \ldots, K$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative expression for $\tilde{\bm \Sigma}_k$ in terms of the matrix of eigenvectors of $\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p \times p}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$ with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, $\bm D_q \in \mathbb{R}_{q \times q}^{>}$ is the diagonal matrix consisting of the positive eigenvalues of $\widehat{\bm \Sigma}$,  the columns of $\bm U \in \mathbb{R}_{p \times p}$ are the corresponding orthonormal eigenvectors of $\widehat{\bm \Sigma}$, and rank$(\widehat{\bm \Sigma}) = q$. Then, we partition $\bm U = (\bm U_1, \bm U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in \mathbb{R}_{p \times (p - q)}$. We also partition $\bm \Phi_k = \bm \Phi_{k1} \oplus \bm \Phi_{k2}$ such that $\bm \Phi_{k1} \in \mathbb{R}_{q \times q}$ and $\bm \Phi_{k2} \in \mathbb{R}_{(p-q) \times (p-q)}$.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) = q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \bm \Phi_{k2})\bm U',\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= \alpha_k \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \bm \Phi_{k1}.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
From Lemma \ref{lemma:null-spaces}, the columns of $\bm U_2$ span the null space of $\widehat{\bm \Sigma}$, which implies that $\widehat{\bm \Sigma}_k \bm U_2 = \bm 0_{p \times (p - q)}$. Hence, for $k = 1, \ldots, K$, $\bm U' \widehat{\bm \Sigma}_k \bm U = \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm 0_{p-q}$. Then, notice that $\bm U' \tilde{\bm \Sigma}_k \bm U = \alpha_k \{(1 - \lambda) \bm U' \widehat{\bm \Sigma}_k \bm U + \lambda \bm D\} + \bm U' \bm \Phi_k \bm U$. Thus, \eqref{eq:rda-matrix} follows because $\bm U$ is orthogonal.
\end{proof}

As an immediate immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k}, we have the following corollary.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$, rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$ for all $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\bm \Phi_k = \bm 0_p$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ to $q$, $k = 1, \ldots, K$ if the pooling parameter $\lambda \ne 0$. Next, we provide an essential result that enables us to prove that, under mild regularity conditions, \eqref{eq:grda} is invariant to adjustments to the eigenvalues of $\tilde{\bm \Sigma}_k$ corresponding to the null space of $\widehat{\bm \Sigma}$ for any unlabeled observation $\bm x \in \mathbb{R}_{p \times 1}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times 1}$, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$, $1 \le k, k' \le K$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$. Then, $\xbar_k - \xbar_{k'} = \sum_{i=1}^N \beta_i \bm x_i$, where $\beta_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i = k') n_k \}$.  Then, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$ is equivalent to $\bm U_2'(\xbar_k - \xbar_{k'}) = \sum_{i=1}^N \beta_i \bm U_2' \bm x_i$. Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that $\bm U_2' \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma 1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{\bm \Sigma})$, for $i = 1, \ldots, N$, $\bm U_2' \bm x_i = \bm 0_{(p-q) \times 1}$, which implies that $\bm U_2'(\xbar_k - \xbar_{k'}) = \bm 0_{(p-q) \times 1}$. Therefore, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$.
\end{proof}

Now, we present our main result, where we decompose the \emph{GRDA} decision rule and show that the component requiring the largest computational costs does not contribute to the classification performed using Lemma \ref{lemma:RDA-constant-term}. To reduce the computational costs of the \emph{GRDA} classifier, we require that $\bm \Phi_{k2} = \bm \Phi_2$, $k = 1, \ldots, K$.
Here, we present our main result that reduces \eqref{eq:grda} to an equivalent, more computationally efficient decision rule.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ and $\bm W_k$ be defined as in \eqref{eq:rda-matrix} and \eqref{eq:Wk}, respectively, and let $\bm U_1$ be defined as above. Additionally, let $\bm \Phi_{k2} = \bm \Phi_2$, $k = 1, \ldots, K$. Then, the \emph{GRDA} decision rule in \eqref{eq:grda} is equivalent to the reduced form
	\begin{align}
		D_{GRDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k) + \ln | \bm W_k |. \label{eq:rda-decomposed}
	\end{align}
	
\end{thm}
\begin{proof}
	From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{-1} = \bm U(\bm W_k^{-1} \oplus\bm \Phi_{k2}^{-1})\bm U'$ and $|\tilde{\bm \Sigma}_k| = | \bm W_k | |\bm \Phi_{k2}|$. Therefore, for all $\bm x \in \mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)' \tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k| &= (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)\\
	&+ (\bm x - \xbar_k)' \bm U_2 \bm \Phi_{k2}^{-1} \bm U_2' (\bm x - \xbar_k) + \ln | \bm W_k | + \ln |\bm \Phi_{k2}|.
	\end{align*}
From Lemma \ref{lemma:RDA-constant-term}, $\bm U_2' (\bm x - \xbar_k)$ is constant for all $k = 1, \ldots, K$. Therefore, the proof follows because $\bm \Phi_{k2} = \bm \Phi_2$.
\end{proof}

\subsection{Model Selection}

Thus far we have shown that the \emph{GRDA} classifier in \eqref{eq:rda-decomposed} is invariant to the $\bm U_2$ term, yielding a substantial reduction in the computational costs. While the classifier is much improved in terms of computational performance compared to the \emph{RDA} classifier originally proposed by \cite{Friedman:1989tm}, we provide two additional results that further improve the computational performance of our \emph{GRDA} classifier. First, we demonstrate that the inverse and determinants of $\bm W_k$ can be written so that they are performed on $n_k \times n_k$ matrices rather than the larger $q \times q$ matrices, resulting in further improvements to the computational efficiency of the \emph{GRDA} classifier.

\begin{proposition}\label{proposition:rda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - \alpha_k(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},\label{eq:W_k_inv}
	\end{align}
where $\bm Q_k = \bm I_{n_k} + \alpha_k(1- \lambda) \bm X_k \bm U_1 \bm \Gamma_k^{-1} \bm U_1' \bm X_k'$ and $\bm \Gamma_k = \alpha_k \lambda \bm D_q + \bm \Phi_{k1}$.
\end{proposition}
\begin{proof}
	To calculate $|\bm W_k|$, from \cite{Harville:2008wja} we apply Theorem 18.1.1, which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} + \bm C \bm A^{-1} \bm B|$. Thus, setting $\bm A = \bm \Gamma_k$, $\bm B = \alpha_k (1 - \lambda) \bm U_1' \bm X_k'$, $\bm T = \bm I_{n_k}$, and $\bm C = \bm X_k \bm U_1$, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also,  \eqref{eq:W_k_inv} follows from the well-known Sherman-Woodbury formula \citep[Theorem 18.2.8]{Harville:2008wja}.
\end{proof}

Thus far, we have constructed each $\tilde{\bm \Sigma}_k$ in terms of the first $q$ eigenvectors of $\widehat{\bm \Sigma}$. By utilizing a technique similar to the so-called kernel trick, we can obtain $\bm U_1$ by computing the eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$ \citep{Hastie:2008dt}. Recall that the pooled sample covariance matrix $\widehat{\bm\Sigma} = \bm X' \bm X$. Then, by writing the SVD of $\bm X = \bm M \bm D^{1/2} \bm U'$, we have the spectral decomposition $\widehat{\bm\Sigma} = \bm U \bm D \bm U'$, where $\bm M \in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D^{1/2} \in \mathbb{R}_{N \times N}^{\ge}$ is a diagonal matrix consisting of the singular values of $\bm X$, and $\bm U \in \mathbb{R}_{p \times N}$ is orthogonal. Now, we see that $\bm X \bm X' = \bm M \bm D \bm M'$ is the eigenvalue decomposition of $\bm X \bm X'$, where $\bm D$ is the matrix of eigenvalues of $\bm X \bm X'$ and the columns of $\bm M$  are the corresponding eigenvectors. Hence, we obtain $\bm M$ and $\bm D$ from an eigendecomposition of $\bm X \bm X' \in \mathbb{R}_{N \times N}$. Next, we compute $\bm U = \bm X' \bm M \bm D^{+/2}$, where $\bm D^{+/2} = \bm D_q^{-1/2} \oplus \bm 0_{N-q}$. We determine $q$ efficiently by computing the number of numerically nonzero eigenvalues present in $\bm D$ such that $q$ is the number of eigenvalues that exceeds some tolerance value, say, $1 \times 10^{-6}$. After calculating $q$, we retain only the first $q$ columns of $\bm U$ to extract $\bm U_1$.

\section{High-Dimensional Microarray Data Sets}

In this section, we describe four high-dimensional microarray data sets and compare our proposed \emph{GRDA} classifier with five recently proposed classifiers designed for small-sample, high-dimensional data: the \emph{DLDA} classifier from \cite{Dudoit:2002ev}, the shrunken centroids \emph{RDA} (\emph{SCRDA}) classifier from \cite{Guo:2007te}, the shrinkage-based \emph{DLDA} (\emph{SDLDA}) classifier from \cite{Pang:2009ik}, and the shrinkage-mean-based DLDA (\emph{SmDLDA}) classifier from \cite{Tong:2012hw}, and the penalized \emph{LDA} (\emph{PLDA}) classifier from \cite{Witten:2011kc}. We evaluated the classification performance of each classifier by randomly partitioning the data such that $2/3$ of the observations were allocated as training data and the remaining $1/3$ of the observations were allocated as a test data set. For each random partition, we applied the feature-selection method from \cite{Dudoit:2002ev} to choose the $d$ features having the largest ratio of their between-group to within-group sums of squares from the training data. We then filtered the test data set with the corresponding $d$ features. After training each classifier from the filtered training data, we classified the filtered test data sets and computed the proportion of mislabeled test observations to estimate the classification error rate for each classifier. Repeating this process 500 times, we computed the average of the error-rate estimates for each classifier. We remark that our simulation design avoids the selection bias discussed in \cite{Ambroise:2002fa}.  In Table \ref{tab:data-summary}, we summarize the attributes of each microarray data set.

We used version 2.15.1 of the open-source statistical software {\tt R} for the classification study. For each classifier's {\tt R} implementation, we used the default settings with the exception that we explicitly set the prior probabilities as equal. In our {\tt R} implementation of the \emph{GRDA} classifier in the {\tt regdiscrim} package, we set $\alpha_k = 1$, $k = 1, \ldots, K$ for simplicity. We estimated $\lambda$ from a grid of 21 equidistant candidate values between 0 and 1, inclusively. Similarly, we estimated $\gamma$ from a grid consisting of the values $10^{-1}, \ldots, 10^4$, and $10^5$. We selected optimal estimates of $\lambda$ and $\gamma$ using 10-fold cross-validation \citep{Hastie:2008dt}.

 % latex table generated in R 2.15.1 by xtable 1.7-0 package
 % Sun Mar 24 19:54:37 2013
 \begin{table}[ht]
 \begin{center}
 \begin{tabular}{lrrrl}
   \hline
 Author & $N$ & $p$ & $K$ & Disease \\
   \hline
 Burczynski et al. (2006) & 127 & 22283 & 3 & Crohn's Disease \\
   Nakayama et al. (2001) & 105 & 22283 & 5 & Sarcoma \\
   Shipp et al. (2002) & 58 & 6817 & 2 & Lymphoma \\
   Singh et al. (2002) & 102 & 12600 & 2 & Prostate Cancer \\
    \hline
 \end{tabular}
 \caption{Summary of high-dimensional microarray data sets.}
 \label{tab:data-summary}
 \end{center}
 \end{table}


\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC) through hybridization to microarrays from 127 individuals resulting in 22,283 sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD), and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide microarray from 105 samples of 10 types of soft tissue tumors. This included 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS), 15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21 samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl} determined from their data that these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following \cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at least 15 observations.

\subsection{\cite{Shipp:2002ka} Data Set}

Approximately 30-40\% of adult non-Hodgkin lymphomas are diffuse large B-cell lymphomas (DLBCLs) \cite{Shipp:2002ka}.  However, only a small proportion of DLBCL patients are cured with modern chemotherapeutic regimens.  Several models have been proposed, such as the International Prognostic Index (IPI), to determine a patient's curability.  These models rely on clinical covariates, such as age, to determine if the patient can be cured and are often ineffective. \cite{Shipp:2002ka} have argued that more effective means are desired to determine a patient's curability. The authors measured 6,817 gene expression levels from 58 DLBCL patient samples with customized cDNA (‘lymphochip’) microarrays to investigate the curability of patients treated with cyclophosphamide, adriamycin, vincristine, and prednisone (CHOP)-based chemotherapy.  Among the 58 DLBCL patient samples, 32 are from cured patients while 26 are from patients with fatal or refractory disease.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from surgery patients between 1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600 genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are of high quality: 52 prostate tumor samples and 50 non-tumor prostate samples.
 
\subsection{Classification Results}

For the microarray data sets, we computed error-rate estimates for the competing classifiers after reducing the original feature space dimension with variable selection to $d=100,250,500$, and $1000$.  Our choices for the values of $d$ are similar to those of \cite{Xu:2009fl}.  We report the error-rate estimates along with approximate standard errors in parentheses in Tables \ref{tab:results-d100} -- \ref{tab:results-d1000} along with the chosen reduced dimension $d$.

The \emph{GRDA} classifier was superior in classification performance for the majority of the simulations. While the error-rate estimates for the majority of the classifiers increased as $d$ was increased, the classification accuracy for the \emph{GRDA} classifier improved as $d$ increased for the Burczynski and Nakayama data sets. For the Nakayama data set, the \emph{GRDA} classifier was outperformed by the Dudoit, Pang, Tong, and Witten classifiers for small values of $d$ but attained classification accuracy comparable to its competitors for larger values of $d$. Moreover, the only situation where the other classifiers considered were competitive with the \emph{GRDA} classifier was in the case of the Nakayama data set.

The error-rate estimates for the Dudoit, Pang, Tong, and Witten classifiers were comparable regardless of the choice of $d$. In particular, the error-rate estimates for Dudoit's and Tong's methods were nearly identical, which suggests that the improved mean estimator employed by Tong provides little improvement to the classification performance in the \emph{DLDA} classifier. The popular classifier from Guo is competitive only for large values of $d$ for the Shipp and Singh data sets. For the Burczynski and Nakayama data sets, the implementation of Guo's method in the {\tt rda} package often classified all of the test observations into a single class, thus yielding larger error rates than its competitors.

\[ \left[\text{Table 2 goes about here }\right]\]

\[ \left[\text{Table 3 goes about here }\right]\]

\[ \left[\text{Table 4 goes about here }\right]\]

\[ \left[\text{Table 5 goes about here }\right]\]

\section{Discussion}

The analysis of high-dimensional microarray data sets is important to modern biomedical studies and presents challenges with supervised classification because the number of gene expressions greatly outnumbers the number of collected responses. Consequently, regularization techniques are often employed to stabilize popular classifiers for improved classification performance. We have provided a generalization of the well-known \emph{RDA} classifier from \cite{Friedman:1989tm}, aimed for small-sample, high-dimensional classification. The covariance-matrix regularization employed in the \emph{GRDA} classifier includes a large family of shrinkage methods. Our formulation of the \emph{GRDA} classifier in terms of the nonzero eigenvalues of the pooled sample covariance matrix indicates that eigenvalue-adjustment methods applied to the \emph{LDA} classifier explains the similiarities in classification accuracy of several regularization methods applied to the \emph{LDA} classifier reported by \cite{Ramey:2013ji} in a high-dimensional classification study.

We have provided an implementation of the \emph{GRDA} classifier in the {\tt regdiscrim} R package, available on CRAN. Our implementation facilitates a rapid classification of high-dimensional data by employing the novel results provided in this paper. Moreover, our 10-fold cross-validation procedure typically requires a couple of seconds for data sets having large $p$. When preliminary variable selection is applied to the data as we performed in our classification study, the time to perform model selection is nearly instantaneous, confirming the computational complexity reported regarding the operations employed in the \emph{GRDA} classifier.

	

\bibliographystyle{plainnat}
\bibliography{rda}

\clearpage

% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Mon Mar 25 07:04:20 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{lllllll}
  \hline
Dataset & Dudoit & GRDA & Guo & Pang & Tong & Witten \\ 
  \hline
Burczynski & 0.232 (0.019) & 0.179 (0.017) & 0.513 (0.022) & 0.232 (0.019) & 0.231 (0.019) & 0.232 (0.019) \\ 
  Nakayama & 0.262 (0.020) & 0.308 (0.021) & 0.586 (0.022) & 0.262 (0.020) & 0.262 (0.020) & 0.278 (0.020) \\ 
  Shipp & 0.136 (0.015) & 0.106 (0.014) & 0.310 (0.021) & 0.136 (0.015) & 0.136 (0.015) & 0.136 (0.015) \\ 
  Singh & 0.111 (0.014) & 0.097 (0.013) & 0.338 (0.021) & 0.111 (0.014) & 0.111 (0.014) & 0.111 (0.014) \\ 
   \hline
\end{tabular}
\caption{The average of the test error rates for the microarray data sets with $d = 100$. Approximate standard errors are given in parentheses.}
\label{tab:results-d100}
\end{center}
\end{table}

% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Mon Mar 25 07:04:37 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{lllllll}
  \hline
Dataset & Dudoit & GRDA & Guo & Pang & Tong & Witten \\ 
  \hline
Burczynski & 0.244 (0.019) & 0.163 (0.017) & 0.459 (0.022) & 0.244 (0.019) & 0.244 (0.019) & 0.243 (0.019) \\ 
  Nakayama & 0.244 (0.019) & 0.267 (0.020) & 0.498 (0.022) & 0.244 (0.019) & 0.244 (0.019) & 0.259 (0.020) \\ 
  Shipp & 0.153 (0.016) & 0.104 (0.014) & 0.188 (0.017) & 0.153 (0.016) & 0.153 (0.016) & 0.154 (0.016) \\ 
  Singh & 0.135 (0.015) & 0.117 (0.014) & 0.188 (0.017) & 0.135 (0.015) & 0.135 (0.015) & 0.135 (0.015) \\ 
   \hline
\end{tabular}
\caption{The average of the test error rates for the microarray data sets with $d = 250$. Approximate standard errors are given in parentheses.}
\label{tab:results-d250}
\end{center}
\end{table}

% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Mon Mar 25 07:04:48 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{lllllll}
  \hline
Dataset & Dudoit & GRDA & Guo & Pang & Tong & Witten \\ 
  \hline
Burczynski & 0.243 (0.019) & 0.159 (0.016) & 0.429 (0.022) & 0.243 (0.019) & 0.243 (0.019) & 0.243 (0.019) \\ 
  Nakayama & 0.247 (0.019) & 0.248 (0.019) & 0.465 (0.022) & 0.247 (0.019) & 0.248 (0.019) & 0.261 (0.020) \\ 
  Shipp & 0.169 (0.017) & 0.097 (0.013) & 0.119 (0.015) & 0.169 (0.017) & 0.169 (0.017) & 0.170 (0.017) \\ 
  Singh & 0.174 (0.017) & 0.125 (0.015) & 0.155 (0.016) & 0.174 (0.017) & 0.174 (0.017) & 0.174 (0.017) \\ 
   \hline
\end{tabular}
\caption{The average of the test error rates for the microarray data sets with $d = 500$. Approximate standard errors are given in parentheses.}
\label{tab:results-d500}
\end{center}
\end{table}

% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Mon Mar 25 07:05:01 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{lllllll}
  \hline
Dataset & Dudoit & GRDA & Guo & Pang & Tong & Witten \\ 
  \hline
Burczynski & 0.249 (0.019) & 0.168 (0.017) & 0.381 (0.022) & 0.249 (0.019) & 0.249 (0.019) & 0.249 (0.019) \\ 
  Nakayama & 0.237 (0.019) & 0.238 (0.019) & 0.460 (0.022) & 0.237 (0.019) & 0.237 (0.019) & 0.246 (0.019) \\ 
  Shipp & 0.196 (0.018) & 0.106 (0.014) & 0.095 (0.013) & 0.196 (0.018) & 0.195 (0.018) & 0.198 (0.018) \\ 
  Singh & 0.224 (0.019) & 0.140 (0.016) & 0.142 (0.016) & 0.224 (0.019) & 0.224 (0.019) & 0.224 (0.019) \\ 
   \hline
\end{tabular}
\caption{The average of the test error rates for the microarray data sets with $d = 1000$. Approximate standard errors are given in parentheses.}
\label{tab:results-d1000}
\end{center}
\end{table}


\end{document} 
