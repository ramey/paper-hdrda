\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Another Look at Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
\cite{Friedman:1989tm} proposed the popular regularized discriminant analysis (\emph{RDA}) classifier for the small-sample, high-dimensional setting by employing a regularized covariance-matrix estimator, which incorporates a ridge-like shrinkage factor by adding a positive constant to the zero-eigenvalue estimates to ensure positive definiteness. We show that this shrinkage is unnecessary because shrinking the zero eigenvalues corresponding to the null space of the pooled sample covariance matrix has no effect on the \emph{RDA} classifier. Accordingly, we propose a novel generalization of the \emph{RDA} classifier that is more computationally efficient than its original counterpart because the inverses and determinants computed in the decision rule can be reduced from the original feature space to a linear subspace with dimensions corresponding to the individual class sample sizes. Additionally, our formulation of the \emph{RDA} classifier allows us to estimate efficiently optimal tuning parameters from a large set of candidate values. Furthermore, our generalization simplifies the construction of the covariance-matrix estimator to improve its interpretation and allows for a more flexible classification model by viewing shrinkage applied to the nonzero eigenvalue estimates in terms of matrix functions. Fortuitously, this matrix-function formulation includes as special cases the original \emph{RDA} classifier and several other well-known regularized classifiers. Moreover, we demonstrate with several high-dimensional microarray data sets that our proposed classifier is competitive and often superior to several recently proposed sparse and regularized classifiers. Finally, we provide implementations of the competing classifiers in the {\tt regdiscrim} R package, available on CRAN.
\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of thousands of genes in a single experiment to identify a subset of genes and their expression patterns that either characterize a particular cell state or facilitate an accurate prediction of future cell state.  We focus on the classification of tissues and other biological samples into one of $K$ disjoint, predefined cancer types, where our goal is to ascertain the class membership of a new tissue sample based on gene-expression microarray data. However, microarray data typically consist of a large number of gene expressions $p$ relative to the sample size $N$. Moreover, standard benchmark microarray data sets usually consist of between $p = 10,000$ and $55,000$ probes and fewer than $N = 100$ observations, in which case standard supervised-learning methods exhibit poor classification performance, due to the \emph{curse of dimensionality} \citep{Bellman:1961tn}. Furthermore, well-known classifiers, including linear discriminant analysis \emph{(LDA)} and quadratic discriminant analysis \emph{(QDA)}, are incalculable when $p > N$ because the sample covariance matrices employed in both classifiers are singular. As a result, many researchers, including \cite*{Merchante:2012vk}, \cite{Witten:2011kc}, \cite*{Pang:2009ik}, \cite*{Huang:2010ju}, and \cite*{Clemmensen:2011kr}, have introduced restrictions on the population covariance matrices in the \emph{LDA} and \emph{QDA} models to reduce number of covariance parameters that must be estimated to ensure that the resulting classifiers are calculable.  For instance, \cite{Dudoit:2002ev} have proposed the diagonal \emph{LDA} (\emph{DLDA}) classifier, a modification of the \emph{LDA} classifier where the class covariance matrices are assumed to be diagonal.

Alternatively, several researchers, including \cite*{Zhang:2010va}, \cite{Ye:2009gd}, \cite{Ji:2008wp}, \cite*{Guo:2007te}, \cite{Srivastava:2007ud}, \cite{Ye:2006vx}, \cite*{Thomaz:2006ef}, \cite{Ye:2005uu}, \cite{Mkhadri:1995jp}, and \cite{Peck:1982tt}, have proposed regularization methods to improve the estimation of the population covariance matrices before applying the \emph{LDA} and \emph{QDA} classifiers. Covariance-matrix regularization methods often are based on the well-known ridge-regression approach from \cite{Hoerl:1970cd} and adjust the eigenvalues of the sample covariance matrices to ensure that the estimator is positive definite. Typically, a positive shrinkage value is added to each eigenvalue estimate, replacing the zero eigenvalues with the shrinkage constant and stabilizing the eigenvalues near zero to ensure that the inverse of the the sample covariance matrix is well-posed. This common form of covariance-matrix regularization is equivalent to summing the original covariance-matrix estimator with the identity matrix scaled by the shrinkage constant. For an excellent overview of the regularization problem in discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we consider specifically the popular regularized discriminant analysis (\emph{RDA}) classifier from \cite{Friedman:1989tm}, who has proposed a weighted average of the covariance-matrix estimators from the \emph{LDA} and \emph{QDA} classifiers. Because the resulting biased covariance-matrix estimator can remain singular or nearly singular, \cite{Friedman:1989tm} has additionally incorporated applied covariance-matrix shrinkage, where the shrinkage constant is a function of the average eigenvalue of the original estimator. \cite{Friedman:1989tm} has shown that the \emph{RDA} classifier can attain improved classification accuracy, compared to the \emph{LDA} and \emph{QDA} classifiers when $p > N$. However, in its original form, the \emph{RDA} classifier is impractical when $p \gg N$ because the \emph{RDA} decision rule requires the calculation of the inverses and determinants of the $p \times p$ covariance-matrix estimators for each class. Furthermore, the \emph{RDA} classifier has two tuning parameters that are selected via cross-validation, which adds a substantial computational burden to the model-selection process. Despite the introduction of a computational shortcut by \cite{Friedman:1989tm} for model selection to alleviate some of the computation, the \emph{RDA} classifier is ill-advised on modern data sets because the model selection requires on the order of $Kp^3$ calculations for each tuning-parameter pair for each fold in the cross-validation scheme.

In this paper we address the computational shortcomings of the \emph{RDA} classifier. Applying a technique similar to \cite{Ye:2006tq}, we decompose the \emph{RDA} classifier and show that the sum related to the null space of the pooled sample covariance matrix can be discarded. In turn, this omission provides a substantial increase in the computational efficiency of the \emph{RDA} classifier and its model-selection. In fact, we show rigorously that the inverse and determinants can be reduced to calculations of $n_k \times n_k$ matrices, where $n_k$ is the sample size of the $k$th class, $k = 1, \ldots, K$. This reduction in calculation is clearly advantageous because $n_k$ is often quite small, say, 15--20, in microarray data sets. Thus, we demonstrate that the calculation of the \emph{RDA} decision rule involving inverses and determinants of extremely large covariance matrices is equivalent a decision rule involving inverses and determinants on much smaller matrices.

Our improvements to the calculation and model selection of the \emph{RDA} classifier yields a natural generalization of the \emph{RDA} classifier. In our proposed generalization we first simplify the pooling matrix in the \emph{RDA} classifier by constructing instead a convex combination of the covariance-matrix estimators employed in the \emph{LDA} and \emph{QDA} classifiers. This simplification is advantageous in that it improves the interpretation of the pooling operation of the \emph{RDA} classifier. Next, we consider covariance-matrix regularization in terms of matrix functions, which includes a large family of shrinkage methods, such as the methods from \cite{Srivastava:2007ww}, \cite{Rao:1971ul}, and a wide variety of other shrinkage methods studied by \cite{Ramey:2011ji} and \cite*{Xu:2009fl}. While a large number of these methods can be viewed as variants of ridge regression, we consider the more general case, where a real monotonic function is applied to the eigenvalue estimates. We show that adjusting the zero eigenvalues with a general matrix function has no effect on the \emph{RDA} classifier. That is, we show that the \emph{RDA} classifier is invariant to any adjustments of the eigenvalues corresponding to the null space of the pooled sample covariance matrix. Moreover, there is no classificatory information available in the null space of the pooled sample covariance matrix.  Hence, no gain in classification performance can be obtained by adjusting these zero eigenvalues. Although \cite{Ji:2008wp} and \cite{Ye:2006jm} have provided a similar invariance property for the \emph{LDA} decision rule, the \emph{LDA} classifier involves the assumption that the population covariance matrices are equal, which is often too stringent \citep{Clemmensen:2011kr}. The \emph{RDA} classifier allows us to relax this assumption by including unrestricted covariance matrices in the model. The \emph{RDA} classifier is then a compromise between the two approaches, favoring the stronger linear assumption if a larger value of the pooling parameter is selected as optimal. Additionally, in the linear case  \cite{Ye:2009gd} have remarked that  a lack of a systematic study has been conduced to explore the relationships among various shrinkage methods. Our formulation provides a general framework from which we can elucidate properties of covariance-regularization methods and the relationships among them in the linear case, the quadratic case, and the cases in between.

In addition to the improvement in computational performance of classifiers with regularized covariance matrices, we demonstrate that our generalized \emph{RDA} classifier is competitive and often superior to several recent classifiers designed for small-sample, high-dimensional data. In particular, we compare our generalized RDA classifier with an alternatively-defined \emph{RDA} classifier from \cite*{Hastie:2008dt}, the regularized \emph{LDA} classifier from \cite{Guo:2007te}, a recently proposed variant of the \emph{DLDA} classifier from \cite*{Tong:2012hw}, and the penalized \emph{LDA} classifier from \cite{Witten:2011kc}. We conduct our comparison of the classification accuracy of the competing classifiers using three small-sample, high-dimensional microarray data sets and examine the classification accuracy of the competing classifiers using \textbf{three} simulation configurations.

We have organized the remainder of the paper as follows. In Section 2 we present our generalization of the \emph{RDA} classifier after reviewing its original counterpart from \cite{Friedman:1989tm}. In Section 3 we show that our proposed classifier includes several well-known regularized classifiers as special cases and briefly discuss some commonalities among them. In Section 4 we describe the \textbf{three} simulation configurations and three small-sample, high-dimensional microarray data sets and then demonstrate that our two proposed classifiers are superior to the competing classifiers in terms of classification accuracy. We then conclude with a brief discussion in Section 5.

\section{Regularized Discriminant Analysis}
\label{sec:rda}

In discriminant analysis we wish to assign correctly an unlabeled $p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes (or populations) by constructing a classifier from $n$ training observations that can accurately predict the class membership of $\bm x$. Let $\bm x_i = (x_{i1}, x_{i2}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots, n)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{a \times b}$ denotes the matrix space of all $a \times b$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of class membership of the $k$th class.

The \emph{QDA} classifier is the optimal Bayesian decision rule with respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate normal distribution with known mean vector  $\bm\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite matrices. Because the parameters are typically unknown, we estimate the unknown parameters $\bm \mu_k$ and $\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the MLEs into the \emph{QDA} classifier. Assuming that the prior probabilities of class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an unlabeled observation $\bm x$ to class $\omega_k$ with the sample \emph{QDA} classifier
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further that the population covariance matrices are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$ for all $k$), then the pooled sample covariance matrix $\widehat{\bm\Sigma}$ is substituted for $\widehat{\bm \Sigma}_k$ in \eqref{eq:qda}, where
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{n} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}
is the MLE for $\bm \Sigma$. In this case \eqref{eq:qda} reduces to Fisher's \emph{LDA} classifier, and the log-determinant is omitted because it is constant across all classes.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{\bm \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest eigenvalues are underestimated \citep{Seber:2004uh}, Moreover, if $p > n_k$, then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p - n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are 0. Hence, the bias of the eigenvalue estimators increases as $p$ increases relative to $n_k$. Furthermore, although more feature information is available to discriminate among the $K$ classes, if $p > n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does not exist.

Several regularization methods, such as the methods considered by \citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$ so that \eqref{eq:qda} is calculable and has reduced variability in the standard bias-variance tradeoff. These methods often can be written concisely as a matrix function, which we define here using the notation of \cite{Izenman:2008gm}. Let $\bm A \in \mathbb{R}_{p \times p}$ be symmetric, and let $\phi:\mathbb{R} \rightarrow \mathbb{R}$ be monotonic. Then
\begin{align}
	\phi(\bm A) = \sum_{j = 1}^p \phi(e_j) \bm v_j \bm v_j'\label{eq:spectral-decomp}
\end{align}
is a matrix function defined explicitly in terms of the eigenvalues of $\bm A$. For this reason, $\phi$ is often called a spectral transform \citep*{Zhu:2010vu} or a transfer function \citep{Ye:2009gd}. Special cases of \eqref{eq:spectral-decomp} include the spectral decomposition of $\bm A$ when $\phi(e_j) = e_j$, the matrix square-root of $\bm A$ when $\phi(e_j) = e_j^{1/2}$, and, if it exists, the inverse of $\bm A$ when $\phi(e_j) = e_j^{-1}$ \citep{Harville:2008wja}.

Notice that if  $\phi(e_{jk}) = e_{jk}$, $k = 1, \ldots, K$, no eigenvalue adjustment is performed, and \eqref{eq:qda} remains incalculable. Similarly, if $\phi(e_{jk}) = e_{j}$, $k = 1, \ldots, K$, the \emph{LDA} classifier is employed without eigenvalue shrinkage. Additionally, setting $\phi(e_j) = e_j^{+}$ with $z^{+} = 1/z$ if $z > 0$ and $0$ otherwise, we ignore the zero eigenvalues, so that $\phi(\widehat{\bm \Sigma}_k) = \widehat{\bm \Sigma}_k^{+}$, the Moore-Penrose pseudoinverse of $\widehat{\bm \Sigma}_k$.

Another common form of the covariance-matrix regularization applies a shrinkage factor $\gamma_k > 0$, so that
\begin{align}
	\phi(e_{jk}) = e_{jk} + \gamma_k, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression. Equation \eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance matrix $\widehat{\bm\Sigma}_k$ towards the $p$-dimensional identity matrix $\bm I_p$, thereby increasing the eigenvalues of $\widehat{\bm\Sigma}_k$ by $\gamma_k$. Specifically, the zero eigenvalues are replaced with $\gamma_k$, resulting in $\widehat{\bm \Sigma}_k + \gamma_k \bm I_p$ being positive definite. For additional covariance-matrix regularization methods, see \cite{Ramey:2011ji}, \cite{Xu:2009fl}, and \cite{Ye:2009gd}.

\subsection{Original Definition of Regularized Discriminant Analysis}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$ to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda n$, $\bm S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret \eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in \eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces to \eqref{eq:pooled-cov} used in the \emph{LDA} classifier.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased \emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter that controls the shrinkage of \eqref{eq:sig-rda}  towards the $p$-dimensional identity matrix $\bm I_p$, which is weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from $\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the amount of shrinkage applied.

\cite{Friedman:1989tm} substituted \eqref{eq:sig-rda} into \eqref{eq:qda} to obtain the \emph{RDA} classifier
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}
Notice that we can write the shrinkage term in \eqref{eq:sig-rda} as the matrix function
\begin{align*}
	\phi(e_{jk}(\lambda)) = (1 - \gamma) e_{jk}(\lambda) + \gamma \bar{e}_k,
\end{align*}
for class $\omega_k$, where $e_{jk}(\lambda)$ is the $j$th largest eigenvalue of $\widehat{\bm\Sigma}_k(\lambda)$ and $\bar{e}_k = p^{-1} \sum_{j=1}^p e_{jk}(\lambda)$ is the average of the eigenvalues of \eqref{eq:sig-lambda}.

\subsection{A Generalization of Regularized Discriminant Analysis}

Here, we define our generalization of the \emph{RDA} classifier by first simplifying the formulation of $\widehat{\bm \Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda} and demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations centered by their respective class sample means. Then, we generalize the adjustment of the eigenvalues of $\widehat{\bm \Sigma}_k(\lambda)$ by introducing a more general matrix-function formulation.  We define
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we no longer have need for the convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By rewriting \eqref{eq:sig-lambda-alternative} in terms of the observations $\bm x_i$, $i = 1, \ldots, N$, each centered by its class sample mean, we attain a clear interpretation of $\widehat{\bm\Sigma}_k(\lambda)$. We have that
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
%	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'} \nonumber \\
%	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of each of the $N$ observations to estimating $\bm \Sigma_k$. That is, for $0 < \lambda \le 1$, we estimate $\bm \Sigma_k$ with $N$ rather than $n_k$ observations. Below, we will show that the pooling operation is advantageous in increasing the rank of each $\widehat{\bm\Sigma}_k(\lambda)$ from rank$(\widehat{\bm\Sigma}_k)$ to rank$(\widehat{\bm\Sigma}_k)$ for $0 < \lambda \le 1$. Notice that if $\lambda  = 0$, then the observations from the remaining $K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to $\widehat{\bm\Sigma}$.

As we have discussed, several eigenvalue adjustment methods have been proposed to increase eigenvalues (approximately) equal to 0. In this section we define a more general eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} by applying the matrix function $\phi_k(\widehat{\bm \Sigma}_k(\lambda))$. To simplify the notation, we write $\tilde{\bm \Sigma}_k = \phi_k(\widehat{\bm \Sigma}_k(\lambda))$. Thus, we have the generalized \emph{RDA} (\emph{GRDA}) classifier
\begin{align}
	D_{GRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k|, \label{eq:grda}
\end{align}
The specification of $\phi_k$ in \eqref{eq:grda} allows for a large family of eigenvalue shrinkage methods to be examined and for their relationships to be examined in depth.

\section{The Generalized RDA Classifier}
In this section, we demonstrate that our proposed generalization of the \emph{RDA} classifier facilitates an efficient calculation of \eqref{eq:grda} and a rapid model selection of the pooling parameter $\lambda$. Our approach is to decompose \eqref{eq:grda} into a sum of two components, where the first summand consists of matrix operations applied to low-dimensional matrices and the second summand corresponds to the null space of $\widehat{\bm \Sigma}$ in \eqref{eq:pooled-cov}. We show that the matrix operations performed on the null space of $\widehat{\bm \Sigma}$ yield constant quadratic forms across all classes and can be omitted. For $p \gg N$, the constant component involves determinants and inverses of high-dimensional matrices, and by ignoring these calculations, a substantial reduction in computational costs is achieved relative to the \emph{RDA} classifier originally proposed by \cite{Friedman:1989tm}. Furthermore, a byproduct of this omission is that adjustments to the associated eigenvalues have no effect on \eqref{eq:grda}. Lastly, we exploit the so-called Fast singular value decomposition (SVD) to construct the eigenvalue decomposition of $\widehat{\bm \Sigma}$, further reducing the computational costs of our proposed \emph{RDA} classifier.

First, we require the following relationship regarding the null spaces of $\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm \Sigma}_k$.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ be the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$ as discussed in Section \ref{sec:rda}, and let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $k = 1, \ldots, K$, $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k(\lambda))$ for some $k = 1, \ldots, K$. Hence, $0 = \bm z' \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 - \lambda) \bm z' \widehat{\bm \Sigma}_k \bm z + \lambda \bm z' \widehat{\bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k, \widehat{\bm \Sigma}\in \mathbb{R}_{p \times p}^{\ge}$, we have $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. In particular, we have that $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma})$. Now, suppose $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$. Similarly, we have that $0 = \bm z' \widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z' \widehat{\bm \Sigma}_k \bm z$, which implies that $z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$ because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times p}^{\ge}$ for $k = 1, \ldots, K$. Therefore, $\mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$.
\end{proof}

In Lemma \ref{lemma:rda-tilde-Sigma_k} below, we derive an alternative expression for $\tilde{\bm \Sigma}_k$ in terms of the eigenvectors of $\widehat{\bm \Sigma}$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p \times p}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$ with $\bm D = \bm D_q \oplus \bm 0_{p-q}$, the columns of $\bm U \in \mathbb{R}_{p \times p}$ are the corresponding eigenvectors of $\widehat{\bm \Sigma}$, and rank$(\widehat{\bm \Sigma}) = q$, where $\bm A \oplus \bm B$ denotes the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}. Then, partitioning $\bm U = (\bm U_1, \bm U_2)$ such that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in \mathbb{R}_{p \times (p - q)}$, we see that the component of $\tilde{\bm \Sigma}_k$ corresponding to the null space of $\widehat{\bm \Sigma}$ is in terms of the matrix function $\phi_k$ alone.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ as above, and suppose that rank$(\widehat{\bm \Sigma}) = q \le p$. Then, we have
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U(\bm W_k \oplus \gamma \bm I_{p - q})\bm U',\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= (1-\gamma) \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm I_q.\label{eq:Wk}
\end{align}
\end{lemma}
\begin{proof}
First, notice that
\begin{align*}
	\bm U' \tilde{\bm \Sigma}_k \bm U = (1-\gamma) \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm U' \bm U.
\end{align*}
From Lemma \ref{lemma:null-spaces}, we have that the columns of $\bm U_2$ span the null space of $\widehat{\bm \Sigma}$, which implies that $\widehat{\bm \Sigma}_k \bm U_2 = \bm 0$. Hence, for $k = 1, \ldots, K$, $\bm U' \widehat{\bm \Sigma}_k \bm U = \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 \oplus \bm 0_{p-q}$. Thus, \eqref{eq:rda-matrix} follows because $\bm U$ is orthogonal.
\end{proof}

An immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k} is rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$, $k = 1, \ldots, K$ if the pooling parameter $\lambda \ne 0$. Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we increase the rank of $\widehat{\bm \Sigma}_k(\lambda)$ and, therefore, $\tilde{\bm \Sigma}_k$. We state this formally here.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$, rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$ for all $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}

Also from Lemma 2, we see that $\tilde{\bm \Sigma}_k$ has a similar form, even if the matrix function $\phi_k$ is unique.  Next, we provide an essential result that will allow us to prove that for any unlabeled observation $\bm x \in \mathbb{R}_{p \times 1}$ that \eqref{eq:grda} is invariant to adjustments to the eigenvalues of $\tilde{\bm \Sigma}_k$ corresponding to the null space of $\widehat{\bm \Sigma}$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times 1}$, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$, $1 \le k, k' \le K$.
\end{lemma}
\begin{proof}
Let $\bm x \in \mathbb{R}_{p \times 1}$, and suppose that $1 \le k, k' \le K$. Then, $\xbar_k - \xbar_{k'} = \sum_{i=1}^N \alpha_i \bm x_i$, where $\alpha_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i = k') n_k \}$.  Then, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$ is equivalent to $\bm U_2'(\xbar_k - \xbar_{k'}) = \sum_{i=1}^N \alpha_i \bm U_2' \bm x_i$. Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that $\bm U_2' \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma 1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{\bm \Sigma})$, for $i = 1, \ldots, N$, we have that $\bm U_2' \bm x_i = \bm 0$, which implies that $\bm U_2'(\xbar_k - \xbar_{k'}) = \bm 0$. Therefore, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$.
\end{proof}

Lemma 2 is essential to our main result below because we decompose \eqref{eq:grda} into two components, one of which involves the constant term given in Lemma 3. The remaining components involve $\bm U_1$, a much smaller matrix than $\bm U_2$. Now, we present our main result, where we partition our generalized \emph{RDA} decision rule and show that the component requiring the largest computational costs does not contribute to the classification performed. Hence, we are able to omit the term and apply the reduced decision rule as follows.

\begin{thm}
Let $\tilde{\bm \Sigma}_k$ be defined as in \eqref{eq:rda-matrix} and $\bm W_k$ be defined as in \eqref{eq:Wk}. Also, let $\bm U_1$ be defined as above. Then, the generalized \emph{RDA} decision rule in \eqref{eq:grda} is equivalent to the reduced form
	\begin{align}
		D_{RDA}(\bm x) &= \argmin_k  (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)' + \ln | \bm W_k | + (p - q) \ln \gamma. \label{eq:rda-decomposed}
	\end{align}
	
\end{thm}
\begin{proof}
	From \eqref{eq:rda-matrix}, we have that $\tilde{\bm \Sigma}_k^{-1} = \bm U(\bm W_k^{-1} \oplus\gamma^{-1} \bm I_{p - q})\bm U'$ and $|\tilde{\bm \Sigma}_k| = \gamma^{p-q} | \bm W_k |$. Therefore, for all $\bm x \in \mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)' \tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k| &= (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)'\\
	&+ \gamma^{-1} (\bm x - \xbar_k)' \bm U_2 \bm U_2' (\bm x - \xbar_k) + \ln | \bm W_k | + (p - q) \ln \gamma.
	\end{align*}
From Lemma \ref{lemma:RDA-constant-term}, $\bm U_2' (\bm x - \xbar_k)$ is constant for all $k = 1, \ldots, K$, and the proof follows.
\end{proof}

\subsection{Model Selection}

Thus far we have seen that the generalized \emph{RDA} classifier in \eqref{eq:rda-decomposed} is invariant to the $\bm U_2$ term, yielding a substantial reduction in the computational costs. While the classifier is much improved in terms of computational performance compared to the \emph{RDA} classifier originally proposed by \cite{Friedman:1989tm}, we provide two additional results that further improve the computational performance of our generalized \emph{RDA} classifier. First, we show that the inverse and determinants of $\bm W_k$ can be written so that they are performed on $n_k \times n_k$ matrices rather than $q \times q$ matrices. Given that $n_k$ is often quite small, say, between 15 and 50, the following result facilitates a further reduction in the computational costs.

\begin{proposition}\label{proposition:rda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - (1 - \gamma)(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},
	\end{align}
where $\bm Q_k = \bm I_{n_k} + (1 - \gamma)(1- \lambda) \bm X_k \bm U_1 \bm \Gamma_k^{-1} \bm U_1' \bm X_k'$ and $\bm \Gamma_k = (1 - \gamma) \lambda \bm D_q + \gamma \bm I_q$.
\end{proposition}
\begin{proof}
	Set $\bm A = \bm \Gamma_k$, $\bm B = (1 - \gamma)(1 - \lambda) \bm U_1' \bm X_k'$, $\bm T = \bm I_{n_k}$, and $\bm C = \bm X_k \bm U_1$. To calculate $|\bm W_k|$, from \cite{Harville:2008wja} we apply Theorem 18.1.1, which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} + \bm C \bm A^{-1} \bm B|$. Thus, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also from Theorem 18.2.8 in \cite{Harville:2008wja}, we apply the Woodbury formula $(\bm A + \bm B \bm C)^{-1} = \bm A^{-1} - \bm A^{-1} \bm B(\bm I + \bm C \bm A^{-1} \bm B)^{-1} \bm C \bm A^{-1}$, which implies that $\bm W_k^{-1} = \bm \Gamma_k^{-1} - (1 - \gamma)(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1}$.
\end{proof}

Thus far, we have construction each $\tilde{\bm \Sigma}_k$ in terms of the first $q$ eigenvectors of $\widehat{\bm \Sigma}$. In practice, we would usually require a costly eigendecomposition of $\widehat{\bm \Sigma}$, but by utilizing a technique similar to the so-called kernel trick, we can obtain $\bm U_1$ by computing the eigendecomposition of a much smaller $N \times N$ matrix when $p \gg N$ \citep{Hastie:2008dt}. We begin our derivation by writing the pooled sample covariance matrix in \eqref{eq:pooled-cov} as $\widehat{\bm\Sigma} = \bm X' \bm X$, where we define $\bm X = [\sqrt{c_{1k}(1)} \bm x_1', \ldots,  \sqrt{c_{Nk}(1)} \bm x_N']'$. Then, by writing the SVD of $\bm X = \bm M \bm D \bm U'$, we have the spectral decomposition $\widehat{\bm\Sigma} = \bm U \bm D^2 \bm U'$, where $\bm M \in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D \in \mathbb{R}_{N \times N}$ is a diagonal matrix consisting of the singular values of $\bm X$, and $\bm U \in \mathbb{R}_{p \times N}$ is orthogonal. Now, we see that $\bm X \bm X' = \bm M \bm D^2 \bm M'$ is the eigenvalue decomposition of $\bm X \bm X'$, where $\bm D^2$ is the matrix of eigenvalues of $\bm X \bm X'$ and the columns of $\bm M$  are the corresponding eigenvectors. Hence, after a much faster matrix multiplication, we can obtain $\bm M$ and $\bm D$ from a quick eigendecomposition of $\bm X \bm X' \in \mathbb{R}_{N \times N}$. Next, we compute $\bm U = \bm X' \bm M \bm D^{+}$, where $\bm D^{+} = \bm D_q^{-1} \oplus \bm 0_{N-q}$ is the Moore-Penrose pseudoinverse of $\bm D$. We can determine $q$ efficiently by computing the number of nonzero eigenvalues present in $\bm D^2$. However, in our experience, we must take care because the $N-q$ smallest eigenvalues are numerically nonzero, albeit they are near zero. Thus, we select $q$ to be the number of eigenvalues that exceeds some given tolerance value, say, $1 \times 10^{-6}$. After calculating $q$, we need only retain the first $q$ columns of $\bm U$ to extract $\bm U_1$.

\section{Monte Carlo Simulations}

Our generalization of the \emph{RDA} classifier facilitates a large number of potential models based on the specification of the matrix functions to shrink the eigenvalues. In our simulations below, we consider two such eigenvalue value functions. First, ...  Second, we consider the matrix function from \cite{Srivastava:2007ww}, who derive an empirical-Bayes covariance-matrix estimator under the assumption that the data are independent and identically distributed multivariate normal observations. \cite{Srivastava:2007ww} then substitute the estimator into the \eqref{eq:lda} to attain the minimum distance empirical Bayes (\emph{MDEB}) classifier. To derive the empirical-Bayes covariance-matrix estimator, \cite{Srivastava:2007ww} first assume that $\bm \Sigma_k^{-1}$ follows a Wishart distribution \emph{a priori} with mean $\lambda^{-1} \bm I_p$, $\lambda > 0$, and degrees of freedom, $l \ge p$. The resulting posterior estimator for $\bm \Sigma_k^{-1}$ is $\frac{n_k + l}{n_k}(n_k^{-1}\lambda \bm I_p + \widehat{\bm \Sigma}_k)^{-1}$. Using an empirical Bayes argument to estimate $\lambda$ for $p > n_k$, \cite{Srivastava:2007ww} propose an empirical-Bayes covariance-matrix estimator with matrix function
\begin{align}
	\phi(e_{jk}) = e_{jk} + n_k^{-1} \sum_{j = 1}^p e_{jk},\label{eq:cov-mdeb}
\end{align}
where the shrinkage factor in \eqref{eq:cov-mdeb} is approximately equal to the average of the nonzero eigenvalues of $\widehat{\bm \Sigma}_k$. We note that \cite{Srivastava:2007ww} set assumed that $\bm \Sigma_k = \bm \Sigma$ so that $\phi(e_{jk}) = \phi(e_k)$, $k = 1, \ldots, K$.

\cite{Srivastava:2007ww} claim that the \emph{MDEB} classifier is the best regularization method they have encountered in the literature. When $p \le n$, the shrinkage factors of the \emph{RDA} and \emph{MDEB} classifiers are equal, but for $p > n$ this shrinkage can differ substantially. In fact, \cite{Ramey:2011ji} show empirically that the \emph{MDEB} classifier can yield superior classification accuracy to the \emph{RDA} classifier.

\section{High-Dimensional Microarray Data Sets}

In this section, we describe three high-dimensional microarray data sets and compare our proposed \emph{GRDA} classifier with two recently proposed classifiers for small-sample, high-dimensional data: the penalized \emph{LDA} (\emph{PLDA}) classifier from \cite{Witten:2011kc}, the shrinkage-mean-based DLDA (\emph{SmDLDA}) classifier from \cite{Tong:2012hw}, the \emph{RDA} classifier as defined in \cite{Hastie:2008dt}, and the sparse discriminant analysis (\emph{SDA}) classifier from \cite{Clemmensen:2011kr}. We calculated 10-fold cross-validation error rates \citep{Hastie:2008dt} to evaluate the performance of each classifier considered. Because the \emph{PLDA} classifier has a built-in variable selection method, for fair comparison we compared each of the classifiers using the variables selected from each cross-validation fold.

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC) through hybridization to microarrays from 127 individuals resulting in 22,283 sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD), and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide microarray from 105 samples of 10 types of soft tissue tumors. This included 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS), 15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21 samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl} determined from their data that these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following \cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at least 15 samples observed.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from surgery patients between 1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600 genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are of high quality: 52 prostate tumour samples and 50 non-tumour prostate samples.

\subsection{Classification Results}


\section{Discussion}

Question to answer: What do we do when $\lambda = 0$, corresponding to \emph{QDA}?

It is interesting to note that our approach is similar to an aside made by Friedman (1989) in the case that the RDA covariance matrix estimator is singular. Friedman (1989) recommended that the zero eigenvalues be replaced with a small value such as 0.001. However, in Theorem 1, we saw that even small adjustments can be inappropriate because they result in ineffective shrinkage.

\section{Next Actions}

\begin{itemize}
\item Add paragraph of other notation, including null space, orthogonal complement, and direct sum.
\item Generalize the shrinkage. Define the diagonal matrix of adjusted eigenvalues as $\bm \Phi$ and partition it into two parts: $\bm \Phi_1$ corresponding to the first $q$ eigenvalues of $\widehat{\bm \Sigma}$, and $\bm \Phi_2$ corresponding to the remaining $p - q$ eigenvalues of $\widehat{\bm \Sigma}$.
\item Look at Ye's (2005) JMLR paper on his choice of the $\bm M$ matrix. He has a nice way of saying that he considers special cases
\item Implement classifier
\item Run simulations
\item Add simulation results
\item Discussion simulation results
\item Use the Sherman-Morrison formula for cross-validation
\item Discuss choice of $\lambda$ and eigenvalue shrinkage, if necessary
\item Question to answer: What do we do when $\lambda = 0$, corresponding to \emph{QDA}?
\item Show a timing comparison between our implementation and that of the {\tt klaR} package.
\end{itemize}

	

\bibliographystyle{plainnat}
\bibliography{rda}


\end{document} 
