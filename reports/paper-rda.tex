\documentclass[11pt]{article}
\usepackage{graphicx, amsmath, amssymb, bm, url, mathtools, natbib, amsthm, setspace}

\pagestyle{plain}
%----------------Page dimensions ----------------
\oddsidemargin 0.0in
\evensidemargin 0.0in
\topmargin -0.75in
\leftmargin 0in
\headheight 0.0in
\headsep 0.5in
%\footheight 0.0in
\footskip 0.5in
\footnotesep 0.0in
\textwidth 6.7in
\textheight 9.5in
%-----------Define Pictures---------------------
\def\picture #1 by #2 (#3){
 \vbox to #2{
   \hrule width #1 height 0pt depth 0pt
   \vfill
   \special{picture #3} % this is the low-level interface
   }
 }
\def\scaledpicture #1 by #2 (#3 scaled #4){{
 \dimen0=#1 \dimen1=#2
 \divide\dimen0 by 1000 \multiply\dimen0 by #4
 \divide\dimen1 by 1000 \multiply\dimen1 by #4
 \picture \dimen0 by \dimen1 (#3 scaled #4)}
 }

\newcommand{\xbar}{\bar{\bm x}}
\newcommand{\tr}{\text{tr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Another Look at Regularized Discriminant Analysis for High-Dimensional Classification}

\author{John A. Ramey, Caleb K. Stein, and Dean M. Young}

\begin{document}

\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}

\bibpunct{(}{)}{;}{a}{,}{,}

\doublespacing

\maketitle

\begin{abstract}
\cite{Friedman:1989tm} proposed the popular regularized discriminant analysis (\emph{RDA}) classifier for the small-sample, high-dimensional setting by employing a regularized covariance-matrix estimator, which incorporates a ridge-like shrinkage factor by adding a positive constant to the zero-eigenvalue estimates to ensure positive definiteness. We show that this shrinkage is unnecessary because shrinking the zero eigenvalues corresponding to the null space of the pooled sample covariance matrix has no effect on the \emph{RDA} classifier. Accordingly, we propose a novel generalization of the \emph{RDA} classifier that is more computationally efficient than its original counterpart because the inverses and determinants computed in the decision rule can be reduced from the original feature space to a linear subspace with dimensions corresponding to the individual class sample sizes. Additionally, our formulation of the \emph{RDA} classifier allows us to estimate efficiently optimal tuning parameters from a large set of candidate values. Furthermore, our generalization simplifies the construction of the covariance-matrix estimator to improve its interpretation and allows for a more flexible classification model by viewing shrinkage applied to the nonzero eigenvalue estimates in terms of matrix functions. Fortuitously, this matrix-function formulation includes as special cases the original \emph{RDA} classifier and several other well-known regularized classifiers. Moreover, we demonstrate with several high-dimensional microarray data sets that our proposed classifier is competitive and often superior to several recently proposed sparse and regularized classifiers. Finally, we provide implementations of the competing classifiers in the {\tt regdiscrim} R package, available on CRAN.
\end{abstract}

\section{Introduction}

Microarrays have enabled researchers to measure gene-expression levels of thousands of genes in a single experiment to identify a subset of genes and their expression patterns that either characterize a particular cell state or facilitate an accurate prediction of future cell state.  We focus on the classification of tissues and other biological samples into one of $K$ disjoint, predefined cancer types, where our goal is to determine accurately the class membership of a new tissue sample based on gene-expression microarray data. However, microarray data typically consist of a large number of gene expressions $p$ relative to the sample size $n$. Moreover, standard benchmark microarray data sets consist of between $p = 10,000$ and $55,000$ probes and fewer than $n = 100$ observations, in which case standard supervised-learning methods exhibit poor classification performance, due to the \emph{curse of dimensionality} \citep{Bellman:1961tn}. Furthermore, well-known classifiers, including linear discriminant analysis \emph{(LDA)} and quadratic discriminant analysis \emph{(QDA)}, are incalculable when $p > n$ because the sample covariance matrices employed in both classifiers are singular. As a result, many researchers, including \cite*{Merchante:2012vk}, \cite{Witten:2011kc}, \cite*{Pang:2009ik}, \cite*{Huang:2010ju}, and \cite*{Clemmensen:2011kr}, have restricted the \emph{LDA} and \emph{QDA} models to reduce number of covariance parameters that must be estimated to ensure that the resulting classifiers are calculable.  For instance, \cite{Dudoit:2002ev} have proposed a modified \emph{LDA} classifier where the class covariance matrices are assumed to be diagonal.

Alternatively, several researchers, including \cite*{Zhang:2010va}, \cite*{Guo:2007te}, \cite{Srivastava:2007ud}, and \cite{Mkhadri:1995jp}, have proposed regularization methods to improve the estimation of the population covariance matrices before applying the \emph{LDA} and \emph{QDA} classifiers. Covariance-matrix regularization methods often are based on the well-known ridge-regression approach from \cite{Hoerl:1970cd} and adjust the eigenvalues of the sample covariance matrices to ensure that the estimator is positive definite. This shrinkage stabilizes eigenvalues that are near zero to ensure that the inverse of the the sample covariance matrix is well-posed. For an excellent overview of the regularization problem in discriminant analysis, see \cite{Murphy:2012uq} and \cite{Mkhadri:1997gy}.

Here, we consider specifically the  popular regularized discriminant analysis (\emph{RDA}) classifier from \cite{Friedman:1989tm} that incorporates a convex combination of the sample covariance matrices from the \emph{LDA} and \emph{QDA} classifiers. Because the resulting biased covariance-matrix estimator often remains singular in the small-sample, high-dimensional setting, \cite{Friedman:1989tm} applies covariance-matrix shrinkage, where the resulting covariance-matrix estimator is shrunken towards a scaled identity matrix. \cite{Friedman:1989tm} have shown that the \emph{RDA} classifier can attain improved classification accuracy, compared to the \emph{LDA} and \emph{QDA} classifiers when $n > p$.

In this paper we present a generalization of the \emph{RDA} classifier in terms of matrix functions. We show that several common covariance-matrix regularization methods can be specified in terms of a matrix function including the \emph{RDA} classifier as well as methods from \cite{Srivastava:2007ww} and \cite*{Thomaz:2006ef} and several sparse diagonal classifiers, including the diagonal \emph{LDA} (\emph{DLDA}) and \emph{QDA} (\emph{DQDA}) classifiers from \cite{Dudoit:2002ev}, and some of its variants including those proposed by \cite{Pang:2009ik}.  Our generalization elucidates some properties of these covariance-regularization methods and the relationships among them. Moreover, our generalization allows us to view the covariance-matrix regularization methods in terms of eigenvalue thresholds, which further allows us to identify situations where each method is ineffective and ineffective. From a perspective of eigenvalue thresholds, we propose two new classifiers that are often superior to the \emph{RDA} classifier in terms of classification performance because both methods employ a more robust eigenvalue thresholding to regularize the convex combination of covariance matrix estimators employed in the \emph{RDA} classifier. Our two proposed methods also have an advantage over the \emph{RDA} classifier because both methods employ dimension reduction by reducing the number of basis functions included in the \emph{RDA} classifier, thereby reducing the variance of the resulting classifiers.

Using three small-sample, high-dimensional microarray data sets, we demonstrate that our two proposed classifiers often yield superior classification accuracy to the \emph{RDA}, \emph{MDEB}, and \emph{NLDA} classifiers. We also include in our comparison the alternatively-defined \emph{RDA} classifier from \cite*{Hastie:2008dt}, the regularized \emph{LDA} classifier from \cite{Guo:2007te}, a recently proposed variant of diagonal discriminant analysis from \cite*{Tong:2012hw}, and the penalized \emph{LDA} classifier from \cite{Witten:2011kc}. Additionally, we examine the classification accuracy of the classifiers considered using \textbf{three} simulation configurations. Furthermore, we show that our proposed eigenvalue-threshold methods are more numerically stable than the other considered regularization methods using the simulated configurations.

Quote from \cite{Ye:2009gd}: ``There is a lack of a systematic study to explore the commonalities and differences of these algorithms, as well as their intrinsic relation- ship. This has been a challenging task, since different algorithms apply completely different schemes when dealing with the singularity problem.'' Cite this as an issue with LDA and that regularization has been less explored with QDA. Our formulation provides the necessary tools to study this is in detail.

\cite{Ye:2006jm} have shown that shrinking the zero eigenvalues does not improve the classification performance of the \emph{RDA} classifier when $\lambda = 1$. Consequently, we instead shrink only the nonzero eigenvalues of $\widehat{\bm\Sigma}_k(\lambda)$, which has two additional benefits. First, by considering only the nonzero eigenvalues of $\widehat{\bm\Sigma}_k(\lambda)$, we effectively ignore the eigenvectors corresponding to the zero eigenvalues, which reduces significantly the number of parameters that we incorporate into our \emph{RDA} classifier because $p \gg q_k$ if $p \gg N$. Furthermore, the size of the matrix $\bm V_k$ is $p \times N$ as opposed to a $p \times p$ matrix in the original definition of the \emph{RDA} classifier. For significantly large $p$, the reduced size of $\bm V_k$ expedites calculation of the \emph{RDA} classifier and reduces its memory usage.\footnote{TODO: Provide an example for a realistic $p$ and $N$.}

As noted by \cite{Hoyle:2011vt}, \cite{Raudys:1998dd}, and \cite{Ramey:2011ji}, the pseudoinverse is often inadvisable because the resulting classifier often yields a sharp decrease in classification accuracy if $p \approx n$.\footnote{TODO: Contrast the computational efficiency we gain with the need to stabilize the smallest eigenvalues (those near $q$).}

We prove several results using arguments similar to \cite{Ye:2006tq}.

We have organized the remainder of the paper as follows. In Section 2 we present our generalization of the \emph{RDA} classifier after reviewing its original counterpart from \cite{Friedman:1989tm}. In Section 3 we show that our proposed classifier includes several well-known regularized classifiers as special cases and briefly discuss some commonalities among them. In Section 4 we describe the \textbf{three} simulation configurations and three small-sample, high-dimensional microarray data sets and then demonstrate that our two proposed classifiers are superior to the competing classifiers in terms of classification accuracy. We then conclude with a brief discussion in Section 5.

\section{Regularized Discriminant Analysis}

In discriminant analysis we wish to assign correctly an unlabeled $p$-dimensional observation vector $\bm x$ to one of $K$ unique, known classes (or populations) by constructing a classifier from $n$ training observations that can accurately predict the class membership of $\bm x$. Let $\bm x_i = (x_{i1}, x_{i2}, \ldots, x_{ip}) \in \mathbb{R}_{p \times 1}$ be the $i$th observation $(i = 1, \ldots, n)$ with true, unique membership $y_i \in \{\omega_1, \ldots, \omega_K\}$, where $\mathbb{R}_{a \times b}$ denotes the matrix space of all $a \times b$ matrices over the real field $\mathbb{R}$. We assume that $(\bm x_i, y_i)$ is a realization from a mixture distribution $p(\bm x) = \sum_{k=1}^K p(\bm x | \omega_k) p(\omega_k)$, where $p(\bm x | \omega_k)$ is the probability density function (PDF) of the $k$th class, $p(\omega_k)$ is prior probability of class membership of the $k$th class.

The \emph{QDA} classifier is the optimal Bayesian decision rule with respect to a $0-1$ loss function when $p(\bm x | \omega_k)$ is the PDF of the multivariate normal distribution with known mean vector  $\bm\mu_k \in \mathbb{R}_{p \times 1}$ and known covariance matrix $\bm\Sigma_k \in \mathbb{R}_{p \times p}^{>}$, $k = 1, 2, \ldots, K$, where $\mathbb{R}_{p \times p}^{>}$ denotes the cone of real $p \times p$ positive-definite matrices. Because the parameters are typically unknown, we estimate the unknown parameters $\bm \mu_k$ and $\bm\Sigma_k$ with their maximum likelihood estimators (MLEs) and substitute the MLEs into the \emph{QDA} classifier. Assuming, for simplicity, that the prior probabilities of class membership $p(\omega_k)$ are equal for $k = 1, \ldots, K$, we assign an unlabeled observation $\bm x$ to class $\omega_k$ using the sample \emph{QDA} classifier:
\begin{align}
	D_{QDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k|, \label{eq:qda}
\end{align}
where $\xbar_k$ and $\widehat{\bm\Sigma}_k$ are the MLEs for $\bm \mu_k$ and $\bm \Sigma_k$, respectively. If we assume further that the covariance matrix parameters are equal for each class (i.e., $\bm\Sigma_k = \bm\Sigma$ for all $k$), then the pooled sample covariance matrix $\widehat{\bm\Sigma}$ is applied instead for each class, where
\begin{align}
	\widehat{\bm\Sigma} = \frac{1}{n} \sum_{k=1}^K n_k \widehat{\bm\Sigma}_k. \label{eq:pooled-cov}
\end{align}
In this case \eqref{eq:qda} reduces to Fisher's linear discriminant analysis (LDA) classifier, and the log-determinant is omitted because it is constant across all classes.

\subsection{Covariance Matrix Regularization}

The smallest eigenvalues and the directions associated with their eigenvectors highly influence \eqref{eq:qda}. In fact, the eigenvalues of $\widehat{\bm \Sigma}_k$ are well-known to be biased if $p \ge n_k$ such that the smallest eigenvalues are underestimated \citep{Seber:2004uh}, and this bias increases as $p$ increases relative to $n_k$ \citep{TODONeedCitation}. Moreover, if $p > n_k$, then rank$(\widehat{\bm \Sigma}_k) \le n_k$, which implies that at least $p - n_k$ eigenvalues of $\widehat{\bm \Sigma}_k$ are 0. Therefore, although more feature information is available to discriminate among the $K$ classes, if $p > n_k$, \eqref{eq:qda} is incalculable because $\widehat{\bm \Sigma}_k^{-1}$ does not exist.

Several regularization methods, such as the methods considered by \citet*{Guo:2007te}, \cite{Mkhadri:1995jp}, and \citet*{Xu:2009fl}, have been proposed in the literature to adjust the eigenvalues of $\widehat{\bm \Sigma}_k$ so that \eqref{eq:qda} is calculable and has reduced variability in the standard bias-variance tradeoff. These methods often can be written concisely as a matrix function, which we define here using the notation of \cite{Izenman:2008gm}. Let $\bm A \in \mathbb{R}_{p \times p}$ be symmetric, and let $\phi:\mathbb{R} \rightarrow \mathbb{R}$ be monotonic. Then
\begin{align}
	\phi(\bm A) = \sum_{j = 1}^p \phi(e_j) \bm v_j \bm v_j'\label{eq:spectral-decomp}
\end{align}
is a matrix function defined explicitly in terms of the eigenvalues of $\bm A$. For this reason, $\phi$ is often called a spectral transform \citep{Zhu:2010vu} or a transfer function \citep{Ye:2009gd}. Special cases of \eqref{eq:spectral-decomp} include the spectral decomposition of $\bm A$ when $\phi(e_j) = e_j$, the matrix square-root of $\bm A$ when $\phi(e_j) = e_j^{1/2}$, and, if it exists, the inverse of $\bm A$ when $\phi(e_j) = e_j^{-1}$ \citep{Harville:2008wja}.

Notice that if  $\phi(e_{jk}) = e_{jk}$, $k = 1, \ldots, K$, no eigenvalue adjustment is performed, and \eqref{eq:qda} remains incalculable. Similarly, if $\phi(e_{jk}) = e_{j}$, $k = 1, \ldots, K$, the \emph{LDA} classifier is employed without eigenvalue shrinkage. Additionally, setting $\phi(e_j) = e_j^{+}$ with $z^{+} = 1/z$ if $z > 0$ and $0$ otherwise, we ignore the zero eigenvalues, so that $\phi(\widehat{\bm \Sigma}_k) = \widehat{\bm \Sigma}_k^{+}$, the Moore-Penrose pseudoinverse of $\widehat{\bm \Sigma}_k$.

Another common form of the covariance-matrix regularization applies a shrinkage factor $\gamma_k > 0$, so that
\begin{align}
	\phi(e_{jk}) = e_{jk} + \gamma_k, \label{eq:ridge-estimator}
\end{align}
similar to a method employed in ridge regression. Equation \eqref{eq:ridge-estimator} effectively \emph{shrinks} the sample covariance matrix $\widehat{\bm\Sigma}_k$ towards the $p$-dimensional identity matrix $\bm I_p$, thereby increasing the eigenvalues of $\widehat{\bm\Sigma}_k$ by $\gamma_k$. Specifically, the zero eigenvalues are replaced with $\gamma_k$, resulting in $\widehat{\bm \Sigma}_k + \gamma_k \bm I_p$ being positive definite. For additional covariance-matrix regularization methods, see \cite{Ye:2009gd} and \cite{Ramey:2011ji}.

\subsection{Original Definition of Regularized Discriminant Analysis}

\cite{Friedman:1989tm} has proposed the \emph{RDA} classifier by incorporating a weighted average of the sample covariance matrix $\widehat{\bm \Sigma}_k$ for class $\omega_k$ and the pooled sample covariance matrix $\widehat{\bm\Sigma}$ to estimate the covariance matrix for class $\omega_k$ with
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = n_k^{-1}(\lambda) \bm S_k(\lambda),\label{eq:sig-lambda}
\end{align}
where $\lambda \in [0, 1]$, $n_k(\lambda) = (1 - \lambda) n_k + \lambda n$, $\bm S_k = n_k \widehat{\bm\Sigma}_k$, $\bm S = \sum_{k=1}^K \bm S_k$, and $\bm S_k(\lambda) = (1 - \lambda) \bm S_k + \lambda \bm S$. We can interpret \eqref{eq:sig-lambda} as a covariance matrix estimator for class $\omega_k$ that borrows from $\widehat{\bm\Sigma}$ in \eqref{eq:pooled-cov} to better estimate $\bm \Sigma_k$. If $\lambda = 0$, \eqref{eq:sig-lambda} is equal to $\widehat{\bm\Sigma}_k$ used in the \emph{QDA} classifier in \eqref{eq:qda}. Alternatively, if $\lambda = 1$, \eqref{eq:sig-lambda} reduces to \eqref{eq:pooled-cov} used in the \emph{LDA} classifier.

To further improve the estimation of $\bm \Sigma_k$ and to stabilize the inverse of \eqref{eq:sig-lambda}, \cite{Friedman:1989tm} has proposed the biased \emph{RDA} covariance-matrix estimator
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda, \gamma) = (1 - \gamma) \widehat{\bm\Sigma}_k(\lambda) + \gamma \frac{\tr\{\widehat{\bm\Sigma}_k(\lambda)\}}{p} \bm I_p,\label{eq:sig-rda}
\end{align}
for class $\omega_k$, where $\gamma \in [0, 1]$ is a regularization parameter that controls the shrinkage of \eqref{eq:sig-rda}  towards the $p$-dimensional identity matrix $\bm I_p$, which is weighted by the the average of the eigenvalues of \eqref{eq:sig-lambda}. Thus, the \emph{pooling} parameter $\lambda$ controls the amount that we borrow from $\widehat{\bm\Sigma}$ to estimate $\bm \Sigma_k$, and the \emph{shrinkage} parameter $\gamma$ determines the amount of shrinkage applied.

\cite{Friedman:1989tm} substituted \eqref{eq:sig-rda} into \eqref{eq:qda} to obtain the \emph{RDA} classifier
\begin{align}
	D_{RDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\widehat{\bm\Sigma}_k(\lambda, \gamma)^{-1}(\bm x - \xbar_k)  + \ln |\widehat{\bm\Sigma}_k(\lambda, \gamma)|. \label{eq:rda}
\end{align}
Notice that we can write the shrinkage term in \eqref{eq:sig-rda} as the matrix function
\begin{align*}
	\phi(e_{jk}(\lambda)) = (1 - \gamma) e_{jk}(\lambda) + \gamma \bar{e}_k,
\end{align*}
for class $\omega_k$, where $e_{jk}(\lambda)$ is the $j$th largest eigenvalue of $\widehat{\bm\Sigma}_k(\lambda)$ and $\bar{e}_k = p^{-1} \sum_{j=1}^p e_{jk}(\lambda)$ is the average of the eigenvalues of \eqref{eq:sig-lambda}.

\subsection{A Generalization of Regularized Discriminant Analysis}

Here, we define our generalization of the \emph{RDA} classifier by first simplifying the formulation of $\widehat{\bm \Sigma}_k(\lambda)$ in \eqref{eq:sig-lambda} and demonstrating its clear interpretation as a linear combination of the crossproducts of the training observations centered by their respective class sample means. Then, we generalize the adjustment of the eigenvalues of $\widehat{\bm \Sigma}_k(\lambda)$ by introducing a more general matrix-function formulation. This formulation allows us to adjust the eigenvalues of $\widehat{\bm \Sigma}_k(\lambda)$ in a more general manner so that we are not limited to ridge-like estimators.
\begin{align}
  \widehat{\bm\Sigma}_k(\lambda) = (1 - \lambda) \widehat{\bm\Sigma}_k + \lambda \widehat{\bm\Sigma}.\label{eq:sig-lambda-alternative}
\end{align}
as a convex combination of the MLEs of $\bm \Sigma_k$ and $\bm \Sigma$, where $\lambda \in [0, 1]$ is the \emph{pooling} parameter. Thus, we no longer have need for the convex combination of $n_k$ and $N$ as defined by \cite{Friedman:1989tm}. By rewriting \eqref{eq:sig-lambda-alternative} in terms of the observations $\bm x_i$, $i = 1, \ldots, N$, each centered by its class sample mean, we attain a clear interpretation of $\widehat{\bm\Sigma}_k(\lambda)$. We have that
\begin{align}
	\widehat{\bm\Sigma}_k(\lambda) &= \widehat{\bm\Sigma}_k + \lambda (\widehat{\bm\Sigma} - \widehat{\bm\Sigma}_k) \nonumber\\
	&= \widehat{\bm\Sigma}_k + \lambda \left(\sum_{k' = 1}^K \frac{n_k}{N} \widehat{\bm\Sigma}_{k'}  - \widehat{\bm\Sigma}_k \right) \nonumber\\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right) \widehat{\bm\Sigma}_k +  \frac{\lambda}{N} \sum_{\substack{k' = 1\\k' \ne k}} n_{k'} \widehat{\bm\Sigma}_{k'} \nonumber \\
	&= \left( \frac{1 - \lambda}{n_k} + \frac{\lambda}{N} \right)\sum_{i=1}^N I(y_i = k) \bm x_i \bm x_i' +  \frac{\lambda}{N} \sum_{i=1}^N I(y_i \ne k) \bm x_i \bm x_i' \nonumber \\
	&= \sum_{i=1}^N c_{ik}(\lambda) \bm x_i \bm x_i',\label{eq:sig-lambda-alternative2}
\end{align}
where $c_{ik}(\lambda) = \lambda N^{-1} + (1 - \lambda)n_k^{-1}I(y_i = k)$.
From \eqref{eq:sig-lambda-alternative2}, $\lambda$ weights the contribution of each of the $N$ observations to estimating $\bm \Sigma_k$. That is, for $0 < \lambda \le 1$, we estimate $\bm \Sigma_k$ with $N$ rather than $n_k$ observations. Below, we will show that the pooling operation is advantageous in increasing the rank of each $\widehat{\bm\Sigma}_k(\lambda)$ from rank$(\widehat{\bm\Sigma}_k)$ to rank$(\widehat{\bm\Sigma}_k)$ for $0 < \lambda \le 1$. Notice that if $\lambda  = 0$, then the observations from the remaining $K - 1$ classes do not contribute to the estimation of $\bm \Sigma_k$, corresponding to $\widehat{\bm \Sigma}_k$. Furthermore, if $\lambda = 1$, the weights in \eqref{eq:sig-lambda-alternative2} reduce to $1/N$, corresponding to $\widehat{\bm\Sigma}$.

As we have discussed, several eigenvalue adjustment methods have been proposed to increase eigenvalues (approximately) equal to 0. Here, we define a more general eigenvalue adjustment of \eqref{eq:sig-lambda-alternative} by applying the matrix function $\phi_k(\widehat{\bm \Sigma}_k(\lambda))$. To simplify the notation, we write $\tilde{\bm \Sigma}_k = \phi_k(\widehat{\bm \Sigma}_k(\lambda))$. Thus, we have the generalized \emph{RDA} (\emph{GRDA}) classifier
\begin{align}
	D_{GRDA}(\bm x) = \argmin_{k}  (\bm x - \xbar_k)'\tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k|, \label{eq:grda}
\end{align}

\section{Squiggles}
We require the following relationship about the null spaces of $\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm \Sigma}_k$ in order to attain improved computational efficiency of our generalized \emph{RDA} classifier.

\begin{lemma}\label{lemma:null-spaces}
Let $\widehat{\bm \Sigma}_k(\lambda)$, $\widehat{\bm \Sigma}$, and $\widehat{\bm \Sigma}_k$ be defined as above. Then, $\mathcal{N}(\widehat{\bm \Sigma}_k(\lambda)) \subset \mathcal{N}(\widehat{\bm \Sigma}) \subset \mathcal{N}(\widehat{\bm \Sigma}_k)$, $k = 1, \ldots, K$.
\end{lemma}
\begin{proof}
Let $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k(\lambda))$ for some $k = 1, \ldots, K$. Hence, $0 = \bm z' \widehat{\bm \Sigma}_k(\lambda) \bm z = (1 - \lambda) \bm z' \widehat{\bm \Sigma}_k \bm z + \lambda \bm z' \widehat{\bm \Sigma} \bm z$. Because $\widehat{\bm \Sigma}_k$ and $\widehat{\bm \Sigma}$ are each positive semidefinite, we have $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$ and $\bm z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$. Now, suppose $\bm z \in \mathcal{N}(\widehat{\bm \Sigma})$. We have that $0 = \bm z' \widehat{\bm \Sigma} \bm z = N^{-1} \sum_{k = 1}^K n_k \bm z' \widehat{\bm \Sigma}_k \bm z$, which implies that $z \in \mathcal{N}(\widehat{\bm \Sigma}_k)$ once again because $\widehat{\bm \Sigma}_k \in \mathbb{R}_{p \times p}^{\ge}$ for $k = 1, \ldots, K$.
\end{proof}

Here, we derive an alternative expression for $\tilde{\bm \Sigma}_k$ to decompose the decision rule of our proposed classifier. In particular, the alternative expression is terms of the first $q$ regularized eigenvalues of $\tilde{\bm \Sigma}_k$ and the adjustments applied to the $p - q$ zero eigenvalues.

\begin{lemma}\label{lemma:rda-tilde-Sigma_k}
Suppose that rank$(\widehat{\bm \Sigma}) = q \le p$. Let $\widehat{\bm \Sigma} = \bm U \bm D \bm U'$ be the eigendecomposition of $\widehat{\bm \Sigma}$ such that $\bm D \in \mathbb{R}_{p \times p}$ is the diagonal matrix of eigenvalues of $\widehat{\bm \Sigma}$ with
\begin{align*}
	\bm D = \begin{bmatrix}
		\bm D_q & 0\\
		\bm 0 & \bm 0
	\end{bmatrix}
\end{align*}
and $\bm U \in \mathbb{R}_{p \times p}$ is the matrix whose columns are the corresponding eigenvectors of $\widehat{\bm \Sigma}$. Then, partitioning $\bm U = (\bm U_1, \bm U_2)$ so that $\bm U_1 \in \mathbb{R}_{p \times q}$ and $\bm U_2 \in \mathbb{R}_{p \times (p - q)}$, we have the equivalent expression\footnote{To generalize the shrinkage, we define the diagonal matrix of adjusted eigenvalues as $\bm \Phi$ and partition it into two parts: $\bm \Phi_1$ corresponding to the first $q$ eigenvalues of $\widehat{\bm \Sigma}$, and $\bm \Phi_2$ corresponding to the remaining $p - q$ eigenvalues of $\widehat{\bm \Sigma}$.}
\begin{align}
	\tilde{\bm \Sigma}_k &= \bm U \begin{bmatrix}
		\bm W_k & \bm 0\\
		\bm 0 & \gamma \bm I_{p - q}
	\end{bmatrix}\bm U',\label{eq:rda-matrix}
\intertext{where}
\bm W_k &= (1-\gamma) \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm I_q.
\end{align}
\end{lemma}
\begin{proof}
First, notice that
\begin{align*}
	\bm U' \tilde{\bm \Sigma}_k \bm U = (1-\gamma) \{(1 - \lambda) \bm U_1' \widehat{\bm \Sigma}_k \bm U_1 + \lambda \bm D_q\} + \gamma \bm U' \bm U.
\end{align*}
From Lemma \ref{lemma:null-spaces}, we have that $\widehat{\bm \Sigma}_k \bm U_2 = \bm 0$, which implies that 
\begin{align*}
	\bm U' \widehat{\bm \Sigma}_k \bm U = \begin{bmatrix}
		\bm U_1' \widehat{\bm \Sigma}_k \bm U_1 & \bm 0\\
		\bm 0 & \bm 0
	\end{bmatrix},
\end{align*}
for $k = 1, \ldots, K$. Thus, recalling that $\bm U$ is orthogonal, we have that 
\begin{align*}
	\tilde{\bm \Sigma}_k = \bm U \begin{bmatrix}
		\bm W_k & \bm 0\\
		\bm 0 & \gamma \bm I_{p - q}
	\end{bmatrix}\bm U'.
\end{align*}
\end{proof}

An immediate consequence of Lemma \ref{lemma:rda-tilde-Sigma_k} is that ranks of the \emph{RDA} covariance matrices for all classes are each equal to the rank of the pooled sample covariance matrix if the pooling parameter $\lambda$ is positive. Thus, by incorporating each $\bm x_i$ into the estimation of $\bm \Sigma_k$, we increase the rank. We state this formally here.

\begin{cor}
Let $\widehat{\bm \Sigma}_k(\lambda)$ be defined as in \eqref{eq:sig-lambda-alternative}. Then, for $\lambda \in (0, 1]$, rank$(\widehat{\bm \Sigma}_k(\lambda)) = q$ for all $k = 1, \ldots, K$.
\end{cor}
\begin{proof}
The proof follows by setting $\gamma = 0$ in Lemma \ref{lemma:rda-tilde-Sigma_k}.
\end{proof}



In our following result we show that the inverse and determinants of $\bm W_k$ can be written so that they are performed on $n_k \times n_k$ matrices rather than $q \times q$ matrices. Given that $n_k$ is often quite small, say, between 15 and 50, this can reduce the amount of computation substantially. We apply the well-known Sherman-Woodbury-Morrison formula and matrix determinant lemma.\footnote{TODO: This is the term given on Wikipedia, but I have not seen this phrase elsewhere. However, the Wikipedia page references the Harville text.}


\begin{lemma}\label{lemma:rda-W_k}
Let $\bm W_k$ be defined as above. Then, $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$ and
	\begin{align}
		\bm W_k^{-1} &= \bm \Gamma_k^{-1} - (1 - \gamma)(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1},
	\end{align}
where $\bm Q_k = \bm I_{n_k} + (1 - \gamma)(1- \lambda) \bm X_k \bm U_1 \bm \Gamma_k^{-1} \bm U_1' \bm X_k'$ and $\bm \Gamma_k = (1 - \gamma) \lambda \bm D_q + \gamma \bm I_q$.
\end{lemma}
\begin{proof}
	Set $\bm A = \bm \Gamma_k$, $\bm B = (1 - \gamma)(1 - \lambda) \bm U_1' \bm X_k'$, $\bm T = \bm I_{n_k}$, and $\bm C = \bm X_k \bm U_1$. To calculate $|\bm W_k|$, from \cite{Harville:2008wja} we apply Theorem 18.1.1, which states that $|\bm A + \bm B \bm T \bm C| = |\bm A| |\bm T| |\bm T^{-1} + \bm C \bm A^{-1} \bm B|$. Thus, we have $|\bm W_k| = |\bm \Gamma_k| |\bm Q_k|$. Also from \cite{Harville:2008wja}\footnote{TODO: Cite the appropriate theorem from Harville's text.}, we apply the Sherman-Woodbury-Morrison formula $(\bm A + \bm B \bm C)^{-1} = \bm A^{-1} - \bm A^{-1} \bm B(\bm I + \bm C \bm A^{-1} \bm B)^{-1} \bm C \bm A^{-1}$, which implies that $\bm W_k^{-1} = \bm \Gamma_k^{-1} - (1 - \gamma)(1 - \lambda) \bm \Gamma_k^{-1} \bm U_1' \bm X_k' \bm Q_k^{-1} \bm X_k \bm U_1 \bm \Gamma_k^{-1}$.
\end{proof}

The following result is essential to our main result. Here, we show that the difference of a vector $\bm x \in \mathbb{R}_{p \times 1}$ to each class sample mean is constant across all classes by showing that the difference is in the null space of $\bm U_2'$.

\begin{lemma}\label{lemma:RDA-constant-term}
Let $\bm U_2$ be defined as above. Then, for all $\bm x \in \mathbb{R}_{p \times 1}$, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$, $1 \le k, k' \le K$.
\end{lemma}
\begin{proof}
	First, we have that $\xbar_k - \xbar_{k'} = \sum_{i=1}^N \alpha_i \bm x_i$, where $\alpha_i = (n_k n_{k'})^{-1} \{ I(y_i = k) n_{k'} - I(y_i = k') n_k \}$. Let $\bm x \in \mathbb{R}_{p \times 1}$. Then, $\bm U_2' (\bm x - \xbar_k) = \bm U_2' (\bm x - \xbar_{k'})$ is equivalent to $\bm U_2'(\xbar_k - \xbar_{k'}) = \sum_{i=1}^N \alpha_i \bm U_2' \bm x_i$. Recall that $\bm U_2 \in \mathcal{N}(\widehat{\bm \Sigma})$, which implies that $\bm U_2' \in \mathcal{C}(\widehat{\bm \Sigma})^{\perp}$ \citep[Lemma 1.2.5]{Kollo:2005vp}. Now, because $\bm x_i \in \mathcal{C}(\widehat{\bm \Sigma})$, for $i = 1, \ldots, N$, we have that $\bm U_2' \bm x_i = \bm 0$, which implies that $\bm U_2'(\xbar_k - \xbar_{k'}) = \bm 0$.
\end{proof}

Here, we present our main result. We partition the \emph{RDA} decision rule into a sum.

\begin{thm}
	The \emph{RDA} decision rule in \eqref{eq:rda} is equivalent to
	\begin{align}
		D_{RDA}(\bm x) &= \argmin_k (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)' + \ln |\bm Q_k| + (p - q) \ln \gamma + q \ln \{ (1 - \gamma) \lambda \} \label{eq:rda-decomposed}
	\end{align}
	
\end{thm}
\begin{proof}
	From \eqref{eq:rda-matrix}, we have that
	\begin{align*}
		\tilde{\bm \Sigma}_k^{-1} = \bm U \begin{bmatrix}
			\bm W_k^{-1} & \bm 0\\
			\bm 0 & \gamma^{-1} \bm I_{p - q}
		\end{bmatrix} \bm U'
	\end{align*}
	and $|\tilde{\bm \Sigma}_k| = \gamma^{p-q} | \bm W_k |$. Therefore, for all $\bm x \in \mathbb{R}_{p \times 1}$, we have that
	\begin{align*}
	(\bm x - \xbar_k)' \tilde{\bm\Sigma}_k^{-1}(\bm x - \xbar_k)  + \ln |\tilde{\bm\Sigma}_k| &= (\bm x - \xbar_k)' \bm U_1 \bm W_k^{-1} \bm U_1' (\bm x - \xbar_k)'\\
	&+ \gamma^{-1} (\bm x - \xbar_k)' \bm U_2 \bm U_2' (\bm x - \xbar_k) + \ln | \bm W_k | + (p - q) \ln \gamma.
	\end{align*}
Applying Lemma \ref{lemma:RDA-constant-term}, $\bm U_2' (\bm x - \xbar_k)$ is constant for all $k = 1, \ldots, K$. Also, from Lemma \ref{lemma:rda-W_k}, we have that $\ln |\bm W_k| = \ln |\bm \Gamma_k| + \ln |\bm Q_k|$, and because \eqref{eq:rda-decomposed} is independent of $\ln |\bm D_q|$, the proof follows.
\end{proof}

\section{Properties of the Generalized RDA Classifier}

\subsection{Computational Advantages}

Thus far, our alternative parameterization simplifies the definition of the \emph{RDA} classifier and has an improved interpretation in terms of the contribution of each observation. Here, we demonstrate that our parameterization yields significant computational advantages over the originally proposed \emph{RDA} classifier from \cite{Friedman:1989tm} by writing \eqref{eq:sig-lambda-alternative3} as
\begin{align*}
	\widehat{\bm\Sigma}_k(\lambda) &= \bm X_k(\lambda)' \bm X_k(\lambda),
\end{align*}
where $\bm X_k(\lambda) = [\sqrt{c_{1k}(\lambda)} \bm x_1', \ldots,  \sqrt{c_{Nk}(\lambda)} \bm x_N']'$. Now, we can apply the Fast SVD to compute the eigenvalue decomposition of $\widehat{\bm\Sigma}_k(\lambda)$ efficiently. By writing the SVD of $\bm X_k(\lambda) = \bm U_k \bm D_k \bm V_k'$, we have the spectral decomposition $\widehat{\bm\Sigma}_k(\lambda) = \bm X_k(\lambda)' \bm X_k(\lambda) = \bm V_k \bm D_k^2 \bm V_k'$, where $\bm U_k \in \mathbb{R}_{N \times N}$ is orthogonal, $\bm D_k \in \mathbb{R}_{N \times N}$ is a diagonal matrix consisting of the singular values of $\bm X_k(\lambda)$, and $\bm V_k \in \mathbb{R}_{p \times N}$ is orthogonal. Notice that $\bm X_k(\lambda)\bm X_k(\lambda)' = \bm U_k \bm D_k^2 \bm U_k'$, so that the eigenvalue decomposition of $\bm X_k(\lambda)\bm X_k(\lambda)'$ yields $\bm U_k$ and $\bm D_k$. This is much faster when $p \gg N$ because $\bm X_k(\lambda)\bm X_k(\lambda)' \in \mathbb{R}_{N \times N}$. Also, notice that the singular values of $\bm D_k$ are nonzero, so that $\bm D_k^{-1}$ exists. After obtaining $\bm U_k$ and $\bm D_k$, we compute the matrix consisting of the first $N$ eigenvectors of $\widehat{\bm\Sigma}_k(\lambda)$ by simply computing\footnote{TODO: Update this paragraph to theorem/proof.}
\begin{align*}
	\bm V_k = \bm X_k(\lambda)' \bm U_k \bm D_k^{-1}.
\end{align*}

\section{Special Cases}

\subsection{Minimum Distance Empirical Bayes}

\cite{Srivastava:2007ww} derive an empirical-Bayes covariance-matrix estimator under the assumption that the data are independent and identically distributed multivariate normal observations. \cite{Srivastava:2007ww} then substitute the estimator into the \eqref{eq:lda} to attain the minimum distance empirical Bayes (\emph{MDEB}) classifier. To derive the empirical-Bayes covariance-matrix estimator, \cite{Srivastava:2007ww} first assume that $\bm \Sigma_k^{-1}$ follows a Wishart distribution \emph{a priori} with mean $\lambda^{-1} \bm I_p$, $\lambda > 0$, and degrees of freedom, $l \ge p$. The resulting posterior estimator for $\bm \Sigma_k^{-1}$ is $\frac{n_k + l}{n_k}(n_k^{-1}\lambda \bm I_p + \widehat{\bm \Sigma}_k)^{-1}$. Using an empirical Bayes argument to estimate $\lambda$ for $p > n_k$, \cite{Srivastava:2007ww} propose an empirical-Bayes covariance-matrix estimator with matrix function
\begin{align}
	\phi(e_{jk}) = e_{jk} + n_k^{-1} \sum_{j = 1}^p e_{jk},\label{eq:cov-mdeb}
\end{align}
where the shrinkage factor in \eqref{eq:cov-mdeb} is approximately equal to the average of the nonzero eigenvalues of $\widehat{\bm \Sigma}_k$. Substituting \eqref{eq:cov-mdeb} into \eqref{eq:generalized-qda}, we have the \emph{MDEB} classifier. We note that \cite{Srivastava:2007ww} set assumed that $\bm \Sigma_k = \bm \Sigma$ so that $\phi(e_{jk}) = \phi(e_k)$, $k = 1, \ldots, K$.

\cite{Srivastava:2007ww} claim that the \emph{MDEB} classifier is best regularization method they have encountered in the literature. When $p \le n$, the shrinkage factors of the \emph{RDA} and \emph{MDEB} classifiers are equal, but for $p > n$ this shrinkage can differ substantially. In fact, \cite{Ramey:2011ji} show empirically that the \emph{MDEB} classifier can yield superior classification accuracy to the \emph{RDA} classifier.


\subsection{New LDA}
Whereas shrinkage methods often have the form given in \eqref{eq:ridge-estimator}, the covariance matrix estimator from \cite{Thomaz:2006ef} stabilizes the inverse by replacing the smallest eigenvalues of \eqref{eq:pooled-cov} with the average of the eigenvalues of \eqref{eq:pooled-cov}. Let $e_{jk}$ denote the $j$th largest eigenvalue of $\widehat{\bm \Sigma}_k$, and let $\bar{e}_k$ be the average of the eigenvalues of \eqref{eq:pooled-cov}. Then, the inverse of the covariance-matrix estimator has the matrix function
\begin{align}
	\phi(e_{jk}) = \bar{e}_k + (e_j - \bar{e}_k)_+,\label{eq:cov-nlda}
\end{align}
with $z_+ = z$ if $z > 0$ and 0 otherwise. \cite*{Xu:2009fl} and \cite{Ramey:2011ji} demonstrate that the substituting \eqref{eq:cov-nlda} into \eqref{eq:generalized-qda} yields classification accuracies comparable to the \emph{RDA} and \emph{MDEB} classifiers for small-sample, high-dimensional microarray data sets. Similar to the \emph{MDEB} classifier, \cite{Thomaz:2006ef} assume that $\bm \Sigma_k = \bm \Sigma$ so that $\phi(e_{jk}) = \phi(e_k)$, $k = 1, \ldots, K$.

\section{Notation}

$\bm A \oplus \bm B$ denote the direct sum of $\bm A \in \mathbb{R}_{r \times r}$ and $\bm B \in \mathbb{R}_{s \times s}$ \citep[Chapter 1]{Lutkepohl:1996uz}.


\section{Monte Carlo Simulations}

\section{High-Dimensional Microarray Data Sets}

In this section, we describe three high-dimensional microarray data sets and compare our proposed \emph{GRDA} classifier with two recently proposed classifiers for small-sample, high-dimensional data: the penalized \emph{LDA} (\emph{PLDA}) classifier from \cite{Witten:2011kc}, the shrinkage-mean-based DLDA (\emph{SmDLDA}) classifier from \cite{Tong:2012hw}, the \emph{RDA} classifier as defined in \cite{Hastie:2008dt}, and the sparse discriminant analysis (\emph{SDA}) classifier from \cite{Clemmensen:2011kr}. We calculated 10-fold cross-validation error rates \citep{Hastie:2008dt} to evaluate the performance of each classifier considered. Because the \emph{PLDA} classifier has a built-in variable selection method, for fair comparison we compared each of the classifiers using the variables selected from each cross-validation fold.

\subsection{\cite{Burczynski:2006ik} Data Set}

\cite{Burczynski:2006ik} acquired the peripheral blood mononuclear cells (PBMC) through hybridization to microarrays from 127 individuals resulting in 22,283 sequences. Of the 127 individuals, 42 were healthy, 59 had Crohn's disease (CD), and 26 had ulcerative colitis (UC). The goal of \cite{Burczynski:2006ik} was to improve accuracy in the discrimination of Inflammatory Bowel Disease (IBD) using the PBMC-based gene expression signature of a patient.

\subsection{\cite{Nakayama:2007fl} Data Set}

\cite{Nakayama:2007fl} acquired the gene expression through an oligonucleotide microarray from 105 samples of 10 types of soft tissue tumors. This included 16 samples of synovial sarcoma (SS), 19 samples of myxoid/round cell liposarcoma (MLS), 3 samples of lipoma, 3 samples of well-differentiated liposarcoma (WDLS), 15 samples of dedifferentiated liposarcoma (DDLS), 15 samples of myxofibrosarcoma (MFS), 6 samples of leiomyosarcoma (LMS), 3 samples of malignant nerve sheathe tumor (MPNST), 4 samples of fibrosarcoma (FS), and 21 samples of malignant fibrous histiocytoma (MFH). \cite{Nakayama:2007fl} determined from their data that these 10 types fell into 4 broader groups: (1) SS; (2) MLS; (3) Lipoma, WDLS, and part of DDLS; (4) Spindle cell and pleomorophic sarcomas including DDLS, MFS, LMS, MPNST, FS, and MFH. Following \cite{Witten:2011kc}, we restrict our analysis to the five tumor types having at least 15 samples observed.

\subsection{\cite{Singh:2002fh} Data Set}

\cite{Singh:2002fh} have examined 235 radical prostatectomy specimens from surgery patients between 1995 and 1997. The authors used oligonucleotide microarrays containing probes for approximately 12,600 genes and expressed sequence tags. They have reported that 102 of the radical prostatectomy specimens are of high quality: 52 prostate tumour samples and 50 non-tumour prostate samples.

\subsection{Classification Results}

TODO\footnote{TODO: Add results discussion and table here.}


\section{Discussion}

It is interesting to note that our approach is similar to an aside made by Friedman (1989) in the case that the RDA covariance matrix estimator is singular. Friedman (1989) recommended that the zero eigenvalues be replaced with a small value such as 0.001. However, in Theorem 1, we saw that even small adjustments can be inappropriate because they result in ineffective shrinkage.
	

\bibliographystyle{plainnat}
\bibliography{rda}


\end{document} 
