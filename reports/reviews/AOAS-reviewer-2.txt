
This paper addresses an important question: how best to perform classification on the basis of high-dimensional data? An marriage between LDA, QDA, and diagonal LDA is proposed; this is obtained by defining a discriminant function that is a linear combination of the covariance estimates associated with those three approaches. 

My main concern with this paper is that the main ideas [ (1.) take a convex combination of the class-specific within-class covariance matrix and the  overall within-class covariance matrix, this is equation 4; and (2.) add a multiple of the identity (this is equation 6) ] have already been well-studied. For instance, these equations appear in equations (16c) and (18) of Friedman (1989) (a paper which is in fact cited by the authors). Furthermore, ridge penalties have already been extensively studied in the literature, and the idea of a compromise between LDA and QDA has already been studied in many very interesting ways (more interesting than the one proposed in this paper) -- for instance, see http://arxiv.org/abs/1111.1687 

There are also issues of clarity in the paper - for instance, I cannot follow the logic in equation 5.

Section 2.3 contains a lot of linear algebra indicating that computations for inverting the proposed estimate for the covariance matrix are simple -- but as pointed out by the authors, the kernel trick makes computations quite simple here provided that p >> N (which is the setting of interest), and the linear algebra details worked out in this section are not a particularly novel contribution. In particular, clever linear algebra can (and should) be performed for any of the competitors, and so the claim in the last sentence of the discussion that the proposed approach requires less computing time than competitors is suspect. (And just to press the point -- the proposed approach cannot possibly require less computing time than a method that uses a diagonal estimate of the covariance matrix, if all implementations are equally clever!) 

I have major concerns about the real data analysis. As far as I can tell, no comparisons are made to other approaches that use a compromise between LDA & QDA: for instance, equation (16c) in Friedman (1989), or the technique of  http://arxiv.org/abs/1111.1687 . Instead, the comparisons are all to methods that are essentially doing either LDA or diagonal LDA, and so this is an apples-to-oranges comparison.  To further confuse the comparison, the competitors perform built-in feature selection, but the proposed approach does not -- and yet pre-screening is done for all techniques. 

A simulation study indicating, in an objective way, the domain on which this approach will outperform competitors (e.g. other competitors that marry LDA and QDA, such as those mentioned earlier) is missing.

Furthermore, the 1st full paragraph on p11 (sentence "We used the default settings for eachâ€¦." ) to me suggests that cross-validation wasn't used to choose tuning parameters for the competitors, and that instead the default settings of the software were used!!! This would be really wacky and it may well be that I misunderstood what the authors meant -- but this issue needs to be corrected and/or clarified. 